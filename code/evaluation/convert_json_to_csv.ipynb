{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f366779-647d-4b25-a316-9669cc0b979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert_json_to_csv.ipynb\n",
    "\n",
    "This script processes JSON-formatted outputs from LLM evaluations and merges\n",
    "them into a single, consolidated CSV file for unified analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e314d164-9f67-475e-af6e-e1f35100e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b6ee39-3cf0-4e90-a742-3374b46667a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sources(original_csv_path: Path, json_paths: Dict[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads the original dataset and all JSON evaluation files into memory.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading original dataset from: {original_csv_path}\")\n",
    "    # Initialize a dictionary to hold all data, starting with the original CSV.\n",
    "    data_frames = {\n",
    "        \"original\": pd.read_csv(original_csv_path, encoding='utf-8')\n",
    "    }\n",
    "\n",
    "    logging.info(\"Loading JSON evaluation files...\")\n",
    "    # Loop through the dictionary of JSON file paths provided.\n",
    "    for key, path in json_paths.items():\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # Load each JSON file and convert it into a pandas DataFrame.\n",
    "            data_frames[key] = pd.DataFrame(json.load(f))\n",
    "    \n",
    "    logging.info(\"All data sources loaded successfully.\")\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0c1063-b5ca-4c54-b647-d6fa042283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_single_dataframe(data_frames: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges the original DataFrame with all evaluation DataFrames into one.\n",
    "\n",
    "    It renames the columns from each evaluation to prevent conflicts and then\n",
    "    concatenates all of them together side-by-side.\n",
    "\n",
    "    Args:\n",
    "        data_frames (Dict[str, pd.DataFrame]): A dictionary of loaded DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        A single pandas DataFrame containing all merged data.\n",
    "    \"\"\"\n",
    "    original_df = data_frames[\"original\"]\n",
    "    \n",
    "    # Create a list to hold all DataFrames that will be merged, starting with the original.\n",
    "    dfs_to_merge: List[pd.DataFrame] = [original_df]\n",
    "\n",
    "    logging.info(\"Preparing evaluation data for a unified merge...\")\n",
    "    # Iterate through each loaded DataFrame.\n",
    "    for key, df in data_frames.items():\n",
    "        # Skip the original DataFrame as it's already in the list.\n",
    "        if key == \"original\":\n",
    "            continue\n",
    "        \n",
    "        # Rename columns to make them unique and descriptive.\n",
    "        # This prevents column name collisions (e.g., 'score' from relevance.json and 'score' from correctness.json).\n",
    "        renamed_df = df.rename(columns={\n",
    "            \"score\": f\"{key}_score\",\n",
    "            \"reasoning\": f\"{key}_reasoning\"\n",
    "        })\n",
    "        dfs_to_merge.append(renamed_df)\n",
    "    \n",
    "    # Concatenate all DataFrames column-wise (side-by-side) into a single DataFrame.\n",
    "    # axis=1 is crucial for merging along columns rather than rows.\n",
    "    consolidated_df = pd.concat(dfs_to_merge, axis=1)\n",
    "    logging.info(\"Successfully merged all data into a single DataFrame.\")\n",
    "    return consolidated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816dd75d-adbc-4bb1-90b5-c1adae97fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df: pd.DataFrame, output_path: Path):\n",
    "    \"\"\"\n",
    "    Saves a single DataFrame to a specified CSV file path.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The consolidated DataFrame to be saved.\n",
    "        output_path (Path): The file path for the output CSV.\n",
    "    \"\"\"\n",
    "    # Ensure the directory for the output file exists, creating it if necessary.\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logging.info(f\"Writing consolidated data to '{output_path}'...\")\n",
    "    # Write the DataFrame to a CSV file with specific formatting.\n",
    "    # index=False prevents writing the DataFrame's row index as a column.\n",
    "    # quoting=csv.QUOTE_ALL ensures that all fields are quoted, which is safer for text data that might contain commas.\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    logging.info(\"Consolidated result file has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16d9bbc-296b-450c-991a-91e8ef228d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data processing pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Configuration ---\n",
    "        # Define the base directory to make file paths relative and portable.\n",
    "        BASE_DIR = Path.cwd().parent.parent\n",
    "        EVAL_DATA_DIR = BASE_DIR / \"evaluation_data\"\n",
    "        OUTPUT_DIR = BASE_DIR / \"evaluation_results\"\n",
    "        \n",
    "        # Path to the original dataset.\n",
    "        original_csv = EVAL_DATA_DIR / \"rag_evaluation_data.csv\"\n",
    "        \n",
    "        # A dictionary mapping evaluation types to their corresponding JSON result files.\n",
    "        json_files = {\n",
    "            \"relevance\": EVAL_DATA_DIR / \"relevance.json\",\n",
    "            \"faithfulness\": EVAL_DATA_DIR / \"faithfulness.json\",\n",
    "            \"correctness\": EVAL_DATA_DIR / \"correctness.json\"\n",
    "        }\n",
    "        # Define the final output file path.\n",
    "        output_file = OUTPUT_DIR / \"consolidated_evaluation_results_custom_dataset.csv\"\n",
    "\n",
    "        # --- Pipeline Execution ---\n",
    "        # Execute the pipeline steps in sequence: load, merge, and save.\n",
    "        loaded_data = load_data_sources(original_csv, json_files)\n",
    "        merged_data = merge_to_single_dataframe(loaded_data)\n",
    "        save_dataframe_to_csv(merged_data, output_file)\n",
    "        \n",
    "        logging.info(\"Script finished successfully!\")\n",
    "\n",
    "    # --- Error Handling ---\n",
    "    # Catch common errors to provide user-friendly feedback.\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Input file not found: {e}. Please verify file paths and names.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ba14f7-200a-45fb-96d5-7490532ed877",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8c4f8-a606-4ca8-ace8-d32c164166f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
