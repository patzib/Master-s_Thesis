{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b43bb7-e82a-474b-9ec2-718a9d065236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "q_a_generator.ipynb\n",
    "\n",
    "This script automates the creation of a question-answer dataset from a collection\n",
    "of processed documents. It is designed to generate evaluation data for RAG systems\n",
    "\n",
    "The script iterates through JSON files containing pre-chunked text, groups these chunks\n",
    "to form coherent contexts, and then uses a locally-run LLM via Ollama to generate\n",
    "question-answer pairs based on the text. The final output is a single, consolidated .txt\n",
    "file containing all the generated pairs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b71b99c-6b54-49e5-89b1-e6300c180cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d320afde-c806-4722-889c-6973b598dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "OLLAMA_MODEL = \"gemma3:12b\"\n",
    "# Define the base directory of the project to ensure file paths are portable.\n",
    "BASE_DIR = Path.cwd().parent.parent \n",
    "# The source directory containing the processed, chunked JSON files.\n",
    "INPUT_DIRECTORY = BASE_DIR / \"data\" / \"processed_output\"\n",
    "# The destination file where the final generated Q&A dataset will be saved.\n",
    "OUTPUT_FILE = BASE_DIR / \"evaluation_data\"/ \"generated_qa_dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1139a0-e922-4e39-92a4-006774a0dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Extracts a list of text content from the 'chunks' key in a JSON file.\n",
    "\n",
    "    This function is designed to parse the output of a document pre-processing\n",
    "    pipeline where a document is segmented into smaller text chunks.\n",
    "\n",
    "    Args:\n",
    "        file_path (str or Path): The full path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list[str] | None: A list of text strings, where each string is the\n",
    "                          content of a chunk. Returns None if the file cannot\n",
    "                          be parsed, is not found, or lacks the 'chunks' key.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            # Ensure the \"chunks\" key exists and is a list before processing.\n",
    "            if 'chunks' in data and isinstance(data['chunks'], list):\n",
    "                # Extract the \"text\" value from each dictionary in the \"chunks\" list.\n",
    "                return [chunk.get('text', '') for chunk in data['chunks']]\n",
    "            else:\n",
    "                print(f\"Warning: JSON file {file_path} lacks the expected 'chunks' list. Skipping.\")\n",
    "                return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle cases where the file is not valid JSON.\n",
    "        print(f\"Error: Could not decode JSON from {file_path}. Is it a valid JSON file?\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch other potential file I/O errors.\n",
    "        print(f\"Error reading JSON file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d305351-66a9-4ec3-b842-51b954199fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(document_text, num_questions, ollama_model):\n",
    "    \"\"\"\n",
    "    Generates a specified number of question-answer pairs for a given document\n",
    "    text using the Ollama library.\n",
    "\n",
    "    Args:\n",
    "        document_text (str): The source text from which to generate Q&A pairs.\n",
    "        num_questions (int): The exact number of Q&A pairs to generate.\n",
    "        ollama_model (str): The identifier for the Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the formatted Q&A pair(s). Returns an\n",
    "             empty string if generation fails or the input text is empty.\n",
    "    \"\"\"\n",
    "    # Pre-condition check: Do not query the model with empty text.\n",
    "    if not document_text or not document_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # --- Prompt Engineering ---\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following document, please generate exactly {num_questions} question-answer pair.\n",
    "    The question should be answerable directly from the text.\n",
    "    The answer should be concise and extracted from the document.\n",
    "    Provide the output in a simple text format.\n",
    "\n",
    "    Example format:\n",
    "    Q: What is the capital of France?\n",
    "    A: Paris.\n",
    "\n",
    "    Document:\n",
    "    ---\n",
    "    {document_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # API call to the local Ollama service.\n",
    "        response = ollama.generate(\n",
    "            model=ollama_model,\n",
    "            prompt=prompt,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        qa_text = response.get(\"response\", \"\")\n",
    "\n",
    "        # Validate the response from the model.\n",
    "        if qa_text.strip():\n",
    "             print(f\"Successfully generated Q&A pair.\")\n",
    "             return qa_text\n",
    "        else:\n",
    "            print(\"Warning: LLM returned an empty response.\")\n",
    "            return \"\"\n",
    "\n",
    "    except ollama.ResponseError as e:\n",
    "        # Handle API-specific errors, such as a missing model.\n",
    "        print(f\"Error calling Ollama API: {e.error}\")\n",
    "        print(f\"Please ensure Ollama is running and the model '{ollama_model}' is available.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected errors (e.g., network issues).\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e864de02-412e-4ec8-9273-3ad1bf579714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(input_dir, output_file, ollama_model):\n",
    "    \"\"\"\n",
    "    Processes all JSON documents in a directory. For every N chunks of text,\n",
    "    it generates one Q&A pair and saves the results to a single .txt file.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str or Path): The directory containing source JSON files.\n",
    "        output_file (str or Path): The path to the output text file.\n",
    "        ollama_model (str): The identifier for the Ollama model.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"Error: Input directory '{input_dir}' not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Open the output file in 'write' mode to create a fresh dataset each run.\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for filename in os.listdir(input_dir):\n",
    "                if not filename.lower().endswith(\".json\"):\n",
    "                    continue # Ignore non-JSON files.\n",
    "\n",
    "                file_path = os.path.join(input_dir, filename)\n",
    "                print(f\"\\\\n--- Processing: {filename} ---\")\n",
    "                f.write(f\"--- Q&A for: {filename} ---\\\\n\")\n",
    "\n",
    "                chunks = extract_chunks_from_json(file_path)\n",
    "                if not chunks:\n",
    "                    continue # Skip if file was empty or invalid.\n",
    "\n",
    "                # --- Batching Logic ---\n",
    "                # Group 10 chunks together to provide the LLM with enough context to\n",
    "                # form meaningful questions. This is a tunable hyperparameter. A larger\n",
    "                # number provides more context; a smaller number yields more granular Q&A.\n",
    "                chunk_group_size = 10\n",
    "                for i in range(0, len(chunks), chunk_group_size):\n",
    "                    chunk_group = chunks[i:i + chunk_group_size]\n",
    "                    combined_text = \"\\\\n\\\\n\".join(chunk_group)\n",
    "\n",
    "                    if combined_text.strip():\n",
    "                        print(f\"Generating 1 Q&A pair for chunks {i+1}-{i+len(chunk_group)}...\")\n",
    "\n",
    "                        # Generate one Q&A pair for the combined text block.\n",
    "                        qa_text = generate_qa_pairs(combined_text, num_questions=1, ollama_model=ollama_model)\n",
    "\n",
    "                        if qa_text:\n",
    "                            # Write the valid Q&A pair to the output file.\n",
    "                            f.write(qa_text.strip() + \"\\\\n\\\\n\")\n",
    "                            print(\"--- Generated Q&A Pair ---\")\n",
    "                            print(qa_text)\n",
    "\n",
    "        print(f\"\\\\nSuccessfully saved all Q&A pairs to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing or saving the output file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d79888-ab19-414b-88d5-0c0af42f84e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution block. This code runs when the script is executed directly\n",
    "    from the command line or a Jupyter cell.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Q&A Generation Script ---\")\n",
    "    print(f\"Input directory: {INPUT_DIRECTORY}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"Using model: {OLLAMA_MODEL}\")\n",
    "\n",
    "    # Run the main processing function with the configured parameters.\n",
    "    process_documents(\n",
    "        input_dir=INPUT_DIRECTORY,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        ollama_model=OLLAMA_MODEL\n",
    "    )\n",
    "\n",
    "    print(\"\\\\n--- Script finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
