{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538637fa-db5f-448a-ae94-bf74270d8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script conducts a comparative evaluation of a Retrieval-Augmented Generation \n",
    "(RAG) system against a baseline Large Language Model (LLM). It assesses their \n",
    "performance on two distinct general knowledge question-answering datasets: \n",
    "TriviaQA and TruthfulQA.\n",
    "\n",
    "The script queries both the RAG system and the base LLM with a subset of \n",
    "questions from each dataset, records their answers and response times, and saves\n",
    "the comprehensive results into two separate CSV files for subsequent analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daab744a-c371-4e8a-9747-48b3a791d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 1. Import Dependencies ---\n",
    "# ==================================================================================================\n",
    "import pandas as pd\n",
    "import time\n",
    "import ollama\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# --- Define Project Root ---\n",
    "project_code_root = Path.cwd().parent.resolve()\n",
    "project_root = Path.cwd().parent.parent.resolve()\n",
    "\n",
    "if str(project_code_root) not in sys.path:\n",
    "        sys.path.append(str(project_code_root))\n",
    "    \n",
    "from core.rag_setup import setup_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b15f54-aaca-433d-baf5-4a7edd89ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 2. Logging Configuration ---\n",
    "# ==================================================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9db45a3-2460-4c00-8a67-d1dbe90d9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 3. Core Evaluation Functions ---\n",
    "# ==================================================================================================\n",
    "\n",
    "def query_base_llm(question: str, model_name: str) -> str:\n",
    "    \"\"\"Sends a question directly to a specified baseline LLM via Ollama.\"\"\"\n",
    "    try:\n",
    "        # Uses the ollama.chat function for a direct, non-RAG query.\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': question}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying base LLM '{model_name}': {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def evaluate_on_triviaqa(rag_chain, base_llm_name: str, dataset, output_path: Path):\n",
    "    \"\"\"Runs the evaluation loop specifically for the TriviaQA dataset.\"\"\"\n",
    "    results = []\n",
    "    total_questions = len(dataset)\n",
    "    logging.info(f\"--- Starting Evaluation on TriviaQA ({total_questions} questions) ---\")\n",
    "\n",
    "    for i, item in enumerate(dataset):\n",
    "        question = item['question']\n",
    "        # TriviaQA provides a list of possible correct answers or aliases.        \n",
    "        ground_truth_answers = item['answer']['aliases']\n",
    "\n",
    "        # --- Query RAG System ---\n",
    "        start_time = time.time()\n",
    "        rag_response = rag_chain.invoke(question)\n",
    "        rag_time = time.time() - start_time\n",
    "        rag_answer = rag_response.get('answer', 'No answer found.')\n",
    "\n",
    "        # --- Query Base LLM ---\n",
    "        start_time = time.time()\n",
    "        base_llm_answer = query_base_llm(question, base_llm_name)\n",
    "        base_llm_time = time.time() - start_time\n",
    "\n",
    "        # Append all relevant information for this question to the results list.        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answers\": \", \".join(ground_truth_answers),\n",
    "            \"rag_answer\": rag_answer,\n",
    "            \"base_llm_answer\": base_llm_answer,\n",
    "            \"rag_response_time_seconds\": rag_time,\n",
    "            \"base_llm_response_time_seconds\": base_llm_time\n",
    "        })\n",
    "        logging.info(f\"Processed TriviaQA question {i + 1}/{total_questions}\")\n",
    "\n",
    "    # Convert the list of dictionaries into a pandas DataFrame and save to CSV.\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    logging.info(f\"TriviaQA evaluation complete. Results saved to '{output_path}'\")\n",
    "\n",
    "def evaluate_on_truthfulqa(rag_chain, base_llm_name: str, dataset, output_path: Path):\n",
    "    \"\"\"Runs the evaluation loop specifically for the TruthfulQA dataset.\"\"\"\n",
    "    results = []\n",
    "    total_questions = len(dataset)\n",
    "    logging.info(f\"--- Starting Evaluation on TruthfulQA ({total_questions} questions) ---\")\n",
    "\n",
    "    for i, item in enumerate(dataset):\n",
    "        question = item['question']\n",
    "        # TruthfulQA provides distinct lists of correct and incorrect answers.\n",
    "        correct_answers = item['correct_answers']\n",
    "        incorrect_answers = item['incorrect_answers']\n",
    "\n",
    "        # --- Query RAG System ---\n",
    "        start_time = time.time()\n",
    "        rag_response = rag_chain.invoke(question)\n",
    "        rag_time = time.time() - start_time\n",
    "        rag_answer = rag_response.get('answer', 'No answer found.')\n",
    "\n",
    "        # --- Query Base LLM ---\n",
    "        start_time = time.time()\n",
    "        base_llm_answer = query_base_llm(question, base_llm_name)\n",
    "        base_llm_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"correct_answers\": \", \".join(correct_answers),\n",
    "            \"incorrect_answers\": \", \".join(incorrect_answers),\n",
    "            \"rag_answer\": rag_answer,\n",
    "            \"base_llm_answer\": base_llm_answer,\n",
    "            \"rag_response_time_seconds\": rag_time,\n",
    "            \"base_llm_response_time_seconds\": base_llm_time\n",
    "        })\n",
    "        logging.info(f\"Processed TruthfulQA question {i + 1}/{total_questions}\")\n",
    "        \n",
    "    # Save the final results to a CSV file.\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    logging.info(f\"TruthfulQA evaluation complete. Results saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ae2a4f-2465-4790-9f7d-8100b9983cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 4. Main Execution Block ---\n",
    "# ==================================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the general knowledge evaluation.\"\"\"\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # --- RAG System Configuration ---\n",
    "    # A SimpleNamespace is used to create a lightweight, mutable configuration object.\n",
    "    # This keeps all settings for the RAG pipeline organized and easy to pass to the setup function.\n",
    "    eval_config = SimpleNamespace(\n",
    "        BASE_DIR=project_root,\n",
    "        RAW_DOC_FOLDER=project_root / \"data\" / \"raw_documents\",\n",
    "        PROCESSED_DOC_FOLDER=project_root / \"data\" / \"processed_output\",\n",
    "        PERSIST_DIRECTORY=project_root / \"chroma_db\",\n",
    "        PREPROCESSING_SCRIPT_NAME=project_root / \"code\" / \"core\" / \"pre-processing.py\",\n",
    "        MODEL_NAME=\"MLME_chatbot\",\n",
    "        EMBEDDING_MODEL=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        RERANKER_MODEL=\"mixedbread-ai/mxbai-rerank-xsmall-v1\",\n",
    "        OLLAMA_METADATA_MODEL=\"gemma3:4b\",\n",
    "        VECTOR_STORE_NAME=\"rag_store_MLME\",\n",
    "        CHUNK_SIZE=1250,\n",
    "        OVERLAP_SIZE=250,\n",
    "        INITIAL_RETRIEVAL_K=6,\n",
    "        TOP_N_RERANKED=3,\n",
    "        METADATA_GENERATION_CHAR_LIMIT=40000\n",
    "    )\n",
    "    \n",
    "    # --- General Knowledge Evaluation Settings ---\n",
    "    # Define parameters specific to this evaluation, such as the baseline model and dataset size.\n",
    "    BASE_MODEL_NAME = 'gemma3:4b'\n",
    "    DATASET_SUBSET_SIZE = 100\n",
    "    OUTPUT_DIR = project_root / \"evaluation_results\" / \"general_knowledge\"\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TRIVIAQA_OUTPUT_FILE = OUTPUT_DIR / \"triviaqa_generated_answers.csv\"\n",
    "    TRUTHFULQA_OUTPUT_FILE = OUTPUT_DIR / \"truthfulqa_generated_answers.csv\"\n",
    "\n",
    "    try:\n",
    "        # --- Initialization ---\n",
    "        # Set up the entire RAG pipeline. The '_' placeholders are used to ignore the\n",
    "        # memory and vector_db objects, as only the chain is needed for this evaluation.\n",
    "        logging.info(\"Initializing RAG system for general knowledge evaluation...\")\n",
    "        rag_chain, _, _ = setup_chatbot(eval_config)\n",
    "        logging.info(\"RAG system initialization complete.\")\n",
    "\n",
    "        # --- Dataset Loading ---\n",
    "        # Load a small, manageable subset of questions from the Hugging Face Hub.\n",
    "        logging.info(f\"Loading first {DATASET_SUBSET_SIZE} questions from datasets...\")\n",
    "        trivia_qa_dataset = load_dataset(\"trivia_qa\", \"rc.nocontext\", split=f\"validation[:{DATASET_SUBSET_SIZE}]\")\n",
    "        truthful_qa_dataset = load_dataset(\"truthful_qa\", \"generation\", split=f\"validation[:{DATASET_SUBSET_SIZE}]\")\n",
    "        logging.info(\"Datasets loaded.\")\n",
    "\n",
    "        # --- Run Evaluations ---\n",
    "        # Execute the evaluation functions for each dataset sequentially.\n",
    "        evaluate_on_triviaqa(rag_chain, BASE_MODEL_NAME, trivia_qa_dataset, TRIVIAQA_OUTPUT_FILE)\n",
    "        evaluate_on_truthfulqa(rag_chain, BASE_MODEL_NAME, truthful_qa_dataset, TRUTHFULQA_OUTPUT_FILE)\n",
    "\n",
    "        total_end_time = time.time()\n",
    "        logging.info(f\"All evaluations finished successfully in {total_end_time - total_start_time:.2f} seconds.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e049d2-9c25-492d-a13a-d6bb39fd8c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:57:08 - [INFO] - Initializing RAG system for general knowledge evaluation...\n",
      "2025-08-22 18:57:08 - [INFO] - --- Starting Full RAG Chatbot System Setup ---\n",
      "2025-08-22 18:57:08 - [INFO] - Executing external pre-processing script: C:\\Master's_Thesis\\code\\core\\pre-processing.py...\n",
      "2025-08-22 18:57:11 - [INFO] - Pre-processing script completed successfully.\n",
      "2025-08-22 18:57:11 - [WARNING] - SCRIPT STDERR: 2025-08-22 18:57:11,063 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:57:11,065 - [INFO] - Successfully connected to Ollama and found model 'gemma3:4b'.\n",
      "2025-08-22 18:57:11,066 - [INFO] - Output for 'CRISP_Paper.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,066 - [INFO] - Output for 'Ethics_Discrimination_algo_bias.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,066 - [INFO] - Output for 'Feedback Loops in Machine Learning_ A Study on the Interplay of.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,066 - [INFO] - Output for 'lecture_01.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,066 - [INFO] - Output for 'lecture_02.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_02_transcript.txt' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_03.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_03_transcript.txt' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_04.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_05.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_05_transcript.txt' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_06.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,067 - [INFO] - Output for 'lecture_07.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'lecture_08.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'lecture_09.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'lecture_10.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'lecture_11.pdf' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'lecture_information.txt' already exists. Skipping.\n",
      "2025-08-22 18:57:11,068 - [INFO] - Output for 'von-zahn-et-al-2024-smart-green-nudging-reducing-product-returns-through-digital-footprints-and-causal-machine-learning.pdf' already exists. Skipping.\n",
      "\n",
      "2025-08-22 18:57:11 - [INFO] - Initializing Ollama model...\n",
      "2025-08-22 18:57:11 - [INFO] - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:57:11 - [INFO] - Model 'MLME_chatbot' not found. Creating it now...\n",
      "2025-08-22 18:57:11 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/create \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:57:11 - [INFO] - Custom model 'MLME_chatbot' created successfully.\n",
      "2025-08-22 18:57:11 - [INFO] - Initializing embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-08-22 18:57:16 - [INFO] - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-08-22 18:57:22 - [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-08-22 18:57:22 - [INFO] - Vector database is up-to-date. No new documents to ingest.\n",
      "2025-08-22 18:57:23 - [INFO] - Initializing re-ranker model: mixedbread-ai/mxbai-rerank-xsmall-v1\n",
      "2025-08-22 18:57:24 - [INFO] - Use pytorch device: cuda\n",
      "2025-08-22 18:57:24 - [INFO] - Re-ranking retriever created. Will retrieve 6 docs and keep the top 3.\n",
      "C:\\Master's_Thesis\\code\\core\\rag_setup.py:356: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n",
      "2025-08-22 18:57:24 - [INFO] - Creating the conversational retrieval chain...\n",
      "2025-08-22 18:57:24 - [INFO] - Conversational chain created successfully.\n",
      "2025-08-22 18:57:24 - [INFO] - --- RAG Chatbot Setup Complete ---\n",
      "2025-08-22 18:57:24 - [INFO] - RAG system initialization complete.\n",
      "2025-08-22 18:57:24 - [INFO] - Loading first 100 questions from datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ccbda1829640b9a65bbdb341a0b5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:57:32 - [INFO] - Datasets loaded.\n",
      "2025-08-22 18:57:32 - [INFO] - --- Starting Evaluation on TriviaQA (100 questions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79213c1a4dd84a0f836da4054e2d8712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:57:37 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:57:57 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:57:57 - [INFO] - Processed TriviaQA question 1/100\n",
      "2025-08-22 18:57:57 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eaeb7a6b864f32bb8706f8986f7490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:58:00 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:06 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:06 - [INFO] - Processed TriviaQA question 2/100\n",
      "2025-08-22 18:58:07 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8678b72b0b764e8b8b0fa8cfa5315b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:58:12 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:16 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:16 - [INFO] - Processed TriviaQA question 3/100\n",
      "2025-08-22 18:58:17 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a84ebfd27c64098a8ee2606221f20ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:58:22 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:28 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:28 - [INFO] - Processed TriviaQA question 4/100\n",
      "2025-08-22 18:58:29 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69a11c78cbe4e1fb10cb2aef03f67ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:58:32 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:55 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:58:55 - [INFO] - Processed TriviaQA question 5/100\n",
      "2025-08-22 18:58:56 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf3f79a12b94799b707dc31f9a89001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:58:59 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:04 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:04 - [INFO] - Processed TriviaQA question 6/100\n",
      "2025-08-22 18:59:05 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b060128c2f3b4b90b95e174961d05793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:59:07 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:24 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:24 - [INFO] - Processed TriviaQA question 7/100\n",
      "2025-08-22 18:59:25 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0c6e61fd9d41b2b4072d9d05b62205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:59:28 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:34 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:34 - [INFO] - Processed TriviaQA question 8/100\n",
      "2025-08-22 18:59:35 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daff49b44f041798992d25801a520cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:59:38 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:53 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 18:59:53 - [INFO] - Processed TriviaQA question 9/100\n",
      "2025-08-22 18:59:53 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ddc0bf87d940fd817272fee43c1403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 18:59:56 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:02 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:02 - [INFO] - Processed TriviaQA question 10/100\n",
      "2025-08-22 19:00:02 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403339d1e3de4cb6b1b509d44d2ad0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:05 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:17 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:17 - [INFO] - Processed TriviaQA question 11/100\n",
      "2025-08-22 19:00:18 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ce01f03ce4d70883e5a49acc569a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:21 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:28 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:28 - [INFO] - Processed TriviaQA question 12/100\n",
      "2025-08-22 19:00:29 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e742897add4e4286d29e9243e6a23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:32 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:38 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:38 - [INFO] - Processed TriviaQA question 13/100\n",
      "2025-08-22 19:00:39 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd0d4f06a7b4c8981bdd4585f7a1141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:42 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:45 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:45 - [INFO] - Processed TriviaQA question 14/100\n",
      "2025-08-22 19:00:46 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dcbf85df1140a5aa0f636476a261f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:49 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:51 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:00:51 - [INFO] - Processed TriviaQA question 15/100\n",
      "2025-08-22 19:00:52 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7464f212f94db5bd138894e981de59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:00:55 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:01:01 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:01:01 - [INFO] - Processed TriviaQA question 16/100\n",
      "2025-08-22 19:01:02 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db4d21f162a4c13aff814e5af9e9c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:01:04 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:01:12 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-22 19:01:12 - [INFO] - Processed TriviaQA question 17/100\n",
      "2025-08-22 19:01:13 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2633d396d14b26bf0f0373422f6be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 19:01:17 - [INFO] - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569cdd8-8ef2-4214-a1c2-876e8e2c67c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
