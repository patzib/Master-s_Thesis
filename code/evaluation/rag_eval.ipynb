{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649fe07-f740-4666-8046-a0f851ffa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rag_eval.ipynb\n",
    "\n",
    "This script automates the process of evaluating the RAG chatbot against a\n",
    "pre-defined ground truth dataset. It initializes the full RAG pipeline,\n",
    "iterates through each question in the dataset, and records the chatbot's\n",
    "generated answer, the retrieved context, and the response time.\n",
    "\n",
    "The final output is a CSV file containing the results, which can then be\n",
    "used for scoring and analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64568ab8-7a82-457f-b1bc-2f337841e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 1. Import Dependencies ---\n",
    "# ==================================================================================================\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# --- Define Project Root and Add to Python Path ---\n",
    "project_code_root = Path.cwd().parent.resolve()\n",
    "\n",
    "if str(project_code_root) not in sys.path:\n",
    "        sys.path.append(str(project_code_root))\n",
    "    \n",
    "from core.rag_setup import setup_chatbot\n",
    "from core import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05794b21-67b7-45c0-8670-74fec2b85114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 2. Logging Configuration ---\n",
    "# ==================================================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d173d87f-7bf6-4fe4-9925-858e761ea5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# --- 3. Core Evaluation Functions ---\n",
    "# ==================================================================================================\n",
    "\n",
    "def run_evaluation_pipeline(chain, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes each question in the dataset using the RAG chain and collects results.\n",
    "\n",
    "    Args:\n",
    "        chain: The initialized ConversationalRetrievalChain object.\n",
    "        dataset (pd.DataFrame): The DataFrame containing the ground truth questions and answers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the evaluation results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_questions = len(dataset)\n",
    "    logging.info(f\"Starting evaluation for {total_questions} questions...\")\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        try:\n",
    "            # The source CSV contains JSON strings; they must be parsed.\n",
    "            question_json = json.loads(row['question'])\n",
    "            ground_truth_json = json.loads(row['expected_answer'])\n",
    "\n",
    "            question_text = question_json['question']\n",
    "            ground_truth_answer_text = ground_truth_json['expected_answer']\n",
    "\n",
    "            # --- Query RAG System and Measure Latency ---\n",
    "            start_time = time.time()\n",
    "            response = chain.invoke(question_text)\n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "\n",
    "            # --- Extract and Structure Data ---\n",
    "            generated_answer = response.get('answer', 'Error: No answer found.')\n",
    "            retrieved_context = \" \".join([doc.page_content for doc in response.get('source_documents', [])])\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question_text,\n",
    "                \"ground_truth_answer\": ground_truth_answer_text,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"retrieved_context\": retrieved_context,\n",
    "                \"response_time_seconds\": response_time\n",
    "            })\n",
    "            logging.info(f\"Processed question {index + 1}/{total_questions} in {response_time:.2f} seconds\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process question at index {index}: {e}\")\n",
    "            # Append a row indicating the error to maintain dataset alignment.\n",
    "            results.append({\n",
    "                \"question\": question_text,\n",
    "                \"ground_truth_answer\": ground_truth_answer_text,\n",
    "                \"generated_answer\": f\"ERROR: {e}\",\n",
    "                \"retrieved_context\": \"\",\n",
    "                \"response_time_seconds\": 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0189aa-9143-4bd2-adea-301d1ac0eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire evaluation process.\n",
    "    \"\"\"\n",
    "    project_root = Path.cwd().parent.parent.resolve()\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # --- Extract model version from config for dynamic file naming ---\n",
    "    try:\n",
    "        model_version = config.MODEL_NAME.split('_')[-1]\n",
    "        output_filename = f\"rag_eval_generated_answers_{model_version}.csv\"\n",
    "    except (IndexError, AttributeError):\n",
    "        # Fallback in case MODEL_NAME format is unexpected\n",
    "        logging.warning(\"Could not parse model version from config.MODEL_NAME. Using default filename.\")\n",
    "        output_filename = \"rag_eval_generated_answers.csv\"\n",
    "\n",
    "    # Define input and output paths for this evaluation run.\n",
    "    input_csv_path = project_root / \"evaluation_data\" / \"ground_truth_dataset.csv\"\n",
    "    output_csv_path = project_root / \"evaluation_data\" / output_filename\n",
    "    output_csv_path.parent.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "    try:\n",
    "        # --- Initialization ---\n",
    "        logging.info(\"Initializing RAG system with evaluation configuration...\")\n",
    "        chain, _, _ = setup_chatbot(config)\n",
    "        logging.info(\"RAG system initialization complete.\")\n",
    "\n",
    "        # --- Data Loading ---\n",
    "        logging.info(f\"Loading ground truth dataset from: {input_csv_path}\")\n",
    "        qa_dataset = pd.read_csv(input_csv_path)\n",
    "\n",
    "        # --- Pipeline Execution ---\n",
    "        evaluation_results_df = run_evaluation_pipeline(chain, qa_dataset)\n",
    "\n",
    "        # --- Save Results ---\n",
    "        logging.info(f\"Saving evaluation results to: {output_csv_path}\")\n",
    "        evaluation_results_df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        logging.info(f\"Evaluation script finished successfully in {total_end_time - total_start_time:.2f} seconds.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}. Please ensure all paths are correct.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48822d2-d5fe-412b-9b5e-ee85e81b35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
