question,ground_truth_answer,generated_answer,retrieved_context,response_time_seconds
"In the paper about the Centaur foundation model, what happens to the model's internal representations after being fine-tuned on the Psych-101 dataset?",The model's internal representations become more aligned with human neural activity.,The provided text doesn’t contain information about the Centaur foundation model or what happens to its internal representations after being fine-tuned on the Psych-101 dataset. It discusses a different machine learning model and a general approach to understanding how such models work.,"die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen. eher das Modell nehmen, was 
einfacher ist.
Der Gedanke, der dahinter 
steckt,
ist, dass wir vermeiden wollen,
dass die zufälligen Schwankungen 
in den
Daten mit höherer 
Modellkomplexität
in dem Modell erkannt werden als 
wichtige Muster.
Time ist jetzt auch ein bisschen 
drauf gucken,
dass ich meinen Time Management 
richtig betreibe,
weil wie Sie vielleicht gemerkt 
haben,
habe ich die Tendenz, viel zu 
reden.
Kommen Sie gerne nach der 
Vorlesung,
dann können wir darüber 
sprechen.
Wenn wir das verallgemeinern und 
jetzt nicht nur
über unser EiscremeBeispiel 
nachdenken,
sondern ganz, ganz, ganz High 
Level aus einer fast schon.
Und Uwe Walz würde sich freuen,
wenn diese Formel hier angezeigt 
wird.
Wahrscheinlich aus einer 
mikroökonomischen
Perspektive kann man
natürlich fragen Warum brauchen 
wir Vorhersagen?
Und aus einer ökonomischen
Sicht Wenn ich bestimmte 
Entscheidungsprobleme,
Nutzenmaximierungsprobleme,
Gewinnmaximierungsprobleme 
aufschreibe, dann ist immer so
ein bisschen in
fast allen Optimierungsproblemen 
uncertainty mit drin.
Bestimmte Sachen weiß ich 
einfach nicht.
Es kommt so ein bisschen darauf 
an,
was in der Zukunft passieren 
wird.
Wenn ich heute investiere,
wenn ich eigentlich mein",7.477282286
"In the study on capturing human cognition, how does the Centaur model's performance compare to existing cognitive models?",Centaur captures the behaviour of held-out participants better than existing cognitive models.,"According to the text, the Centaur model, developed by Venkatesh and colleagues, explains approximately 70% of the variation in whether people use and accept technology. This is described as ""massively"" significant, especially in sociotechnological systems where human factors are critical. The text doesn't directly compare its performance to *existing* cognitive models in a detailed way, but it highlights the substantial explanatory power achieved by the Centaur model.","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",10.64578772
"According to the paper on the Centaur foundation model, what is it designed to predict and simulate regarding human cognition?",It can predict and simulate human behaviour in any experiment expressible in natural language.,"The text doesn't provide information about a ""Centaur foundation model."" It describes the use of a mathematical model of a brain cell (a perceptron) and a modified version of an investment game to simulate economic transactions. 

It focuses on using simulations to understand how humans might behave in situations lacking robust enforcement mechanisms, rather than predicting or simulating human cognition in general.","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization robust and widely  accepted tool across multiple 
scientific fields, facilitating the analysis of dynamic 
processes (Kahalé, 2020). Such simulations also 
enable the generation of proofs of concept for novel 
ideas (Wong & Kwong, 2018). We opt for this research 
methodology primarily because the exogenous 
introduction of discrimination, particularly by humans, 
is not only ethically untenable but also practically 
unfeasible in experimental or field settings. 
Our simulation employs a modified version of the 
investment game (Berg et al., 1995). This game 
emulates the incentives and informational structure 
inherent in numerous economic transactions that 
transpire in environments lacking robust enforcement 
mechanisms (Dufwenberg & Kirchsteiger, 2004; Feh r 
& Fischbacher, 2003). In our game, two participants, 
an investor and a borrower, each begin with an 
endowment of 10 monetary units (MU). The investor 
first chooses whether to retain or invest the entire 10 
MU with the borrower. If the investor retains the sum, 
the game ends and both parties keep their initial 
endowment of 10 MU, representing a non -
transactional scenario, e.g., where a bank loan officer 
does not approve a loan. Alternatively, if the investor Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",9.857061386
"For the Centaur model, what is the name and scale of the dataset used for fine-tuning to capture human cognition?","Psych-101, which covers trial-by-trial data from over 60,000 participants in 160 experiments.",The text doesn't provide the name or scale of the dataset used for fine-tuning the Centaur model to capture human cognition. It primarily focuses on the importance of data cleaning and preparation before using data for machine learning models.,"10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization Training von Daten,
das würden wir alles schon 
wissen.
Beziehungsweise es würde uns
aktuell noch nicht 
interessieren.
Da kommen wir zu dem zweiten 
Teil der Vorlesung.
Aber ein Riesenteil.
Bevor wir überhaupt an Modelle 
denken können,
ist es dann, Daten sauber zu 
machen.
Damit werden wir sie noch 
einiges triezen.
Daten, die Sie erhalten,
sind typischerweise insbesondere 
aus der Industrie.
Noisy Bestimmte Observationen 
fehlen.
Dann haben unterschiedliche
Leute unterschiedliche Eingaben 
gemacht,
haben unterschiedliche
Arten und Weisen diese Eingaben 
zu machen.
Manche nutzen Komma, um
Zahlen um decimal points zu 
setzen,
andere nutzen den Punkt.
Die einen schreiben Doppelees, 
die Anschreiben,
scharfes S, Die anderen 
schreiben
groß am Anfang anschreiben, 
klein am Anfang.
Die einen denken das ist eine 
Eins,
die anderen denken das ist eine 
Zwei.
All diese Dinge, wo irrelevante 
Noise sources,
also statistische Fluktuationen,
die nicht zu bedeuten hat.
Das müssen Sie identifizieren.
Sie müssen das sauber machen.
Maschinen, wie wir sehen werden,
können nicht mit 
unstrukturierten
Daten wie Wörtern einfach so 
umgehen.
Wir haben ganz viel 
Datenvorarbeit zu betreiben
und es gibt bestimmte Machine 
Learning Modelle, wo Sie KI einsetzen können,
wann und wo mögliche 
Problemstellungen sind.
Aus diesem ersten Teil hier 
zumindest,
dass Sie uns das strukturiert 
wiedergeben
können und uns zeigen,
dass Sie diese Denkweise 
verstanden haben.
Wenn wir heute dann wie gesagt,
einsteigen und das Thema des 
ersten Blockes abschließen.
Kümmern wir uns jetzt noch mal 
darum,
was eigentlich der Mensch in 
diesem ganzen System
weiterhin noch macht und kümmern 
uns
um diese Mensch Maschine 
Interaktion.
Wenn Sie sich erinnern Sehr früh
in der zweiten Vorlesung haben 
wir festgestellt,
dass die meisten Machine 
Learning Anwendungen
es uns eigentlich erlauben,
Vorhersagen über oder unter 
Unsicherheit zu machen.
Wir kriegen Informationen, die 
uns nicht vorliegen,
und diese Informationen sind
irgendwie nützlich für uns in 
einem Entscheidungsprozess.
Sei es in dem Beispiel,
dass wir eine 
Investitionsentscheidung
treffen möchten und uns
dafür überlegen oder eine 
Vorhersage benötigen.
Wird diese 
Investitionsentscheidung, wie in
diesem Beispiel eine hohe
Rückzahlung für mich ergeben 
oder eine
niedrige Rückzahlung?
Mit dieser Information kann ich 
mir dann überlegen,
ob ich diese Investition
tatsächlich machen möchte oder 
eben nicht.",8.252163649
In the paper 'A foundation model to predict and capture human cognition' what is the name of the computational model introduced?,Centaur.,The paper introduces a mathematical model of a brain cell: a **perceptron**.,"10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization 12. Juni 2025
Please answer the following questions:(a) Label the components of the following equation according to our definition in the lecture. (b) Explain each component in 1-2 short sentences in your own word. (c) Based on the components, explain the overarching goal of machine learning models.
4
Learned modelFrom historic data
The objective of ML is to make predictions !𝒚given new data points 𝒙for which we do not know the true outcome 𝒚
!𝑓(𝑥) &𝑦Input information:Tabular dataImagesText…
Prediction:Loan defaultStock priceCustomer retention…
=
?
? ? Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",6.076759815
The paper 'Algorithmic Bias?' concludes that optimizing for what factor can lead to apparently discriminatory outcomes in ad delivery?,An algorithm that simply optimizes for cost-effectiveness.,"According to the text, optimizing for **inadequate information** can lead to apparently discriminatory outcomes in ad delivery.","significant consequences, such as in healthcare, loan 
approval, hiring, or criminal justice systems. 
Mitigating algorithmic discrimination : The 
expansive literature on algorithmic discrimination 
proposes various methods to identify and alleviate 
such biases. According to d ’Alessandro et al. (2017), 
we can divide these mitigation strategies into three 
categories: pre -processing, in -processing, and post -
processing. Pre -processing strategies strive to 
eliminate disparities inherent in the dataset. Methods 
such as those proposed by Kamiran and Calders (2012) 
and Zafar et al. (2017) employ data pre -processing 
techniques like feature massaging and reweighting to 
balance the dataset based on social group 
characteristics. In -processing strategies involve 
conventional learning algorit hms during model 
training. Researchers often use techniques like naive 
Bayes and decision tr ees for debiasing (Calders & 
Verwer, 2010; Zhang et al., 2018). Additional research 
suggests placing constraints on the classification 
model to mandate the satisfaction of a proxy 
(Woodworth et al., 2017). Lastly, post -processing 
strategies aim to adjust the output to counter biases. 
For example, Corbett -Davies et al. (2017) 15. Mai 2025
Consequences of algorithmic biasIndividual consequences include•rejection or discontinuance of ML tools.•customers paying higher prices than usual Organisational consequences include •violating equal opportunity policies•high customer churn rate Societal-level consequences include •increased gaps between historically disadvantaged groups and others•Fostered polarization and human discrimination
17 that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that",7.522337437
"In the context of the STEM career ad study, what was the difference in click behavior between men and women who were shown the ad?","Among those who saw the ad, women were more likely to click on it than men.","The text does not contain information about ad delivery or optimizing for inadequate information in that context. It focuses on algorithmic discrimination in areas like hiring, loan approvals, and criminal justice systems.","that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that significant consequences, such as in healthcare, loan 
approval, hiring, or criminal justice systems. 
Mitigating algorithmic discrimination : The 
expansive literature on algorithmic discrimination 
proposes various methods to identify and alleviate 
such biases. According to d ’Alessandro et al. (2017), 
we can divide these mitigation strategies into three 
categories: pre -processing, in -processing, and post -
processing. Pre -processing strategies strive to 
eliminate disparities inherent in the dataset. Methods 
such as those proposed by Kamiran and Calders (2012) 
and Zafar et al. (2017) employ data pre -processing 
techniques like feature massaging and reweighting to 
balance the dataset based on social group 
characteristics. In -processing strategies involve 
conventional learning algorit hms during model 
training. Researchers often use techniques like naive 
Bayes and decision tr ees for debiasing (Calders & 
Verwer, 2010; Zhang et al., 2018). Additional research 
suggests placing constraints on the classification 
model to mandate the satisfaction of a proxy 
(Woodworth et al., 2017). Lastly, post -processing 
strategies aim to adjust the output to counter biases. 
For example, Corbett -Davies et al. (2017) Theme IV. How decreasing algorithmic recruitment
discrimination
Changes should be made at the technical and regulatory levels to
ensure that AI algorithms do not replicate existing biases or
introduce new ones based on the provided data (Raub,2018).
Building fair algorithms from a technical perspective
Constructing a more unbiased dataset. Unfair datasets are the root
cause of bias. Therefore, a direct approach to addressing algo-
rithmic bias is reconﬁguring unbalanced datasets. Using multiple
data points can yield more accurate results while carefully elim-
inating data points that re ﬂect past biases. However, this
approach incurs signiﬁcant costs (Bornstein,2018).
Another method is to correct data imbalances by using more
equitable data sources to ensure fair decision-making (36KE,
2020). Understanding the underlying structure of training data
and adjusting the signi ﬁcance of speci ﬁc data points during
training based on known latent distributions makes it possible to
uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold",8.049456835
"In the empirical study of gender-based discrimination in STEM ad display, what reason is given for why fewer women saw the advertisement?","Younger women are a prized demographic and are more expensive to show ads to, so a cost-optimizing algorithm shows the ad to them less often.","According to the text, the reason fewer women saw the advertisements is because “trends of other minority or female applicants, which may differ from the features identified as successful representatives.” Essentially, the algorithm was prioritizing features associated with male applicants, leading to a bias against female applicants.","Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., according to the trends of other minority or female applicants,
which may differ from the features identi ﬁed as successful
representatives.
Word embedding. Microsoft researchers found that words
exhibit distinct associations in news and web data. For instance,
words like “fashion” and “knitting” are more closely related to
females, while “hero” and “genius” are more closely related to
males (36KE, 2020). Microsoft suggests a simple solution by
removing the gender-speciﬁc measures in word embedding to
reduce “presentation bias.”
Differential testing. Scientists at Columbia University devel-
oped Deep Xplore, a software that highlights vulnerabilities in
algorithmic neural networks via “coaxing” the system to make
mistakes (Xie et al., 2018). Deep Xplore utilizes discrepancy
testing, which involves comparing several systems and observing
their outputs’differences. A model is considered vulnerable if all
other models consistently predict a particular input while only
one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation, 0% in Iterations 1 (left panel) to 22% in Iteration 100 
(right panel) if there is no human discrimination (z-axis). 
However, this growth is significantly curtailed, reaching 
merely 1.2%, 6.9%, 14.3% under very high, high, and 
medium human discrimination, respectively. 
Conversely, human discrimination against women does 
not alter the share of positive predictions for male 
borrowers (see Figure A7 in the Appendix), regardless of 
the initial label imbalance. For example, at maximum 
label imbalance, the positive predictions  for men 
minimally fluctuate, moving from 34.4% to 36.1% 
without human discrimination and to 33.8% with very 
high human discrimination (36.3% and 34.2% for high 
and medium levels of human discrimination, 
respectively). Our regression analyses (see Table A2 in 
the Appendix) statistically corroborate these findings. 
Regression estimates in Column (1) suggest that for an 
initially unbiased model under high and very high human 
discrimination, the gender gap widens by about 0.07 
percentage points per iteration, or 7 percentage points 
across all 100 iterations.",9.452150583
"Regarding the display of STEM career ads, what was the empirical result of the ad's delivery in the 'Algorithmic Bias?' paper?",Fewer women saw the ad than men.,"According to the “Algorithmic Bias?” paper by Lambrecht and Tucker (2019), the empirical result was that algorithmic systems displayed a gender-based bias in the delivery of STEM career ads. Specifically, they found that ads for STEM careers were more likely to be shown to women than to men.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)
from an organizational behavior perspective. While recent studies
have primarily discussed discrimination theory in the traditional
economy’s hiring market, future theoretical research should
consider how advanced technology affects equity in hiring within
the digital economy.
Conclusion
The study concludes that the fo urth industrial revolution
introduced technological innovations signiﬁcantly affecting the
recruitment industry. It extends the analysis of statistical dis-
crimination theory in the digital age and adopts a literature
review approach to explore four themes related to AI-based
recruitment. The study argues that algorithmic bias remains an
issue while AI recruitment tools offer beneﬁts such as improved
recruitment quality, cost reduction, and increased ef ﬁciency.
Recruitment algorithms ’bias is evident in gender, race, color,
and personality. The primary source of algorithmic bias lies in
partial historical data. The personal preferences of algorithm
engineers also contribute to algorithmic bias. Technical mea-",10.23932028
"In the study on gender-based discrimination in STEM career ads, what was the explicit intention for the ad's delivery?",The ad was explicitly intended to be gender-neutral in its delivery.,"According to the provided text, the study on gender-based discrimination in STEM career ads aimed to **empirically study apparent gender-based discrimination in the display of STEM career ads.** 

Specifically, Lambrecht and Tucker (2019) investigated whether algorithms were displaying gender-based biases in the presentation of STEM career advertisements.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",10.17115617
The findings of the 'Feeback Loops in Machine Learing' paper underscore the complex nature of algorithmic discrimination as what kind of phenomenon?,A sociotechnological phenomenon.,"According to the text, the findings underscore algorithmic discrimination as a **dynamic and sociotechnological phenomenon** – specifically, one that can diminish or emerge due to the interaction between ML systems and human users in environments where continuous model updating is essential. It highlights how humans can inadvertently teach machines to maintain or even adopt discriminatory behaviors over time.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",9.290673971
What type of setting was used in the simulations of the 'Feedback Loops in Machine Learning' paper to examine the ML model's evolution?,"A setting that mirrors sequential interactions, such as loan approval decisions.","The simulations in the “Feedback Loops in Machine Learning” paper were based on a setting that mirrored sequential interactions, such as loan approval decisions. Specifically, they utilized data from an incentivized investment game involving students (Study A) and investors (Study B) who interacted with an ML model.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. Feedback Loops in Machine Learning 
 
805 
generate predictions (Parisi et al., 2019). However, in 
dynamic, nonstationary environments, the underlying 
data-generating process that ML models aim to 
approximate can change over time. This phenomenon, 
referred to as drift (Lu et al., 2018), can lead to  
progressively poorer predictions. The decrease in 
prediction performance can originate from changes in 
the underlying relationship between the input features 
and the target variable —concept drift (Widmer & 
Kubat, 1996)—or changes in the distributions of t he 
input features or the target variable—data drift (Mallick 
et al., 2022). As a consequence, to maintain or even 
improve prediction accuracy over time, it is imperative 
to continuously update ML models on recent data that 
reflect new patterns in the data -generating process 
(Gama et al., 2014). 
The efficacy of continuous updating inherently links to 
the quality and structure of the collected data. Ideally, 
updating processes should empower models to learn 
from their past mistakes —that is, identify cases for 
which they generate inaccurate predictions. However, 
ML models that support or automate human decisions 
typically do more than merely provide neutral Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were",9.593123674
What is the key finding regarding the continuous updating of ML models?,Continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time.,"That’s correct. Based on the provided text, a key finding is that continuous updating of ML models can diminish or emerge due to the interaction between ML systems and human users. Specifically, human intervention (like overriding predictions) can impede self-correction and even lead to biased models over time.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers.",8.973435402
How do ML models endogenously shape their future training data?,"By influencing human decisions, which then generate new data points (or a lack thereof) for future model training.","According to the text, ML models endogenously shape their future training data by:

*   **Generating new data based on their predictions:** The models’ predictions themselves create new data points that are then used to update the model.
*   **Reinforcing discriminatory patterns:** When human decision-makers override ML predictions, they can inadvertently perpetuate and even amplify existing biases, leading the model to learn and reinforce discriminatory patterns.
*   **Initially unbiased models becoming discriminatory:** Ongoing human discrimination can cause models originally trained on balanced data to develop increasingly discriminatory behavior over time.","endogenous composition of future training data, a 
problem referred to as selective labeling (Lakkaraju et 
al., 2017; Little & Rubin, 2019), can maintain or even 
exacerbate inaccurate or discriminatory predictions. 
In this paper, we explore the dynamic interplay 
between the continuous updating of ML models and 
algorithmic discrimination when ML predictions 
endogenously shape the composition of additional data 
available for model updating. In particular, our work 
focuses on the role of ongoing discrimination by 
human decision  makers who have the final say in an 
ML-supported decision-making task. A steady stream 
of previous work has examined how algorithmic 
discrimination can reproduce and perpetuate 
discriminatory patterns ingrained in societal structures 
(Lambrecht & Tucker, 2019; Morse et al., 202 2; 
Teodurescu et al., 2021). Some studies have developed 
theoretical arguments concerning the emergence of 
feedback loops when ML models continually learn 
from the discriminatory outcomes they help ed to 
generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. endogenously help generate, can enhance 
performance, alleviate algorithmic discrimination, and 
boost the economic efficiency of ML models initially 
grappling with label imbalances in the original training 
dataset. The ability for self -correction inversely 
depends on the degree of initial label imbalance: the 
more biased the ML model ’s predictions are at the 
outset, the le ss capacity it has for recovery through 
continuous updating over time. Crucially, our research 
indicates that persistent discrimination by humans —
who make the final decisions in ML-supported tasks—
significantly impedes the self -correction abilities 
associated with the continuous updating of ML 
models. Our findings even suggest that ongoing human 
discrimination can lead ML models, originally trained 
on balanced data and demonstrating no algorithmic 
discrimination, to exhibit increasingly discriminatory 
behavior over time. 
5.2 Theoretical Contributions and 
Implications  
Our study responds to recent calls for empirical 
investigations into the sociotechnological nature of 
algorithmic discrimination in environments where ML 
models continue to learn based on data they help to 
generate (e.g., Kordzadeh & Ghasemaghaei, 2022;",11.5010922
What effect can human decision-makers have on the self-correction of discriminatory models when they are kept in the loop?,Human decision-makers in the loop may impede the self-correction of discriminatory models and can even cause initially unbiased models to become discriminatory over time.,"According to the text, human decision-makers can negatively impact the self-correction of discriminatory models when they are kept in the loop. Specifically, biases in human decision-making can directly influence the behavior of continuously updated ML models, and humans can interfere with the endogenous creation of new observations, further reducing the chance that the existing training data can be supplemented with observations enabling the model to make better predictions for disadvantaged groups.","discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by 
organizational decision makers. These measures could 
serve as essential complementary elements to ensure that 
continuously updated ML models demonstrate desirable 
behaviors when organizations implement regulatory 
requirements that involve humans in the decision loop. 
5.4 Limitations and Future Research  
Like any study, this study also has limitations—which, 
we believe , provide valuable directions for future 
research to enhance our understanding of how 
algorithmic discrimination evolves in environments 
where ML models undergo continuous updates. One the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” Journal of the Association for Information Systems 
 
824 
Indeed, self-correcting mechanisms may require time 
to take effect, during which disadvantaged social 
groups may continue to endure discriminatory 
practices. Rather, we perceive our findings as a 
testament to the substantial learning capabilities of 
modern ML models undergoing continuous updating. 
We argue that these capabilities can complement other 
interventions aimed at addressing algorithmic 
discrimination, such as the proper preprocessing of 
training data. This approach is particularly effective 
when organizations take measures to ensure that their 
employees do not engage in (subconscious) 
discriminatory practices during ML -supported 
decision-making processes. However, overreliance on 
the self -correcting aspects of continuous updating 
processes could cause significant damage to both an 
organization’s economic performance and reputation if 
ML systems are deployed hastily without rigorous ly 
testing for and addressing algorithmic discrimination.",9.994237185
What is a key challenge with opaque algorithmic designs in recruitment?,"They hinder the detection and correction of biases, making it difficult for HR teams to ensure equitable hiring.","According to the text, a key challenge with opaque algorithmic designs in recruitment is that they can amplify future hiring inequalities. The text states: ""Predicting future hiring outcomes by observing historical data can amplify future hiring inequalities."" This suggests that if the data used to train the algorithm reflects existing biases, the algorithm will perpetuate and potentially worsen those biases in its hiring decisions.","technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data
selection. Data collection tends to prefer accessible,“main-
stream” organizations unequally dispersed by race and
gender. Inadequate data will screen out groups that have
been historically underrepresented in the recruitment
process. Predicting future hiring outcomes by observing
historical data can amplify future hiring inequalities. Yarger
et al. (2019), Miasato and Silva (2019), and Njoto (2020)
propose that discrimination is due to the designer-induced
selection of data features.
3. The third theme is which algorithmic recruitment dis-
crimination exists. According to Correll et al. (2007), Kay
et al. (2015), O’neil (2016), Raso et al. (2018), Miasato and
Silva (2019), Langenkamp et al. (2019), Faragher (2019),
Ong (2019), Fernández and Fernández (2019), Beneduce
(2020), Jackson (2021), Yarger et al. (2023), and Avery et al.
(2023), when partial human data is provided to a machine,
so the algorithm is biased, it will eventually lead to the risk",9.836225033
"According to some studies cited, what are two benefits of AI-supported recruitment?",It can reduce costs and save time.,"According to the provided text, two benefits of AI-supported recruitment are:

1.  **Reduced costs**
2.  **Saves time**","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly",7.396245003
What are two recommended categories of measures to mitigate algorithmic bias in recruitment?,Technical measures (like unbiased dataset frameworks and improved transparency) and management measures (like internal ethical governance and external oversight).,"According to the text, the two recommended categories of measures to mitigate algorithmic bias in recruitment are:

1.  **Technical measures:** This includes things like “unbiased dataset frameworks and improved algorithmic transparency.”
2.  **Management measures:** This includes “internal corporate ethical governance and external oversight.”","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender Theme IV. How decreasing algorithmic recruitment
discrimination
Changes should be made at the technical and regulatory levels to
ensure that AI algorithms do not replicate existing biases or
introduce new ones based on the provided data (Raub,2018).
Building fair algorithms from a technical perspective
Constructing a more unbiased dataset. Unfair datasets are the root
cause of bias. Therefore, a direct approach to addressing algo-
rithmic bias is reconﬁguring unbalanced datasets. Using multiple
data points can yield more accurate results while carefully elim-
inating data points that re ﬂect past biases. However, this
approach incurs signiﬁcant costs (Bornstein,2018).
Another method is to correct data imbalances by using more
equitable data sources to ensure fair decision-making (36KE,
2020). Understanding the underlying structure of training data
and adjusting the signi ﬁcance of speci ﬁc data points during
training based on known latent distributions makes it possible to
uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold",8.424146175
"Besides gender and race, what other characteristic is mentioned as a basis for discriminatory hiring practices by AI?",Personality traits.,"According to the text, personality traits are also mentioned as a characteristic that can lead to discriminatory hiring practices by AI systems.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- Thirdly, concerns are raised regarding potential hiring
discrimination perpetrated by machines. This can arise from AI
tools, such as partial source data, or users unfamiliar with user
interfaces and operations.
Fourthly, intrinsic factors like personality and IQ, as well as
extrinsic factors like gender and nationality, have been observed
to inﬂuence the accurate identi ﬁcation and judgment of AI
systems concerning hiring discrimination.
Fifthly, respondents offer recommendations for combating
discrimination by machines, including technical and non-
technical approaches.
Recommendations for future studies
This study conducted a literature review to analyze algorithmic
recruitment discrimination’s causes, types, and solutions. Future
research on algorithmic recruitment discrimination could explore
quantitative analysis or experimental methods across different
countries and cultures. Additionally, future studies could examine
the mechanics of algorithmic recruitment and the technical rules
that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)",6.349862576
"According to the study of the paper 'Ethics and discrimination in artificial intelligence-enabled recruitment practices', what are two primary sources from which algorithmic bias stems?",Algorithmic bias stems from limited raw data sets and biased algorithm designers.,"According to the study, the two primary sources from which algorithmic bias stems are:

1.  **Limited raw data sets** lacking diverse representation.
2.  **Biased algorithm designers** – specifically, the selection of features or variables based on desired behavior.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not",9.303847551
What does the CRISP-DM User Guide provide that the Reference Model does not?,"The User Guide gives more detailed tips and hints for each phase and task on how to do a data mining project, whereas the Reference Model describes what to do.","The user guide provides templates developed during initial projects that proved helpful, especially for people not proficient with the data mining tool. The reference model does not offer these templates.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",6.738306999
Which task is mentioned as often being the lengthiest in the Data Preparation phase?,The 'Clean data' task.,"The text doesn't explicitly state which task is the longest in the Data Preparation phase. It mentions that tight deadlines created due to the generic CRISP-DM model led to sub-optimal solutions, but doesn’t detail the specific task that caused this issue.","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and G eneric
Tasks
Specialized
Tasks
C ontextC ontext
R eference M odel U ser G uide
• check lists
• questionaires
• tools and techniques
• sequences of steps
• decision points
• pitfalls
Phases
Process
Instances
Figure 1: Four Level B reakdow n of the C R ISP-D M  M ethodology for D ata M ining
The C R ISP-D M  m ethodology distinguishes betw een the Reference M odel and the U ser G uide.
W hereas the R eference M odel presents a quick overview  of phases, tasks, and their outputs, and
describes w hat to do in a data m ining project, the U ser G uide gives m ore detailed tips and hints
for each phase and each task w ithin a phase and depicts how  to do a data m ining project.
3.2 T he G eneric C R ISP-D M  R eference M odel
The C R ISP-D M  reference m odel for data m ining provides an overview  of the life cycle of a data
m ining project. It contains the phases of a project, their respective tasks, and their outputs.
The life cycle of a data m ining project is broken dow n in six phases w hich are show n in Figure 2.
The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase creative process w ith m any parallel activities. In our case, w e em ployed external service
providers for som e tasks. Therefore, there m ust be firm  deadlines for a task to be com pleted to
ensure tim ely com pletion and proper usage of resources. B ut w hen is a task com plete in a data
m ining project? This is a question that cannot be answ ered by a generic process m odel. This m ust
be addressed by the specialized process m odel. B ut even there, it is not trivial to com e up w ith
useful criteria. So far, w e have not yet a com plete, satisfactory and operational set of criteria in
our application.
In our first case studies, w e encountered an unexpected difficulty w ith the generic process m odel.
A lthough in the C R ISP-D M  docum ents it is stated at several places, that the phases and tasks are
not supposed to be strictly sequential, the clear and obvious presentation of the process m odel
inevitably created this im pression in our decision m akers. D espite our argum ents, w e found
ourselves forced to very tight deadlines, w hich in the end let to sub-optim al solutions. In our
case, this w as not a serious problem  because w e still learned enough about the problem  and about",8.056822062
What is the stated aim of the CRISP-DM process model?,"To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the text, the stated aim of the CRISP-DM process model is to be reliably and efficiently repeated by different people and adapted to different situations.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",6.135997534
What does CRISP-DM stand for?,Cross Industry Standard Process for Data Mining.,CRISP-DM stands for Cross Industry Standard Process for Data Mining.,"15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",4.476520538
What are the six phases of the CRISP-DM process model?,"The six phases are Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.","The six phases of the CRISP-DM process model are:

1.  Business Understanding
2.  Data Understanding
3.  Data Preparation
4.  Modeling
5.  Evaluation
6.  Deployment","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining D ata
U nderstanding
D ata
Preparation
M odelling
D ataD ataD ata
B usiness
U nderstanding
D eploym ent
Evaluation
Figure 2: Phases of the C urrent C R ISP-D M  Process M odel for D ata M ining
In the follow ing, w e outline each phase briefly:
• Business U nderstanding
This initial phase focuses on understanding the project objectives and requirem ents from  a
business perspective, and then converting this know ledge into a data m ining problem
definition, and a prelim inary project plan designed to achieve the objectives.
• D ata U nderstanding
The data understanding phase starts w ith an initial data collection and proceeds w ith activities
in order to get fam iliar w ith the data, to identify data quality problem s, to discover first
insights into the data, or to detect interesting subsets to form  hypotheses for hidden
inform ation.
There is  a close link betw een B usiness U nderstanding and D ata U nderstanding. The
form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be 15. Mai 2025
Overview CRISP-DM
9",6.897233963
What three characteristics define a General Purpose Technology (GPT)?,"They are pervasive, improve over time, and lead to complementary innovation.","You are correct. The text discusses the concept of GPT and its use, but it doesn’t offer a clear definition or detailed characteristics of a General Purpose Technology.","in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben. 17. Juli 2025
GPT
33
GPT Architecture
It is “straightforward” to build a GPT model…~100 lines of code would do it. But training it is another story! bestimmte Parameter an die Hand 
geben,
bestimmte Nebenbedingungen, wenn 
sie an Mikroökonomie,
an die Optimierungsprobleme 
denken.
Ich sage, ich möchte etwas 
optimieren.
Unter einer Nebenbedingung kann 
ich versuchen,
dieses System explizit in
eine bestimmte Richtung zu 
drängen.
Wenn Sie an Chat GPT denken,
was haben wir hier anscheinend
für diskriminierende oder biased 
tendencies?
Haben Sie da eine Idee, was da 
anscheinend
ein bisschen in die eine 
Richtung gekippt ist?
Sehr viele positive Nutzer sagen 
ja,
das ist jetzt das akuteste 
Problem,
dass man die Maschinen darauf 
optimiert hat,
einfach zu allem zu sagen Ja,
super, das ist ein ganz toller 
Gedanke.
Da gab es ja dieses 
Modellupdate,
wo sogar gesagt wurde Ich 
glaube,
dass die gesamte Menschheit 
eigentlich Echsen,
Reptilien sind, die unterwandert 
worden sind oder
so und dann hat die Maschine 
teilweise gesagt So,
du hast ein sehr gutes Bild und
sehr tief durchdenkst du diese 
ganzen Muster,
die in der Gesellschaft lässt 
sich nichts vormachen
und das ist eine Sache,
das würde ich jetzt per se 
erstmal nicht,
wenn wir über GPT sprechen,
wenn sie es sehen, ist nicht per 
se Bias,
Es ist ein Fehlverhalten.
Es wurde in eine bestimmte 
Richtung",6.990912676
"In the paper 'THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES', by how much higher was the adjusted measure of Total Factor Productivity (TFP) compared to official measures at the end of 2017?",The adjusted TFP was 15.9% higher than official measures at the end of 2017.,"I understand you’re asking about the paper “THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES” and want to know the TFP increase compared to official measures at the end of 2017.

However, the provided context contains information about a J-Curve productivity pattern, Receiver Operating Characteristic (ROC) analysis, and a course on Machine Learning. It does not contain the answer to your question about the TFP increase at the end of 2017. I do not have the information needed to answer your question.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 3. Juli 2025
What then?
Consider technical performance metric that is independent of threshold
• Solution: Receiver Operating Characteristic
• Simulate thresholds from 0 to 1 and calculate TPR and FPR for every threshold value
• Plot every (TPR, FPR) tuple with TPR being on the y-axis and FPR on the x-axis
18",14.26393557
What did Robert Solow's productivity paradox state?,That one 'can see in the computer age everywhere but in the productivity statistics',Okay.,"Stelle sein.
Sie müssen also identifizieren 
welche
Daten welche Variablen sind 
eigentlich da?
Müssen Sie vielleicht neue 
Informationen erheben?
Sind bestimmte Informationen gar 
nicht zugänglich?
Und das Letzte kann tatsächlich 
und findet häufig statt,
dass man sagt Ich habe eine 
Riesenidee,
wie ich eigentlich Machine 
Learning nutzen kann.
Dazu brauche ich aber in einer 
First World ABCDE.
Und dann kommt der Betriebsrat 
und sagt ihnen Nee,
das können wir nicht machen, das 
funktioniert so nicht.
Und dann stehen sie dort und 
sagen Ja gut,
dann macht meine Modellidee,
um dieses Businessproblem zu 
lösen.
So wie ich es mir vorgestellt 
habe,
vielleicht gar keinen Sinn.
Deswegen befindet sich dann 
dieser Pfeil hier zurück.
Und Sie müssen natürlich sagen,
in der First Best World würde 
ich es gerne so haben.
Das sollte immer Ihr 
Ausgangspunkt sein.
Und dann werden Sie mit
der Realität konfrontiert und 
merken,
dass einige Sachen technisch 
einfach nicht da sind,
vielleicht noch nicht da sind.
Vielleicht bedarf es eben 
Änderungen
in der Art und Weise,
welche Informationen erheben 
werden müssen.
Es kann sein, dass Sie zu der 
Entscheidung kommen,
hier in diesem Data 
Understanding, dass Sie sagen 
Hey, This article was downloaded by: [141.2.113.143] On: 24 April 2025, At: 02:36
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA
Marketing Science
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org
Smart Green Nudging: Reducing Product Returns
Through Digital Footprints and Causal Machine
Learning
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian
Speicher, Oliver Hinz
To cite this article:
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian Speicher, Oliver Hinz (2024)
Smart Green Nudging: Reducing Product Returns Through Digital Footprints and Causal Machine Learning.
Marketing Science
Published online in Articles in Advance 08 Aug 2024
. https://doi.org/10.1287/mksc.2022.0393
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The
Author(s). https://doi.org/10.1287/mksc.2022.0393, used under a Creative Commons Attribution License: https://
creativecommons.org/licenses/by-nc-nd/4.0/.”
Copyright © 2024 The Author(s)
Please scroll down for article—it is on subsequent pages
With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations
research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning
opportunities for individual professionals, and organizations of all types and sizes, to better understand and use
O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",5.050953627
What are some examples of complementary investments required by General Purpose Technologies (GPTs) like AI?,"Business process redesign, co-invention of new products and business models, and investments in human capital.","Based on the provided text, here are some examples of complementary investments required by General Purpose Technologies (GPTs) like AI:

*   **Sensors and Connected Devices:** The text specifically mentions “new inputs: sensors, connected devices.”
*   **Data Pipelines and ML-Ops:** The text refers to “new ‘orgware’…data pipelines, ML-ops, AI-first processes.”
*   **Organizational Adaptation:** The core idea is that companies need to “adapt our organizations” – this implies changes to processes and potentially even structure.
*   **Investment in AI Agents:** The text suggests thinking about “KI Agenten” (AI agents) – this requires investment in developing and deploying these agents.

Essentially, GPTs like AI don’t operate in isolation; they require a surrounding ecosystem of supporting technologies and organizational changes to realize their full potential.","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind Studierende machen sich mit der Funktionsweise von generativen KI-Modellen wie GPT vertraut. Sie lernen, diese durch gezieltes Prompting für Managemententscheidungen einzusetzen und kritisch zu bewerten, insbesondere unter wirtschaftlichen und ethischen Gesichtspunkten.",15.7476449
"According to the text, what is the 'Productivity J-Curve' phenomenon?","It is a phenomenon where, as firms adopt a new General Purpose Technology (GPT), total factor productivity (TFP) growth is initially underestimated because resources are spent on unmeasured intangible capital. Later, measured productivity growth overestimates true productivity as the benefits from those intangibles are realized.","According to the text, the “Productivity J-Curve” phenomenon is a two-phase productivity pattern that emerges when investing in a General Purpose Technology (like GPT). Initially, productivity falls or stalls due to heavy up-front investment in data, skills, process redesign, and complementary tech. However, after a short-run dip, productivity rebounds and accelerates once the complementary investments start paying off. This is driven by learning effects, scale economies, and network externalities.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, mussten und überall Kolben die 
Energie
eigentlich weiterbringen 
mussten.
Das heißt,
die Dampfmaschine hat nicht nur 
eine Effizienz,
die Elektrizität, als wir damit 
angefangen haben,
hat nicht nur eine effizientere
Energieproduktion geschaffen,
sondern sogar wenn wir dem
Faktor Produktion Beispiel 
bleiben,
hat es ganze Fabriken im Design 
verändern.
Das hat dazu geführt, dass wir 
länger
die Straßen beleuchten konnten.
Das wiederum hat Effekte darauf 
gehabt,
wie viel wir bauen konnten,
wie lange wir unterwegs sein 
können,
was Mobilität erhöht hat usw usw
alles ausgehend auf dieser 
Kerninnovation,
die wir haben oder hatten damals 
nämlich Elektrizität.
Ja, jeder nimmt Elektrizität 
jetzt verkantet
einfach hier an den Stecker 
gehen
und Elektrizität Energie ziehen.
Und das ist natürlich ein 
massiver Treiber von Wachstum,
von Produktivität.
War aber nicht immer so.
Am Anfang hat es super lange 
gedauert,
bis diese Elektrizität irgendwas
gebracht hat und die
Frage werde ich ihn gleich 
stellen,
Wie lange sie denn glauben,
dass es gedauert hat, wenn wir 
auf der nächsten
Slide zu den JWs sind
auch wieder mikroökonomisch 
motiviert?
Ja, alles. Aber lassen Sie uns
ganz kurz noch mal darüber 
nachdenken.",11.24690413
"What societal benefit, in terms of CO2 emissions, would a 2.6% return decrease translate to based on U.S. figures from 2022?","It would result in a reduction of 624,000 metric tons of CO2 emissions.","Based on the provided text, a 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions.","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation 24. April 2025
Summary and impact: what ML does
https://www.readycloud.com/info/14-facts-about-ecommerce-product-returns
up to 4.6% decrease
•Our green nudges can be easily implemented at low costin all online shops •Our ""smart"" implementation uses open-source libraries and may predict impact on individual-level profit of any nudge•Our return reduction on global scale would save…•…up to 200,000 tons of CO2 emissions •…up to 120,000 tons of waste
Annual impact for industry partner: in the six figures",9.020157099
What is the estimated annual savings in processing costs for the industry partner due to the green nudge?,"The effect corresponds to annual savings in processing costs of approximately $340,000.","The estimated annual savings in processing costs for the industry partner due to the green nudge is approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than annually. This amount is roughly equivalent to the 
annual emissions from the electricity consumption of 
121,000 U.S. homes.
3
Notably, the actual savings in 
emissions are likely to be even higher both as the vol -
ume of returns continues to grow further (Calma 2023 ) 
and because emissions through packaging and the pro -
duction process of discarded goods are not included.
From a business perspective, according to internal 
figures of our industry partner, the estimated annual 
profit increase quickly amortizes costs associated with 
the development, implementation, and maintenance of 
green nudging. In fact, the developed dual nudge 
turned out to be so successful that our industry partner 
opted to keep the green nudge running in the German 
online shop. The project’s success is affirmed by a prac -
tice executive who corroborates the effectiveness of the 
implemented dual-nudge approach by stating that
[t]he reduction in returns without a negative impact 
on revenue was huge for us. So huge, in fact, that we 
opted for keeping the experiment live and showing 
the successful treatment to all of our German online 
customers. —Practice partner executive
Apart from the immediate business benefits through effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge",7.406930923
"In the 'Smart Green Nudging' paper, what was the sample size (n) of the randomized field experiment discussed?","The experiment had a sample size of 117,304 participants (n = 117,304).","The text doesn't explicitly state the exact sample size (n) of the randomized field experiment. It refers to the experiment as a ""large-scale"" one and describes the treatment policy being applied to ""40.7% of the test data"" and ""59.3% of the test data"".","The rest of the paper is structured as follows. Section 
2 provides the theoretical background by reviewing 
related literature. Section 3 presents the setting and 
results of our randomized field experiment. Section 4
details the configuration and results from our causal 
machine learning model, along with the evaluation of 
smart green nudging. Section 5 concludes by discuss -
ing the practical implications of our work, its limita -
tions, and potential avenues for future research.
2. Related Literature
Our work relates to three streams of literature, namely 
(i) product returns, (ii) nudging, and (iii) treatment het -
erogeneities and causal machine learning.
2.1. Product Returns
The surge in product returns and the corresponding busi -
ness and environmental repercussions have prompted 
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
2 Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 
Downloaded from informs.org by [141.2.113.143] on 24 April 2025, at 02:36 . For personal use only, all rights reserved. significant variability in how individuals respond to 
nudging (Hummel and Maedche 2019 ), rendering the 
outcomes of green nudging unpredictable (Lehner et al. 
2016 ). Additionally, few researchers have explored the 
impact of green nudges on a large scale (see He et al. 
2023 for an exception), leaving a gap in our understand -
ing and a need to reconcile these divergent findings.
We extend prior research by conducting a large- 
scale, randomized field experiment to test the effective -
ness of green nudges in reducing product returns. Our 
green nudge effectively leverages customers’ desire to 
sustain a positive self-image through environmentally 
friendly behavior (Schubert 2017 ). It heightens aware -
ness of the negative environmental impact linked to 
returning products during both the prepurchase and 
postpurchase phases of the customer journey.
2.3. Treatment Heterogeneities and Causal 
Machine Learning
There often exists a notable heterogeneity in customer 
reactions to marketing strategies (El Kihal and Shehu 
2022 ), especially nudging techniques (Lehner et al. 
2016 , Mirsch et al. 2017 , Hummel and Maedche 2019 ). 
Accordingly, relying solely on average treatment vertical axis) as we vary the policy π by applying the 
intervention to different shares of the test data (on the 
horizontal axis). In essence, the curve shows the overall 
efficacy of smart green nudging as we progressively 
include the top 10%, 20%, and 30% of customers who 
the CML model predicts to achieve the largest reduc -
tions in returns. A random administration of the green 
nudge to an equally sized share (the dashed line in 
Figure 4 ) would result in a linear decrease from the 
level of the control group (no customers treated) to that 
of naive green nudging (all customers treated). The 
Figure 3. (Color online) Group Average Treatment Effects 
(ATEs) by Treatment Policy 
Notes. We depict the group average treatment effects of the dual 
nudge on returns (shipment) measured in the test data. The left bar 
presents the ATE for the group with π(x
i
) � 0 (40.7% of the test data), 
whereas the right bar presents the ATE for the group with π(x
i
) � 1 
(59.3% of the test data). The vertical lines represent the 95% confi -
dence intervals.
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 11",9.897112131
What are the two types of returns mentioned in the context of online shopping?,'Real returns' (where the customer intended to keep the item) and 'opportunistic returns' (where the customer orders multiple options with the intent to return some).,"According to the context, the two types of returns mentioned are:

1.  **Returns arising from discrepancies between expectations and reality** (often referred to as returns due to dissatisfaction).
2.  **Returns related to the environmental impact of returns** (the “green nudging” approach).","Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation marketing researchers and practitioners to examine dri -
vers of product returns (e.g., Harris 2008 , Pei and Paswan 
2018 ) and to seek solutions to curb them (P˚ alsson et al. 
2017 , Abdulla et al. 2019 ). Particularly problematic are 
opportunistic return behaviors. An example for such 
behavior is the practice of “bracketing,” where customers 
purchase multiple variants of an item with the intention 
of returning some (Altug et al. 2021 ). A plausible initial 
approach to address this issue is the adaptation of return 
policies. The literature differentiates between lenient poli -
cies, such as “no-questions-asked” full money-back guar -
antees, and more restrictive ones that, for example, 
impose restocking fees or nonrefundable handling 
charges (Su 2009 , Nageswaran et al. 2020 ). Although for -
going lenient policies can decrease returns (Shehu et al. 
2020 ), they also pose business drawbacks. For example, 
money-back guarantees can serve as quality signals 
(Moorthy and Srinivasan 1995 , Altug et al. 2021 ) to boost 
sales and profits (Davis et al. 1995 ). These business ramifi -
cations call into question the efficacy of exclusively rely -
ing on restrictive return policies from a practitioner’s 
standpoint. returns arising from discrepancies between expecta -
tions and reality (Zhou and Hinz 2016 ). Examples of 
customer-based interventions include virtual try-on 
sessions, customer reviews, and online consultations 
(Minnema et al. 2016 ). In essence, monetary and proce -
dural interventions operate in the postorder phase, 
whereas customer-based instruments work before and 
during the order process (Walsh and M ¨ohring 2017 ). 
Accordingly, monetary and procedural interventions 
are rather reactive measures, trying to motivate indivi -
duals to keep products that have already been pur -
chased. By contrast, consumer-based instruments are 
preemptive, aiming to avoid purchases of products 
that are highly likely to be returned.
Our study contributes to the literature on product 
returns by introducing a simple intervention that high -
lights the environmental impact of returns to custo -
mers. This nudging approach, a customer-based 
instrument, is distinct in its minimally invasive, low- 
cost nature, seeking simply to inform customers about 
the repercussions of their postpurchase actions. Closely 
related to our research, Ghose et al. ( 2023 ) recently 
assessed the effectiveness of pressure-oriented and self-",8.860440254
"According to the paper, by how much did the smart green nudge decrease product returns compared to having no nudge at all?",The smart green nudge resulted in a 6.7% decrease in product returns compared to no nudge at all.,"According to the paper, the smart green nudge decreased product returns by 5.19% compared to having no nudge at all.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than our test data and report the results in Table 4 .
Table 4 shows return shipments, the return value, 
the conversion rate of the viewed carts, and (gross) 
sales. Additionally, we depict the “Share treated” col -
umn presenting the proportion of customers subjected 
to green nudging.
We observe a pronounced improvement in effective -
ness of smart green nudging in reducing returns; com -
pared with the naive approach of nudging all 
customers, smart green nudging can approximately 
double the impact, curbing returns by an additional 
3.15%. When compared against no nudging, the reduc -
tion is 5.19%. This decline in returns is not only mean -
ingful from a business standpoint but also holds 
considerable ecological weight.
Although the results from Table 4 look promising, 
we now follow Athey et al. ( 2023 ) and examine varying 
shares of treated customers with accompanying confi -
dence intervals for a more thorough examination. 
Figure 4 presents the mean returned shipments (on the 
vertical axis) as we vary the policy π by applying the 
intervention to different shares of the test data (on the 
horizontal axis). In essence, the curve shows the overall 
efficacy of smart green nudging as we progressively effective for customers who would otherwise return at 
an above-average level (see Table 8 in Online Appendix 
A.8). Conversely, for customers who would typically 
return less than average, green nudging seems to back -
fire. A possible reason is that green nudging may unin -
tentionally remind these customers of the return 
option, thereby inadvertently leading to more returns.
The analysis of group average treatment effects indi -
cates the CML model’s effectiveness in identifying het -
erogeneous treatment effects on unseen test data, and 
thus, it underscores its utility as an effective tool for 
personalization. However, the degree to which a CML- 
based “smart green nudging” policy amplifies the over -
all decrease in returns warrants further investigation, 
which we address in the subsequent analysis.
4.2.2. Gain from Smart Green Nudging. How effec -
tively does the “smart” CML-based policy reduce 
returns? We explore the overall effectiveness by evalu -
ating the IPS estimator from Equations ( 3 ) and ( 4 ) in 
our test data and report the results in Table 4 .
Table 4 shows return shipments, the return value, 
the conversion rate of the viewed carts, and (gross) 
sales. Additionally, we depict the “Share treated” col -",7.262770176
"In the 'Smart Green Nudging' study, which variable was identified by SHAP values as the most pivotal for the prediction?",The initial cart value.,"According to the paper, the initial cart value was identified by SHAP values as the most pivotal variable for the prediction.","contribute to the predictions (Senoner et al. 2022 ). In 
our case, SHAP values indicate that the initial cart 
value is the variable most pivotal for the prediction. We 
report SHAP values from our CML model in Online 
Appendix A.10. However, it is crucial to differentiate 
between variables that predict the treatment effect and 
those that act as causal moderators. We encourage 
future research to rigorously investigate the causal role 
of variables in moderating the effectiveness of green 
nudging.
Our study also underscores the effectiveness of using 
customer carts and digital footprint data to personalize 
marketing interventions. Our analyses reveal multiple 
characteristics that induce heterogeneous treatment 
effects, including the type of internet browser, the inter -
net provider, the federal state, and the weekday.
7
For 
example, weekend shoppers are more effectively 
nudged than weekday shoppers, possibly because of 
lower stress levels, aligning with similar literature find -
ings (Grahl et al. 2023 ). Similarly, the internet browser 
and other characteristics of the digital footprint can 
hint at underlying customer traits and demographics 
(Berg et al. 2020 ). Incorporating these insights, practi - task. In the following, we show SHAP values for initially biased and unbiased ML models. Each figure contains 
information on the overall importance of each feature for the prediction (higher position in the vertical ranking) and 
the features’ marginal effects for predictions about individual instances (position on the x-axis). Each dot represents a 
SHAP value for a feature and an instance. The color of the dots depicts the value of the feature from low (blue dots) 
to high (red dots). Plots jitter identical SHAP values vertically, providing insights into the SHAP value distribution per 
feature. Overall, Figures C1 and C2 show the relationship between the features (borrower characteristics) and the 
prediction (likelihood of this person mak ing a repayment) by revealing their marginal contribution to predictions (in 
terms of probability) conditional on feature values. 
 
Figure C1. SHAP Values, Example of Biased ML Model (max. initial label imbalance) 
The figure for the biased ML reveals that prior to any retraining, the gender attribute (0 = female, 1 = male) is the most 
relevant feature to predict whether a borrower will make a repayment. Our findings indicate that being female always 22. Mai 2025
Shapley scores: weighted average
•Local accuracy: attributions sum (exactly) to the prediction difference•Consistency: if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.•Missingness: unused features get SHAP value of 0 x*=0→𝜙+•Additivity: SHAP values can be added to show the joint contribution of several features to a prediction
22
𝜙%𝑓,𝑥=	2&!⊆	)!𝑧*!𝑀−𝑧*−1!𝑀! (𝑓)𝑧*−𝑓)𝑧*\i)Shapley value for i= savings
Model f
For individual x with a specific combination of features
Weighted sum over allSubsets z’of (transformed) data point x; whereM is total number of features in full set
Difference inModel prediction
𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛!=𝐸(-𝑓𝑋)	+	3""𝜙!",6.375657082
"In the field experiment from the 'Smart Green Nudging' paper, what was the initial aim of the academic team members regarding the prompts they were testing?",The initial aim was to isolate the effects of the cart prompt and the reminder prompt.,"According to the paper, the initial aim of the academic team members was to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects.","Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con -
strained our options, permitting only the testing of one 
of the design elements in isolation. Ultimately, our 
industry partner opted to test the prompt displayed 
after consumers finished their purchasing. This choice 
was based on the belief that the reminder displayed 
after the purchase posed a lower risk of negatively 
impacting sales.
3.1.3. Study Details. We conducted a large-scale, ran -
domized field experiment over seven weeks in our 
industry partner’s German online shop to test the dual- 
nudge and single-nudge interventions.
2
The study ran -
domly assigned website visitors using stationary 
devices to one of three groups upon their first view of 
the shopping cart: the control group (no nudging), the 
single-nudge group (reminder prompt), or the dual- 
nudge group (cart and reminder prompt). During the 
study, millions of visitors browsed the website. Our 
randomization checks (see Table 5 in Online Appendix 
A.2) confirmed the success of the randomization pro - decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were 
recruited to participate in an ongoing field study. The study 
was conducted by a subset of this paper ’s authors and 
aimed to examine correlations between academic 
performance, personal characteristics, and behavioral 
measurements, including outcomes of sequential social 
dilemmas such as the investment game. The study, 
accessible via a link sent to students ’ email addresses, 
comprised a comprehensive survey on personal 
demographics, socioeconomic background, cognitive 
abilities, personality traits, and, crucially for our 
simulations, a sequential social dilemma game 
incorporating our previously described investment game (a 
detailed description of this game can be found in Appendix 
B.1). Participants’ decisions were incentivized, allowing 
for the measurement of revealed preferences, an approach 
superior to simple self -reported measures due to its 
potential to reduce presentation and demand effects 
(Camerer and Hogarth, 1999). The incentives involved 
earning real money based on the game outcomes with other Was sind das? Was abschneiden?
Testdaten? Ja. Sie wollten 
sagen, nächstes Mal.
Unterscheiden zwischen Trainings 
und
Testdaten. Aber Sie haben 
vollkommen recht.
Es sind trotzdem historische 
Daten.
Aber diese Testdaten.
Daher kommt der Name.
Die geben uns zumindest eine 
Idee,
wie gut dieses Modell 
funktionieren sollte.
Angenommen.
Der grundlegende Prozess 
verändert sich nicht.
Da war noch eine Frage. Hier 
irgendwo.
Genau da. Will ich alle 
Informationen reinschreiben?
Könnte auch.
Wenn man die manchmal sehen 
könnte,
kann es ja die Interaktionen 
zwischen den
Relationen gar nicht 
einschätzen.
Das schon. Das ist das Coole an 
Machine Learning.
Sorry, wenn ich jetzt happy bin,
aber das ist eins der 
Kernsachen.
Neuronale Netze haben
eine inhärente Struktur 
beispielsweise oder
andere Modelle wie Random 
Forest,
aber auch Gradient Boosting 
Methods.
Die haben eine inhärente
Möglichkeit, Nichtlinearitäten 
zu lernen,
ohne dass ich sie explizit 
spezifiziere,
indem ich einfach genug 
Beispiele reinschmeiße.
Aber es kann ja nicht für die 
Zukunft sagen,
wie genau diese Informationen 
genau wiedergegeben werden,
In einer. In einer.
Wir reden jetzt super 
theoretisch,
wenn ich nur die Mathematik
dahinter angucke und Machine",7.741206646
"In the 'Smart Green Nudging' paper, what were the estimated annual savings in processing costs for the industry partner as a result of the green nudging intervention?","The estimated annual savings in processing costs were approximately $340,000.","According to the paper, the estimated annual savings in processing costs for the industry partner as a result of the green nudging intervention were approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge annually. This amount is roughly equivalent to the 
annual emissions from the electricity consumption of 
121,000 U.S. homes.
3
Notably, the actual savings in 
emissions are likely to be even higher both as the vol -
ume of returns continues to grow further (Calma 2023 ) 
and because emissions through packaging and the pro -
duction process of discarded goods are not included.
From a business perspective, according to internal 
figures of our industry partner, the estimated annual 
profit increase quickly amortizes costs associated with 
the development, implementation, and maintenance of 
green nudging. In fact, the developed dual nudge 
turned out to be so successful that our industry partner 
opted to keep the green nudge running in the German 
online shop. The project’s success is affirmed by a prac -
tice executive who corroborates the effectiveness of the 
implemented dual-nudge approach by stating that
[t]he reduction in returns without a negative impact 
on revenue was huge for us. So huge, in fact, that we 
opted for keeping the experiment live and showing 
the successful treatment to all of our German online 
customers. —Practice partner executive
Apart from the immediate business benefits through",8.259586334
Where is INFORMS located?,"INFORMS is located in Maryland, USA.","INFORMS is located in Maryland, USA.","This article was downloaded by: [141.2.113.143] On: 24 April 2025, At: 02:36
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA
Marketing Science
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org
Smart Green Nudging: Reducing Product Returns
Through Digital Footprints and Causal Machine
Learning
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian
Speicher, Oliver Hinz
To cite this article:
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian Speicher, Oliver Hinz (2024)
Smart Green Nudging: Reducing Product Returns Through Digital Footprints and Causal Machine Learning.
Marketing Science
Published online in Articles in Advance 08 Aug 2024
. https://doi.org/10.1287/mksc.2022.0393
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The Informationen zur Vorlesung: Gehalten am Do. 12:00 bis 16:00; wöchentlich vom 24.04.2025 bis 24.07.2025; Ort: Hörsaalzentrum Westend - HZ 12 DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org.",4.156997442
What programming language will be used in the course?,Python.,The course will use the programming language Python.,"24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen. 23. Juli 2025
Yourhelpisappreciated
2
Survey on LearnMateChatbot for this course",4.355662823
"According to the text, what are four useful applications of Large Language Models (LLMs) in everyday life?","Four useful applications of LLMs are: synthesizing information (such as summarizing and translating), acting as an argumentative sparring partner to identify weak points in arguments, brainstorming creative ideas, and assisting with code-related tasks like documentation and translation between programming languages.","The text doesn’t list four specific “everyday life” applications of Large Language Models (LLMs). However, it does highlight these uses:

1.  **Synthesizing information:** Summarizing and translating blocks of information.
2.  **Argumentative sparring partner:** Pointing out weak points in arguments.
3.  **Brainstorming:** Co-creation of creative ideas.
4.  **Code documentation or transformation:** Generate docs for software and translate across languages.

It also mentions that the willingness of users to interact with systems is central to how a technology can influence social life, economic processes, and productivity.","23. Juli 2025
Useful in everyday life
20
•Synthesizing information: Summarizing and translating blocks of information, e.g., bullet points, into structured text•Argumentative sparring partner: pointing out weak points in arguments•Brainstorming: co-creation of creative ideas•Code documentation or transformation: generate docs for software and translate across languages Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. gemacht hat,
das User Interface war
die reine Technologie dieser 
großen Sprachmodelle.
Die gibt es theoretisch schon 
seit 2017.
Die wurden natürlich immer 
besser.
Wir hatten am Ende die Version 
drei Punkt irgendwas,
was dann integriert wurde.
Aber es wurde integriert.
Dieses fundamentale KI Modell in
eine ausreichend kompatible und
nutzerfreundliche Umgebung,
nämlich Chat GPT, was
noch mal das Grundmodell GPT
plus weitere Systeme sind, die 
hier zusammenspielen.
Das heißt Forgeat GPT.
Gab es diese großen 
Sprachmodelle schon?
Die haben sich wie gesagt immer 
weiter verbessert.
Grundsätzlich haben
wir aber alle erst damit 
angefangen und
haben eine ganze Industrie 
eigentlich damit angefangen,
es kommerziell zu benutzen.
Ab dem Moment, wo dieses
User Interface richtig 
hinbekommen wurde
und die User des Systems 
hochgegangen ist.
Das ist eine der Kern messages
die Bereitschaft des Nutzers,
mit den Systemen zu 
interagieren.
Generell, wo wir
häufig von Adoption reden 
Technologie use.
Wenn wir ganz, ganz breit sind.
Das ist zentral dafür,
wie eine Technologie tatsächlich
letztlich Einfluss haben kann 
auf unser Sozialleben,
auf unsere ökonomischen 
Prozesse,
auf Unternehmen
und letztlich natürlich auf 
Produktivität.",12.85681272
For what types of tasks are basic chat models the most suitable choice?,Basic chat models are most suitable for low-stake tasks where a fast response is required.,"According to the text, basic chat models are the most suitable choice when a task is “rather low stake and fast response is required.”","23. Juli 2025
Many models, but which one should you use?
18
Reasoning model: Choose when task is rather complex, results are important, and time to receive response
Basic chat model: Choose when task is rather low stake and fast response is required.
Example 1: •Which vegetable has the highest vitamin C amount?•Act as a nutritionist and create a vegetable salad with at least 500𝜇𝑔vitamin C and 200𝜇𝑔vitamin D.Example 2: •What is an LLM?•Act as a computer scientist and explain to me what the advantage of a reasoning model over a chat model is. 15. Mai 2025
Recap: a task-based approach
4
Task 1Task 2... Task N-1 Task N
Workflow
Decomposition of workflowsto identify suitable individual tasks that AI can perform•Evaluate AI applicability based on task attributes (routine, analytical, standardized)•Identify (hidden) prediction problemsin tasks, e.g., customer needs, optimal ad textRethink processesand imagine what AI systems can help with•Installation of individual AI often entail further changes, e.g., new tasks for humans•Understand what is necessary for AI systems to work properly•Notion of complements and substitutesin organizational structures 23. Juli 2025
Give examples whenever possible (few-shot)
13
Humans often rely on previous examples to work on tasks•Examples can also be useful for LLMs (example = shot)•Help show LLMs the structure of desired output•Zero shot-prompting may be ineffective if response needs to be very specificIf you want specific styles and formats try giving examples•Add example(s) as part of prompt•Instruct LLM to follow examples•Beware too many examples!",5.793082714
"According to the document, what is the population of Munich?","1,578,567","According to the document, Munich’s population is 1,578,567.","23. Juli 2025
Human v. LLM ”Thinking”
7
Is the following statement true? “Munich’s population is 2.12 times that of Frankfurt.”A potential Human (cognitive) process:• Think about knowing these numbers; most likely not• Use Wikipedia tool: get both of these populations•Munich: 1,578,567•Frankfurt: 773,068• Try to divide the numbers in your head; rough idea bit not exact number• Use calculator tool: 1,578,567/ 773,068 = 2.042• Reflection about correctness of result; number sounds reasonable• Forming an answer• Start writing an answer: “Yes, the statement is correct.”• Checking the entire solution process, the answer sentence and maybe rewrite it to “The statement is true.”
This is not how LLMs “think”! decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were 
recruited to participate in an ongoing field study. The study 
was conducted by a subset of this paper ’s authors and 
aimed to examine correlations between academic 
performance, personal characteristics, and behavioral 
measurements, including outcomes of sequential social 
dilemmas such as the investment game. The study, 
accessible via a link sent to students ’ email addresses, 
comprised a comprehensive survey on personal 
demographics, socioeconomic background, cognitive 
abilities, personality traits, and, crucially for our 
simulations, a sequential social dilemma game 
incorporating our previously described investment game (a 
detailed description of this game can be found in Appendix 
B.1). Participants’ decisions were incentivized, allowing 
for the measurement of revealed preferences, an approach 
superior to simple self -reported measures due to its 
potential to reduce presentation and demand effects 
(Camerer and Hogarth, 1999). The incentives involved 
earning real money based on the game outcomes with other 15. Mai 2025
Accurate predictions are often not enough
13
Laws against discrimination in many domains includingCredit (Equal Credit Opportunity Act),Education (Civil Rights Act of 1964; Education Amendments of 1972), Employment (Civil Rights Act of 1964), Housing (Fair Housing Act), Public Accommodation (Civil Rights Act of 1964)
§Born and raised in Germany§90% of population§data describes them accurately §accurate predictions (95% accurate)
§Migrated to Germany in past years§10% of population§data describes them poorly§poor predictions (50% accurate)
90.5% accurate prediction about creditworthiness
Consider an example where machine learning model predicts individual creditworthiness
ML model",5.795505762
How many parameters did GPT-2 have?,1.5B parameters.,"According to the document, GPT-2 had 1.5B parameters.","17. Juli 2025
Evolution of OpenAI’s GPT modelsGPT:117M parameters (Radford et al., 2018)•Transformer decoder with 12 layers•Trained on BooksCorpus: over 7000 unique books (4.6GB text)GPT-2:1.5B parameters (Radford et al., 2019)•Same architecture as GPT but more flexible•Also: trained on much more data: 40 GB of internet text dataGPT-3:175B parameters (Brown et al., 2020)•Over 600GB of text dataGPT-4:1.76T parameters (undisclosed)•Mixture of Expert: different expert models with >200B parameter each•10 trillion words ~ 40 Billion pages (exact size unknown)
35 17. Juli 2025
GPT training
34
Source: State of GPT | BRK216HFS
Stage 1Stage 2 Stage 3 17. Juli 2025
GPT
33
GPT Architecture
It is “straightforward” to build a GPT model…~100 lines of code would do it. But training it is another story!",5.172946215
What is the intuition behind how an 'attention head' works in a language model?,"An attention head represents a word as a weighted combination of itself and all the words that came before it. The weights in this combination are based on the similarity between the words, which helps the model understand the context of the sentence.","Okay, let's break down the intuition behind ""attention heads"" in language models, drawing on the context provided and general understanding.

**The Core Idea: Focusing on What Matters**

Essentially, an attention head is a mechanism that allows the model to *dynamically* focus on the most relevant parts of the input sequence when processing each word. It’s like a spotlight, highlighting the words that are most important for understanding the current word.

**Here's how it relates to the context and the intuition:**

1. **Beyond Simple Sequential Processing:** Traditional recurrent neural networks (RNNs) processed words sequentially, often struggling with long-range dependencies (i.e., how words far apart in a sentence relate to each other). Attention heads solve this by considering *all* words in the input simultaneously.

2. **Weighted Relationships:** Each word in the input sequence is assigned a ""weight"" representing its importance to the current word being processed.  Think of it like this:
   *  If ""cat"" is being processed, the attention head might assign high weights to words like ""fluffy"" and ""pet"" because they are strongly related to the concept of a cat.
   * Conversely, it might assign a lower weight to words like ""the"" or ""and"" which are less directly relevant.

3. **Multiple ""Heads"" for Diverse Perspectives:** The ""head"" part is crucial. A language model doesn't just have *one* attention mechanism. It has *multiple* attention heads. Each head learns to focus on different aspects of the input. 
   * One head might focus on grammatical relationships.
   * Another might focus on semantic relationships (meaning).
   * A third might focus on stylistic elements.

4. **Parallel Processing:** Because attention heads are operating in parallel, the model can efficiently capture complex relationships within the input sequence.

**Connecting to the Context's Themes:**

* **Human-Machine Collaboration:** The discussion about ""human in the loop"" and the role of the user in systems like ChatGPT highlights how attention heads allow the system to intelligently prioritize information based on user preferences – a crucial element of collaboration.
* **Understanding User Input:**  The example of asking ChatGPT ""where can I get a good vacation?"" demonstrates how attention heads can weigh different aspects of your request (e.g., ""good,"" ""vacation,"" ""preferences"") to generate a relevant response.

**In short, attention heads are a powerful tool that allows language models to understand the nuances of language by dynamically focusing on the most important parts of the input.**

---

**Disclaimer:** *I am an AI and my understanding is based on the provided text and general knowledge. If you'd like a more in-depth explanation, I recommend exploring resources on the Transformer architecture.*","des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses die Entscheidungsgewalt haben,
der Mensch in diesem Loop
würde nie auf die Maschine 
hören.
Alle möglichen Effizienzgewinne, 
die wir hätten,
werden gar nicht realisiert.
Es kommt immer auf die Nutzung 
anderer
will ich sie heute 
sensibilisieren.
Und da gibt es eine 
Riesenliteratur zu
ganz viel aktive Forschung,
wie wir eigentlich erreichen 
können,
dass Menschen vernünftig mit
diesen Maschinen interagieren.
Richtig hinterfragen,
welche Informationen sie wie 
gewichten sollen.
Und wenn Sie jetzt mal an Chat 
GPT
denken und andere große 
Sprachmodelle,
wo Sie der Nutzer sind Sie 
machen Eingaben,
Sie kriegen Vorschläge an 
Ausgaben.
Wir sind noch nicht bei KI 
Agenten,
wo ich sage ich möchte einen 
Urlaub gebucht haben.
Klick macht das alles, sondern 
diese Systeme.
Sie können hingehen und fragen 
wo
kann ich guten Urlaub machen?
Dieses und jenes sind meine 
Präferenzen.
Eine solche und solche Art von 
Urlaub möchte ich machen.
Und dann kriegen Sie einen 
Vorschlag von diesem System.
Ihre Rolle dabei ist dann 
letztlich zu entscheiden
Ist diese Information gut oder 
schlecht?
Ist es eine Information,
die Sie nutzen sollten in
Ihrer Entscheidungsfindung oder 
nicht?
Und da sehen wir mehr und mehr,
dass ich eben diese Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",41.63568568
What is Word2Vec and what is its function?,Word2Vec is a simple neural network that is used to generate word embeddings from a text corpus.,"Based on the provided context, here’s what Word2Vec is and its function:

**What is Word2Vec?**

Word2Vec is a simple neural network that generates word embeddings from a text corpus. The core idea is to predict words from their surrounding context.

**What is its function?**

Its function is to create a fixed dictionary of vectors (embeddings) for words. Words that appear in similar contexts will have similar embeddings. This allows you to represent words as numerical vectors, which can then be used in various natural language processing tasks.

**Key aspects from the text:**

*   It predicts words based on the words around them.
*   Learned weights become the word embeddings.
*   It creates a large dictionary of vectors (often with dimensions of 100 or more).
*   It uses techniques like negative sampling to speed up training.","17. Juli 2025
Learning word embeddings with Word2VecWord2Vec: simple neural networkto generate word embeddings given a text corpus•Idea: predict words from their surrounding context•Learned weights are embeddings•Through optimization: words in similar context have similar embeddings•Gives us a fixed dictionary of vectors for words we can utilizeIntuitive example: assume we have a text corpus with two sentences
15
Training dataPizza is awesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layer Output layer 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
Word2Vec example (continuous bag of words version)
16
Training dataPizzaisawesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1001
Words àOne-hot Vector One-hot VectoràWords0100
Training dataPizza is awesomePizzaisgreatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1010
Words àOne-hot Vector One-hot VectoràWords0100",14.7880559
What is one potential problem with representing words as indexed tokens?,Lacking interpretability of distance.,"According to the text, a potential drawback of representing words as indexed tokens is “Lacking interpretability of distance.” The text suggests using one-hot encoding to address this issue.","17. Juli 2025
TokenizationMost simple view: words are discrete and independent tokens•Represent words as indexed tokens
What is a potential problem here?•Lacking interpretability of distance •Idea: Employ one-hot-encoding so that distances are all the same
10
WordIndex tokenHello 0… …King 145Queen156… …
WordIndex tokenOne-hot vectorHello 0 [1,0,…,0,0,…,0]… … …King 145 [0,0,…,1,0,…,0]Queen156 [0,0,…,0,1,…,0]… … … 23. Juli 2025
Current LLMs
5
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token
Input sentence 17. Juli 2025
Current LLMs
28
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token",7.080343246
"According to the gradient descent, what two steps are taken to decrease a model's error during training?","To decrease the error, the model's weights (w) are changed in the direction of the negative slope, and the size of this change is scaled by the learning rate (η).","According to the text, the two steps taken during gradient descent to reduce a model’s error during training are:

1.  Compute gradients: ℒ,-, ,ℒ,.(through back-propagation algorithm)
2.  Update weights and biases: 𝑤=𝑤−𝜂⋅,ℒ,-, 𝑏=𝑏−𝜂⋅,ℒ,.","10. Juli 2025
Gradient descent: the optimization algorithm
27
We solve the following problem during training
Complexity of problem requires a step-wise solving using gradient descent:1.Initialize 𝑤,𝑏randomly2.Loop until convergence of loss1.Compute gradients: ,ℒ,-, ,ℒ,.(through back-propagation algorithm)2.Update weights and biases: 𝑤=𝑤−𝜂⋅,ℒ,-, 𝑏=𝑏−𝜂⋅,ℒ,.3.Compute new loss: ℒA𝑦𝑤,𝑏,𝑦3.Return weights
argminY,Z	ℒ/𝑦𝑥|𝑤,𝑏,𝑦 10. Juli 2025
Gradient descent: the intuition
22
To decrease error, change 𝑤in the direction of negative slopeScale the size of change by learning rate 𝜂(here: 0.01)
𝑤1=𝑤0−𝜂⋅𝜕ℒ𝑤0𝜕𝑤0=1−0.01⋅82=0.18
ℒ𝑤
𝑆𝑙𝑜𝑝𝑒	𝑎𝑡	𝑤0=𝜕ℒ(𝑤0)𝜕𝑤0
Error(Loss) 10. Juli 2025
Batches and epochs
28
Did you notice? We always computed the error over all training examples•With many training examples computing the derivatives can take long•Solution: Stochastic gradient descent1.Randomly partition overall sample 𝑋 into X1,X2,…,Xm mini-batches2.Conduct gradient descent for each mini-batch sequentially3.Once all mini-batches used once (epoch) start over•Improves convergence, decreases redundancies
𝜕ℒ𝜕𝑤=∑𝑗=1𝑁𝜕ℒ𝑗𝜕𝑤𝑁≈∑𝑗=1𝑀𝜕ℒ𝑗𝜕𝑤𝑀All dataSubset of data
Works because:",11.2212379
"According to the text, what are hyperparameters and what is their primary function in machine learning?","Hyperparameters are variables that determine the structure of a model and its learning process. Their primary function is to regularize the model, which helps to control for overfitting and underfitting. For example, in a neural network, hyperparameters include the number of layers and neurons.","According to the text, the primary function of hyperparameters in machine learning is to “determine the structure of the model and the learning process” and “allow us to regularize our model and shape over- and underfitting outcomes.”","3. Juli 2025
Hyperparameters
The number of trees (parameters) and other choices on the model 
architecture (e.g., tree depth) are hyperparameters 
Hyperparameters are variables that 
• determine the structure of the model and the learning process
• allow us to regularize our model and shape over- and underfitting outcomes 
• different models and learning algorithms have different hyperparameters
• For example: neural networks’ hyperparameters include
• # layers
• # neurons per layer
• activation functions
By tuning hyperparameters, we can find a “good model complexity”, but 
of course the question is how we do that given the first rule of testing?
24 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
19
Representation +Evaluation +Optimization
Adjustmodel according to an algorithmic procedure until stop criterionis met.In linear regression: minimize the sum of squares (closed form) or gradient descent (iterative optimization).
OLS: min%⁄""&∑'𝑦−𝑦'
𝐿𝛽
𝛽
𝐿
𝛽∗ Journal of the Association for Information Systems 
 
814 
known to the investor. Again, the underlying model was 
a gradient boosted forest for which we automatically 
optimized the same seven hyperparameters on the current 
training dataset using a threefold cross-validation strategy 
with a directed Bayesian search.11 We used the remaining 
30% of the data from Study A to populate the pool of 
borrowers in our simulation, from which we randomly 
drew and matched 50 borrowers with randomly drawn 
investors from the corresponding investor pool. On 
average, the unbiased ML model’s accuracy for the share 
of observations not included in the training before the first 
updating iteration was 68.4%. 12  As we previously 
measured borrowers’ repayment decisions using  the 
strategy method in Study A, we knew whether they would 
make a repayment if there was an investment. Hence, we 
were able to determine whether the investment game 
between a given investor and borrower would result in (1) 
no investment, (2) investment but no repayment, or (3) 
investment and repayment by matching the unconditional 
investor with the conditional borrower decisions. 
Treatments: We introduce d treatment variations",8.714914799
What are the cores advantages of ensemble methods?,"Improved accuracy, Reduction of overfitting and Handling High Dimensionality.","According to the text, the core advantages of ensemble methods are:

1.  Improved Accuracy: Ensemble methods result in a prediction accuracy that is better than any individual model could achieve.
2.  Reduction of Overfitting: Because they use multiple learners, ensemble methods tend to be more robust and less prone to overfitting.
3.  Handling High Dimensionality: Ensemble methods can effectively handle large amounts of data with high-dimensional spaces.","3. Juli 2025
(Dis)advantages of ensembles
Core advantages of ensemble methods include:
1. Improved Accuracy: Ensemble methods result in a prediction accuracy that is better than
any individual model could achieve
2. Reduction of Overfitting: Because they use multiple learners, ensemble methods tend to
be more robust and less prone to overfitting, i.e., typically generalize rather well
3. Handling High Dimensionality: Ensemble methods can effectively handle large amounts
of data with high-dimensional spaces.
Core disadvantages of ensemble methods include:
1. Increased Complexity: Can be computationally expensive and require significant 
resources to train because they involve training multiple models instead of a single one.
2. Interpretability: Ensemble models are difficult to interpret compared to individual models. 
3. Risk of Overfitting with Noise: Although ensemble methods are generally more robust to 
overfitting, they can still overfit if the data is noisy. 
14 3. Juli 2025
Ensemble methods
Ensemble methods rely on different models that are simultaneously applied to the same problem
Predictions are aggregated producing the final prediction
• Hard vote: majority of individual votes
• Soft vote: average individual predicted probabilities and one final vote
• Meta model: train machine learning model to aggregate prediction (stacking)
Models‘ errors should be as uncorrelated as possible so that they cancel out through aggregation
6
Model 1
Model 2
Model 3
Model N-1
Model N
…
Aggregator Overall 
prediction 10. Juli 2025
Last time: ensemble methods
•Different models that are simultaneously applied to the same problem•However: only ”one layer” of models, i.e., no (comprehensive) stacking•Today: layers if modelsfeedingintoeachother•Idea(smallexample):
2
Model 1Model 2Model 3
Model N-1Model N… AggregatorOverall prediction
Model 1, Layer 1Model 2, Layer 1Model 1, Layer 2Model 2, Layer 2Final model(Aggregator)Overall prediction",10.79256845
What are some core disadvantages of decision trees?,"Overfitting, instability, and optimization difficulty.","According to the provided text, some core disadvantages of decision trees include:

1.  **Overfitting:** They can quickly lead to overfitting when the tree is too complex (too many nodes, levels, or leaves).
2.  **Instability:** Decision trees may be remarkably unstable conditional on variation in training data, leading to different trees.
3.  **Optimization difficulty:** Algorithms to build trees are typically heuristics that cannot guarantee global optimality.","3. Juli 2025
Last time: decision trees
Core disadvantages of decision trees include:
1. Overfitting: Can quickly lead to overfitting when tree is too complex, i.e., when it has too 
many nodes, levels, or leaves
2. Instability: Decision trees may be remarkably unstable conditional on variation in training 
data, leading to different trees
3. Optimization difficulty: Algorithms to build tree are typically heuristics that cannot 
guarantee global optimality
2
#  No de fau lt: 1 80  (9 0%)
#  Def au lt: 2 0 ( 10 %)
Ov era ll s ample
#  No de fau lt: 5 3 ( 91 .4 % )
#  Def au lt: 5  (8 .6%)
#  No de fau lt: 1 27  (8 9. 4%)
#  Def au lt: 1 5 ( 10 .6%)
Age = Young Age = O ld
#  No de fau lt: 3 2 ( 10 0%)
#  Def au lt: 0  (0 %)
Income 
>  50 .00 0
Income 
< = 5 0.0 00
#  No de fau lt: 2 1 ( 80 .8 % )
#  Def au lt: 5  (1 9.2 %)
#  No de fau lt: 1 26  (9 7. 7%)
#  Def au lt: 3  (2 .3%)
Uni degree =
y es
Uni degree =
no
#  No de fau lt: 1  (7 .7 % )
#  Def au lt: 1 2 ( 92 .3%)
Representation   + Evaluation + Optimization
How good is our current model?
Uncertainty according to target 
variable
How to improve our current model?
Recursively split data
until stop criterion is met (e.g., no 
more reduction in uncertainty) 26. Juni 2025
(Dis)advantages of decision / regression treesCore advantages of decision trees include:1.Handling numerical and categorical variables: Easy to implement and do not require a lot of preprocessing2.Non-parametric: No assumptions about distribution of variables and the classifier structure, allowing to non-linearities3.Interpretability: Easy to understand and to interpret so that one can detect possible inconsistenciesCore disadvantages of decision trees include:1.Overfitting:Can quickly lead to overfitting when tree is too complex, i.e., when it has too many nodes, levels, or leaves2.Instability:Decision trees may be remarkably unstable conditional on variation in training data, leading to different trees3.Optimization difficulty:Algorithms to build tree are typically heuristics that cannot guarantee global optimality
30 26. Juni 2025
Decision tree classifier: terminology
6
Decision trees belong to the hierarchical methods of classification, classifying data through recursive partitioningA decision tree consists of nodes (data) and edges (splits)•Topmost node: root•Connected posterior nodes: children•Connected prior nodes: parents•Node with parent and child: interior node•Node without child: leafAn interior node has at least two child nodesA tree where every non-leaf node has 2 children: binaryEach internal node tests one attribute 𝑋and splits data•Branch from internal node selects value of 𝑋•Prediction in leaf: 𝑝(𝑌!|𝑋!∈𝑙𝑒𝑎𝑓)
Leaf
Root
Interior
LeafInterior
LeafLeaf",10.75933218
"According to the document, what is the formula for Balanced Accuracy (Bal. Acc.)?",Bal. Acc. = (Sensitivity + Specificity) / 2,"According to the document, the formula for Balanced Accuracy (Bal. Acc.) is:

Bal. Acc. = (Accuracy + Specificity) / 2 

It also mentions F1 = 2 * Sensitivity * Precision / (Sensitivity + Precision)","26. Juni 2025
Technical performance on unseen data (2/2)
Accuracy can be misleading, esp. in case of imbalanced samples, hence, we may also want to consider
Combining these measures results in other important measures:
𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚=𝑻𝑷𝑻𝑷+𝑭𝑵𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏=𝑻𝑷𝑻𝑷+𝑭𝑷
𝑩𝒂𝒍.	𝑨𝒄𝒄.=𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚𝟐 𝑭𝟏=𝟐∗𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚∗𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏
Tree A prediction$𝑦!	 True outcome(𝑦) Tree Bprediction$𝑦""	1 1 11 1 11 0 01 1 01 0 00 1 00 1 00 0 00 0 00 0 0
Confusion matrix
𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚=𝑻𝑵𝑻𝑵+𝑭𝑷
What is the Bal. Acc. And F1for the two models?
𝑦=1𝑦=0$𝑦=13 2$𝑦=02 3
𝑦=1𝑦=0$𝑦=12 0$𝑦=03 5
Tree A Tree B accuracy and establishing itself as a reliable tool (Chen,2022). It can collect and evaluate large
amounts of data that may exceed human analytical capacities, enabling AI to provide decision
recommendations (Shaw, 2019).
Modern technologies, including artiﬁcial intelligence solutions, have revolutionized work and
contributed to developing human resources management (HRM) for improved outcomes
(Hmoud and Laszlo,2019). One signiﬁcant area where their impact is felt is in the recruitment
process, where AI implementation can potentially provide a competitive advantage by enabling a
better understanding of talent compared to competitors, thereby enhancing the company’s
competitiveness (Johansson and Herranen,2019).
AI receives commands and data input through algorithms. While AI developers believe their
algorithmic procedures simplify hiring and mitigate bias, Miasato and Silva (2019) argue that
https://doi.org/10.1057/s41599-023-02079-x OPEN
1 College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China.✉email: njuczs@nuaa.edu.cn
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x 1
1234567890():,; 15. Mai 2025
Different types of solutions
10
Balanced Scorecard§Überarbeitung der Inhalte§Wiedereinführung der BSC zum kontinuierlichem Controlling
ISO-Zertifizierung§Interne Audits im HWS und FSS§ISO INTEGRA 1x1§Verpflichtende Besprechung von Prozessen auf Ressort-/ Stabstreffen
Point solutions•Definition:A solution that addresses a single, specific problem or need.•Scope:Narrow and focused.•Integration:Typically standalone; not integrated with other systems.•Use case: Quick wins, easy to implement, often first step in solving a broader challenge. •Example:Fraud detection model (Anomaly detection)Application solutions•Definition:A complete software or tool that solves a broader problem•Scope: Medium; focused on a domain or function•Integration: May integrate with other applications or systems.•Use case: Useful when a domain-specific tool is needed •Example: Customer support app (Classification + UI + reporting)",8.662404537
"According to the text, what is information gain?",Information gain is the improvement in an evaluation measure (such as the reduction in entropy) that is achieved by splitting the data at a parent node (level t) into its child nodes (level t+1).,"The text doesn't explicitly define ""information gain."" It mentions it in the context of choosing an attribute during a data analysis process (""We choose the attribute with the largest information gain...""). It's presented as a metric used to determine which attribute is most useful for splitting data. 

","26. Juni 2025
Information gain: example
12
We choose the attribute with the largest information gain, i.e., Income
ID AgeIncomeRepayment1 Old HighYes2 YoungHighYes3 Old HighYes4 Old HighNo5 YoungHighYes6 Old LowNo
Data at node t𝐺𝑎𝑖𝑛()*=0.92−46⋅1+26⋅0=0.25
𝐺𝑎𝑖𝑛+,-./*=0.92−56⋅0.72+16⋅0=0.32 integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",8.082275867
What is KNN used for?,KNN is a supervised learning algorithm used for classification and regression.,You are correct. The provided text does not explain what KNN (K-Nearest Neighbors) is used for.,"in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben. 17. Juli 2025
A game changer
27
The transformer architecture1.Transform words into numbers –Word embeddings2.Control word positioning of words –Positional encoding3.Represent words in the context of other words –Self-attention4.Predict the next word using a feed-forward neural net sich nichts vormachen
und das ist eine Sache,
das würde ich jetzt per se 
erstmal nicht,
wenn wir über GPT sprechen,
wenn sie es sehen, ist nicht per 
se Bias,
Es ist ein Fehlverhalten.
Es wurde in eine bestimmte 
Richtung
gedrückt durch bestimmte 
Optimierungsverfahren,
Wenn Sie an Medien
und an politische Einstellungen 
denken.
Es gibt sehr viele Evidenzen 
dafür,
dass viele große Sprachmodelle
eher linksgerichtet sind in 
ihren Antworten.
Dass diese Modelle, wenn man zum 
Beispiel die den
Wahl O Mat mit Chat GPT macht,
dann kommt er raus,
dass Chat Gpt die Meinungen der 
Grünen,
der Linken und der SPD 
unterstützt weniger von den
rechtsgerichteten und 
rechtsradikalen Ansichten.
Was können Sie denken, woran das 
liegt?
Es liegt an den Trainingsdaten. 
Schöne Frage.
Erstmal eine Frage beantworten,
dann können Sie eine Frage 
stellen.
Wahrscheinlich, weil die 
Personen, die dahinter stehen,
die das entwickelt haben,
all diese Überzeugungen haben.
Unklar. Tatsächlich könnte ein 
Faktor sein.
Stimme ich zu.
Problem scheint eher zu sein, 
dass viel aus dem Internet.
Wikipedia ist massiv 
linksgerichtet.
Das hat er diese left leaning 
tendencies.
Auch wieder ein historisches 
Wachstum bestimmter
Tendenzen und Ansichten.",6.354101419
"According to the document, what are the three components of the training process of supervised machine learning?",Representation +Evaluation +Optimization,"According to the document, the three components of the training process of supervised machine learning are:

*   Representation
*   Evaluation
*   Optimization","12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion•In very general terms, we can distinguish three components of three training process•We will first talk this through using simple linear regression•Same logic will be applied to more complex models
16
Representation +Evaluation +Optimization 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
17
Representation+Evaluation +Optimization
The model architecture that stores the learned knowledge.Now: linear regressionthat comprises learnable the parameters called coefficients.
'𝑦=)𝑓𝒙=+𝛽!++𝛽""𝑥""+⋯++𝛽#𝑥$ Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung.",6.102035522
"In the case study with the Sparkasse, is the combination of a human and a machine advisor better or worse than the machine alone?",Worse.,"According to the document, the combination of a human and a machine advisor is perceived as “viel besser und viel vertrauenswürdiger” (much better and more trustworthy). The human helps to avoid excessive risk aversion. However, the document also highlights the potential for algorithmic discrimination if the machine identifies differences between groups that don’t actually matter.","Ende.
So suggerieren unsere 
Ergebnisse,
wenn Mensch plus Maschine da 
sind.
Nicht weil es denkt es besser, 
sondern einfach,
weil da jemand da ist, der ihnen 
die Hand hält.
Ähm, ja.
Takeaways, das habe ich glaube 
ich,
schon so zusammengefasst.
Es geht vor allem um dieses 
Handholding.
Maschine plus Mensch ist
zumindest nicht schädlich in der 
Beratung,
was die Qualität der Beratung 
angeht.
Aber der Mensch, der Kunde von 
Ihnen,
wird es Ihnen danken
und wird Ihre Empfehlung mehr 
annehmen.
Das ist auch das Wort zum 
Sonntag.
Ich hoffe, Sie konnten sich ein 
bisschen was mitnehmen.
Wieder. Und wir sehen uns 
nächste
Woche mit einem kurzen Quiz
und dann dem Einstieg in
die technische Seite. Vielen 
Dank. Kunde ist.
Sie haben die Beratung,
die gleiche identische Beratung 
von dem Mensch
plus KI erhalten und
Ihr alternatives Ich von der 
reinen KI,
dem Robo Advisor das erhalten.
Dann haben Sie in dem Mensch 
plus Maschine Fall.
Haben Sie der Maschine 
signifikant,
haben Sie dem der Beratung 
signifikant häufiger.
Das sind zwei verschiedene 
Sachen
der Beratung des 
Risikoeinschätzung
und die eigentliche 
Investitionsempfehlung
haben Sie dem signifikant 
häufiger gefolgt.
Jetzt können Sie fragen Ist es 
gut oder schlecht,
dass Sie sagen Na ja,
wenn mensch dabei ist, finde 
ich, dass alles
viel besser und viel 
vertrauenswürdiger.
Nachricht ist gut, denn der 
Mensch hilft Ihnen,
exzessive Risikoaversion zu 
umgehen.
Und es scheint so ein bisschen,
wie der Kommilitone vorhin 
gesagt hat zu sein,
dass wenn mensch dabei ist und 
sie 100 € verlieren,
dann haben Sie wenigstens 
jemanden,
auf den sie sauer sein können.
Wenn Sie der Maschine gefolgt 
sind,
hilft Ihnen das nicht ganz so 
viel.
Das ist tatsächlich so ein 
bisschen intuitiv.
Auch die der Mechanismus,
auf den alles hindeutet.
Wir haben uns angeschaut. Glaubt 
der Mensch einfach.
Das glaubt der Kunde, dass 
Maschine
plus Mensch besser berät.
Also dass es genauer ist, technische Performance der 
Maschine,
die für diese beiden Subgruppen 
vergleichbar sind.
Und grundsätzlich ist dieses 
Fehlverhalten von Maschinen,
wenn systematisch basierend auf
bestimmten soziodemographischen 
Faktoren,
die wir rechtlich oder
als Gesellschaft für wichtig 
erachten.
Wenn dort Unterschiede 
vorliegen,
die in der Realität aber 
eigentlich keine Rolle spielen,
also ohne Rechtfertigung,
unterscheidet die Maschine in
ihrer Vorhersagegenauigkeit,
in welchem Maß auch immer wir 
das jetzt messen,
zwischen diesen beiden Gruppen,
dann sprechen wir von 
algorithmischer Diskriminierung.
Algorithmische Diskriminierungen 
gibt es,
ein ganz tolles Papier hier von 
zwei
Kollegen werde ich Ihnen auch 
noch hochladen.
Und diese algorithmische 
Diskriminierung
hat ein ganz großes Problem.
Nämlich wir haben nicht, das es
vorher nicht auch schon hätte 
sein können,
dass Ihr Bankberater in dieser
einen Sparkasse
gegen bestimmte Minoritäten 
diskriminiert hat.
Aber diese Maschinen erzeugen 
oder erzeugen dieses
Muster auf eine Art und Weise,
auf eine skalierbare Art und 
Weise,
wie sie eine einzelne Person nie 
hätte darstellen können.
Und das hat auf der einen Seite 
eben technische Aspekte,
aber auch soziale Aspekte.",10.60490537
"What is the concept of 'Mixed Initiative' in human-AI interaction, as exemplified by a smartwatch asking if it should record a workout?","'Mixed Initiative' is a middle ground between full automation and full manual control, where the system proactively suggests an action, but the human still makes the final choice to accept or reject it.","Based on the provided text, the concept of “Mixed Initiative” in human-AI interaction is being explored through the Sparkasse case study. Here’s a breakdown of what it means in this context:

*   **Not a Single, Pre-Determined Approach:** The Sparkasse project isn't about a simple, automated system where the AI *always* initiates the workout recommendation. Instead, it’s designed to allow for a dynamic interplay between the human (the customer) and the AI.

*   **Shared Control & Response:** The AI (represented by the P2P loan assessment system) *proposes* a workout based on data. However, the human customer has the *option* to accept, reject, or modify that suggestion. This back-and-forth, shared control is the “mixed initiative.”

*   **Human-Centered Design:** The goal is to understand *how* customers actually respond to this AI suggestion. The researchers want to see if customers prefer a fully automated process or if they want to collaborate with a human advisor *using* the AI’s insights.

*   **Understanding User Preferences:** The research isn’t about forcing a particular interaction style. It’s about observing and learning how people *actually* want to interact with AI in this specific scenario – a workout recommendation.

**In essence, “Mixed Initiative” means that the human and the AI are actively involved in determining the course of action, rather than one simply dictating the other.**

The smartwatch example (which isn't explicitly detailed in the text) would fit this concept: Instead of the smartwatch *always* asking, “Should I record your workout?” it would be designed to present an option and wait for the user’s response – “Yes, start recording” or “No, not now.”","So legen wir los. Herzlich 
willkommen.
Vorlesung fünf sozusagen.
Fast schon Bergfest, oder?
Heute ist, glaube ich, Bergfest,
wenn man die restlichen 
Vorlesungen zusammenzählt.
Auch weil ich jetzt schon 
mehrfach gefragt wurde.
Dazu mit der Klausur.
Das hier ist jetzt die letzte
Vorlesung aus diesem ersten 
Teil,
wo es darum geht, ein 
grundsätzliches
Verständnis dafür zu schaffen,
was KI bewirken kann,
wie Menschen mit KI 
zusammenarbeiten können,
was potenzielle Schwachpunkte 
und Probleme sind,
wenn wir KI einsetzen.
Heute sprechen wir über Mensch 
Maschine Interaktion,
bauen sozusagen auf der 
Erklärbarkeit.
Vorlesungen von vor zwei Wochen
auf und dann ist dieser erste 
Block abgeschlossen.
Dann fängt es nächste Woche
an mit dem eher technischen 
Blog,
wo wir jetzt wirklich uns dann 
damit befassen werden,
wie wir diese Systeme bauen,
was die Mathematik dahinter ist,
wo die Stärken und die
Schwächen von bestimmten 
Modellen liegen.
Und zu Beginn der nächsten
Vorlesung werden wir dafür dann 
aber zum Abschluss dieses
ersten Blockes so eine Art 
kleinen Recap machen,
wo ich Ihnen auch
ein paar Fragen interaktiv
mit der Gruppe durchspielen 
werde.
Von der Struktur der Frage 
können Sie dann auf die
Struktur der Fragen in was man denken sollte.
Hier zum Beispiel von Microsoft 
Guidelines
für Human Centered AI,
also Guidelines an, was sie in 
welchen
Process Steps immer wieder 
denken sollten.
Ich werde das jetzt nicht hier 
alles vorlesen.
Die Idee ist wirklich, dass sie 
schon in
der bevor überhaupt mit dem 
System interagiert wird.
Bestimmte Fragen immer stellen,
immer wieder sich klar machen 
was kann das System
und nicht das Ganze als einmal 
sehen,
sondern über die Zeit dynamisch 
immer wieder schauen.
Interagieren Menschen 
ausreichend damit?
Muss das System irgendwo 
abgewandelt werden.
Immer den Menschen in den 
Mittelpunkt zu stellen.
Irgendwelche Fragen bis hierher?
Weil die letzten Minuten und 
dadurch,
dass ich mich länger bei dem 
Output aufgehalten habe,
als ich gedacht habe,
wird das jetzt in fünf Minuten 
abgehandelt,
habe ich ihn noch mal wieder 
eine Case Study
mitgebracht von einem Papier von 
Olli Hinz,
mir und zwei Kollegen aus Paris 
von der Chaussee,
wo wir uns hier mit der 
Sparkasse tatsächlich
im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob
das Ganze vielleicht 
automatisiert ablaufen sollte.
Das Papier ist jetzt
also kommt bald im Management 
Science Journal raus.
Wie gesagt, die letzten fünf 
Minuten.
Aber vorher noch mal 
irgendwelche
Unklarheiten zu dem Thema,
was wir oder Themen,
die wir bisher besprochen haben.
Cool. Wie gesagt, wir haben
uns in diesem Projekt
Sparkasse kam auf uns zu und hat 
gesagt naja,
wir haben hier eine Idee
oder wir möchten gerne mal 
rausfinden,
ob unsere Kunden eigentlich eher 
wollen,
dass sie von einem Robo Advisor 
beraten werden,
also einem automatisierten 
System,
wo kein Mensch mehr dabei ist 
oder von Mensch Berater,
der mit der KI zusammenarbeitet
und gemeinsam sozusagen als
Human AI Team diese Beratung an 
den Kunden gibt.
Der Kunde ist eben darüber 
informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,",29.56274343
What did the speaker say about the possibility of the smartwatch automatically starting to record a workout?,The speaker said the smartwatch could estimate the probability that the user wants their sport recorded.,"The speaker discussed the possibility of a smartwatch automatically starting to record a workout. They explained that the smartwatch *could* potentially suggest recording a workout (like when it measures your pulse and predicts you might want to record your sport). However, the key point is that the smartwatch *doesn't* automatically do this. Instead, it’s a “Not Action” – meaning the user has to initiate the recording themselves. The speaker highlighted four extreme cases: either the smartwatch suggests recording, or it doesn’t, and the user controls the action.","Wenn Sie entscheiden würden, 
nicht zu delegieren,
dass Sie auf jeden Fall das 
selber machen, hilft es ja.
Ja, das ging jetzt gerade,
was sie gesagt hatten.
Naja, muss es denn immer das 
eine oder das andere sein?
Nee, muss es natürlich nicht.
Denken Sie an diejenigen im 
Raum,
die irgendeine Smartwatch haben.
Wenn Sie Sport machen und 
plötzlich
das System Sie fragt Soll ich 
das eigentlich aufzeichnen?
Sind Sie gerade am Sport machen?
Das ist irgendwie so eine Art 
Mischung davon.
Also weder das System selber 
entschieden,
jetzt irgendwas aufzuzeichnen 
ohne Ihre Einwilligung.
Sie haben also diese 
Entscheidung
des automatisierten 
Aufzeichnungs nicht
an die Maschine delegiert.
Aber Sie haben auch nicht aktiv 
gesagt Na ja,
ich gehe jetzt hin und such das 
raus.
Sondern die Maschine von sich 
entscheidet,
ihnen etwas vorzuschlagen in the 
first place.
Und das nennen wir grundsätzlich
The Idea of Mixed Initiative.
Und es geht auch schon auf ein 
sehr frühes
wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.
Und die Idee, die hier in diesem 
Paper auf einer
Informatik Konferenz postuliert 
wird, ist.
Naja, lassen Sie uns mal 
annehmen,
das System könnte die 
Wahrscheinlichkeit einschätzen,
dass Sie ein bestimmtes Goal 
haben.
Die Maschine hat irgendeine 
Möglichkeit vorherzusagen,
was die Wahrscheinlichkeit,
dass Sie gerne Ihren Sport 
aufgezeichnet hätten.
Und jetzt haben wir hier noch 
gegebene Evidence.
N Lassen wir mal weg.
Intuition dahinter ist wenn die 
uhr Ihren Puls misst,
kann die uhr eine Vorhersage 
darüber machen.
Was ist denn eigentlich egal?
Möchten Sie es aufgezeichnet 
haben?
Gegeben Ihren Puls 
beispielsweise
so und das haben die in ein 
Modell gepackt,
was eigentlich sehr simpel ist 
dieses Modell,
wo gesagt wird Naja,
wir haben hier unten unsere 
Wahrscheinlichkeit,
dass ein Nutzer ein bestimmtes 
Ziel verfolgt.
Und entweder haben wir die 
Wahrscheinlichkeit,
dass dieses eine Goal eben nicht 
verfolgt wird.
Was heißt die Wahrscheinlichkeit 
ist
null Wahrscheinlichkeit Null oder die uhr macht einfach 
nichts in Aktion.
In diesen Extremfällen Jetzt 
gehen wir gleich
wieder in die Mitte davon.
Aber Sie laufen und Sie möchten,
dass Laufen aufgezeichnet haben 
Sie laufen, möchten,
dass nicht aufgezeichnet haben 
und
die Maschine kann selber die 
Aktion machen,
das aufzuzeichnen oder die uhr 
macht das eben nicht.
Und dann haben wir vier 
verschiedene Fälle und
das versuchen wir wieder über 
Nutzen darzustellen.
Ganz, ganz allgemein.
Der Nutzen, den ich davon habe,
wenn die uhr die Aktion 
durchführt
und die Messung beginnt, 
gegeben.
Ich möchte auch, dass diese 
Messung gemacht wird.
Wenn ich Sport mache versus die
uhr macht das einfach
und eigentlich will ich das gar 
nicht.
Umgekehrt hier und das ist wer 
kennt dieses
Symbol? Was bedeutet das?
Genau. Das ist aus der 
Mathematik der Logik.
Wenn Sie dieses seitlich 
gedrehte Liter sehen,
heißt das immer Nicht immer.
Nicht a, nicht Action.
Sie können eigentlich hier,
um das Beispiel zu 
vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.",13.09665775
The lecture describes two extremes in human-machine collaboration. What is 'Algorithm Aversion'?,"'Algorithm Aversion' is the observation that people often consciously choose not to use a helpful algorithm, even when they know it is objectively more accurate than their own opinion or that of other humans.","According to the text, “Algorithm Aversion” is the tendency for people to avoid blaming a machine when a decision goes wrong. People feel a psychological need to be able to assign blame to a person in situations where something negative happens.","of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address. significant consequences, such as in healthcare, loan 
approval, hiring, or criminal justice systems. 
Mitigating algorithmic discrimination : The 
expansive literature on algorithmic discrimination 
proposes various methods to identify and alleviate 
such biases. According to d ’Alessandro et al. (2017), 
we can divide these mitigation strategies into three 
categories: pre -processing, in -processing, and post -
processing. Pre -processing strategies strive to 
eliminate disparities inherent in the dataset. Methods 
such as those proposed by Kamiran and Calders (2012) 
and Zafar et al. (2017) employ data pre -processing 
techniques like feature massaging and reweighting to 
balance the dataset based on social group 
characteristics. In -processing strategies involve 
conventional learning algorit hms during model 
training. Researchers often use techniques like naive 
Bayes and decision tr ees for debiasing (Calders & 
Verwer, 2010; Zhang et al., 2018). Additional research 
suggests placing constraints on the classification 
model to mandate the satisfaction of a proxy 
(Woodworth et al., 2017). Lastly, post -processing 
strategies aim to adjust the output to counter biases. 
For example, Corbett -Davies et al. (2017) Dann gibt es ja rein theoretisch 
keine Person,
wo man quasi die man zur 
Rechenschaft ziehen könnte,
wenn die Entscheidung 
fehlschlägt.
Davor schaltet, auch wenn er 
automatisiert
die ganze Zeit quasi einfach nur 
sagt ja, jetzt ist fein.
Dann hat man zumindest jemanden, 
dem man zutraut.
Das ist grundsätzlich richtig.
Behalten Sie den Gedanken bei,
weil nächstes Mal sprechen wir 
über
die Idee von Decision Threshold 
nochmal.
Aber das ist grundsätzlich auch 
etwas,
was so ein bisschen die 
Forschung andeutet.
Menschen haben
diese Algorithmen Aversion und 
machen es nicht,
weil sie am Ende jemanden lähmen 
blamieren möchten.
Wenn eine Maschine etwas macht,
dann können Sie gern die 
Maschine bleiben.
Die Maschine hat er leider 
nicht.
Nicht viel Negatives von.
Und es stellt sich heraus,
dass auch die psychologische
eigene Befriedigung darüber zu 
sagen Ich habe jemanden,
dem ich die Schuld zuweisen 
kann,
wenn mir etwas Schlechtes 
passiert. Einer anderen Person.
Diese psychologische 
Befriedigung haben sie nicht,
wenn Sie am Ende eine Maschine 
bleiben.
Was so ein bisschen darauf 
hindeutet,
dass Menschen eine 
Zahlungsbereitschaft dafür 
haben,
einen anderen Menschen drin zu
haben in dieser 
Entscheidungssituation.",7.895306349
What are the two extremes of observation regarding human-machine collaboration?,Algorithm aversion and Algorithm appreciation.,"According to the text, the two extremes of observation regarding human-machine collaboration are:

*   **Algorithm Aversion:** A preference for human help/decisions over AI ones.
*   **Algorithm Appreciation:** A preference for AI help/decisions over human ones.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Goal: Human-AI complementarities
9
The objective of human-AI collaboration is achieving complementary performance•Humans plus AI outperform either party acting alone for a set of tasks•Complementary strengths can originate from•Access to different data (e.g., soft signals)•Learning of distinct problem-solving strategies (imperfectly correlated errors) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT",8.06017065
What were the developers struggling with regarding Windows eight?,The developers of Windows 8 were struggling with the bad user interface.,"According to the text, the developers were struggling with the user interface of Windows 8. Users found it ""totale Katastrophe"" (a total disaster) and were ""total überfordert"" (completely overwhelmed).","5. Juni 2025
Acceptance and appropriate use of technology
2012, Microsoft released Windows 8 that significantly changed the user interface•Many of its features are standard nowadays•Yet, considered one of Microsoft’s worst OS•In 2021 its market share was about 1% while its predecessor Windows 7 was still at 15%
4
Remember Windows 8?
What does that imply for AI? the loop dahingehend Sie 
entscheiden,
Ich transferiere 
Entscheidungsgewalt in diesen
automatisierten Fällen oder Sie 
behalten immer
die Entscheidungsgewalt,
müssen sich aber entscheiden Wie
viel Gewicht geben Sie 
eigentlich?
Der Information von der Maschine 
macht es Sinn.
So fangen wir aber
mal einen Schritt weiter vorne 
an Erinnern Sie sich noch?
Ich weiß nicht, ob Sie dafür 
noch zu jung sind, Sogar.
Aber erinnern Sie sich noch
an diese Version von Microsoft?
Microsoft acht kam raus
2012 und es war ein vollkommenes 
Desaster für Microsoft.
Warum? Naja, viele der User 
waren total überfordert.
Das war eigentlich grundsätzlich 
ein
tolles System mit ganz vielen 
Neuerungen,
mit ganz vielen Funktionen,
wo sich die Entwickler gedacht 
haben Naja,
das sollte meinen Nutzern 
gefallen.
Das macht alles viel einfacher.
Da soll der Entscheidungsfindung 
Prozessen
arbeiten helfen.
War alles natürlich well 
intended.
Problem war, wir alle fanden das 
nicht so cool.
Das war bei mir auch der 
Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen, Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen,
wohingegen Windows acht, was
viele neue gute Funktionen 
hatte,
immer noch nur um die 1 % war.
Und es ist wieder considered 
eines der
schlechtesten Operating Systeme, 
die es gibt.
weil es einfach niemand benutzt 
hat.
Jetzt natürlich die Frage Wenn 
ich Ihnen hier
so eine anekdotische Geschichte 
erzähle,
um Sie so ein bisschen emotional 
abzuholen,
was glauben Sie, hat das mit KI 
zu tun?
Wie könnte ich aus dem 
vorherigen Beispiel,
dass die KI mir ein Signal gibt?
Ich sehe das und es
ist eigentlich ein hilfreiches 
Signal.
Wie kann ich diese Idee hier auf 
diesen
anekdotischen Fall aus der 
Geschichte
mappen? Wo ist die Verbindung?
Genau hier.
Wir haben ein tolles System. Es 
kann uns helfen.
Viele der Features im
Übrigen sind heute in einer 
anderen Darstellung,
in einer anderen Art und Weise 
in
den Microsoftsystemen immer noch 
da.
Die wurden dort entwickelt für 
Windows acht,
aber die Oberfläche war totale 
Katastrophe.
User Interface Wir hatten 
Entwickler,
die sich ausschließlich damit",7.950159788
"In a case study about the role of the human, what was the investment amount for the P2P vehicle in the experiment?",100 €,"The text doesn't specify the exact investment amount for the P2P vehicle in the experiment. It only mentions that investors were considering whether to invest with a particular borrower with a ""festgeschriebenen Zins"" (fixed interest rate).","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,
also Privatkredite, finanziert 
werden sollten,
also ob ein Kunde 100 € 
beispielsweise in die in
jemand anderen als Privatkredit
investieren möchte mit einem 
festgeschriebenen Zins.
Und dieses KI System war darauf 
ausgelegt vorherzusagen,
ob dieser Privatkredit dann
tatsächlich zurückgezahlt wird 
oder nicht,
ob ein Ausfall dieses Kredits 
auftreten wird oder nicht.
Das System war eigentlich auch 
ziemlich gut.
Es hat irgendwie so 74 % 
Accuracy gehabt.
Was für so soziales
Verhalten oder menschliches 
Verhalten vorherzusagen,
schon immer ganz gut ist.
Dann sind wir in die Sparkasse 
rein und haben mit
27 Bankern haben
wir hingesetzt und gesagt Hier 
sind
verschiedene von diesen 
Privatkrediten.
Gebt doch mal bitte deine eigene 
unabhängige Meinung an,
ob dieser Kredit und ich gebe 
dir ein
paar Informationen zu dem 
Privatkredit,
ob der zurückgezahlt wird oder 
nicht.
Würdest du das deinem Kunden?
Du als Mensch ohne KI Hilfe?
Würdest du dem Kunden empfehlen investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records 
from all previous iterations and newly collected 
instances of borrowers who received an investment and 
made a repayment decision. 8  The next iteration then 
started, where we again randomly sample d—with 
replacement—and paired 50 borrowers and investors. 
Investors and their decisions : At the outset of each 
simulation prior to the first iteration, we randomly split 
Study B data into two equally sized shares. We used the 
first share (comprising 304 unique individuals with 20 
decisions each, yielding 6,080 discrete observations) to 
train an ML model that simulates whether an investor 
would choose to invest with a particular borrower, using 
31 features (20 personal characteristics of the investor, 
10 characteristics of the borrower, and a repayment 
forecast for the borrower). The employed model is a 
gradient boosted forest, executed using the XGBoost 
library in Python (Chen and Guestrin, 2016). We 
employed an automated process to optimize seven",8.95608592
"According to the document, what did Eric Horvitz advocate in his 1999 paper?",Elegant coupling of automation with direct manipulation by user.,"The document doesn't mention Eric Horvitz or a 1999 paper. It describes a conversation about how to approach machine learning and decision-making, focusing on the idea of systems that automate certain tasks while humans retain control over others.","findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. 
Our results have important practical implications, especially considering the impending regulations 
mandating human involvement in ML-supported decision-making processes. 
Keywords: Continuous Updating, Algorithmic Discrimination, Human Discrimination, Feedback 
Loops, Investment Game 
John Qi Dong  was the accepting senior editor. This research article was submitted on  June 22, 2022 and underwent 
three revisions.  
1 Introduction 
With the aim of bolstering economic efficiency and 
societal welfare (Benbya et al., 2020; Teodurescu et al., 
2021), machine learning (ML) systems are augmenting 
human decision -making across diverse domains, 
including bail decisions (Kleinberg et al., 2018), hiring 
processes and student admissions (Horton, 2017; Zhao 
et al., 2020), banking (Fu et al., 2021), and medicine 
(Jussupow et al., 2021). In Europe, Big Data analytics 
software revenue reached US$14.6 billion in 2018 
alone. Wells and Spinoni’s projections (2019) indicate 
an expected annual growth rate of about 8% for this wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen
letztlich verantwortlich ist.
Da. Der hat nämlich damals genau 
das gesagt,
diese ganze Diskussion zwischen 
entweder
automatisieren oder wir bleiben
im Decision Support.
Das sollten wir gar nicht so
als entweder oder betrachten,
sondern wir sollten es aus einer 
Perspektive betrachten,
dass wir vielleicht Systeme 
haben,
die uns Automatisiert von selbst 
bestimmte
Dinge abnehmen und dann autonom 
agieren.
Und für andere Dinge sollten Sie 
uns vielleicht fragen.
Das heißt, wir geben eigentlich 
die Entscheidung,
ob wir im Automation oder 
Delegation,
im Automation oder Augmentation 
Setting sind,
auch an die Maschine ab.
Und er hat dann sehr intuitives,
sehr intuitiven, konzeptionellen 
Vorschlag gemacht,
wie wir darüber nachdenken 
können.
Das ist jetzt auch wieder das 
Modelldenken,
was ich glaube sehr wichtig ist 
an Universitäten.
Gelernt werden sollte,
dass sie ein grundlegendes 
Schubladendenken haben,
wie sie über solche Probleme 
nachdenken können,
ohne dass wir jetzt keine They were therefore assured of their comprehension and consent.
Additional information
Supplementary informations The online version contains supplementary material
available at https://doi.org/10.1057/s41599-023-02079-x.
Correspondence and requests for materials should be addressed to Zhisheng Chen.
Reprints and permission informationis available athttp://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory",8.826157331
What is augmentation in the context of human-AI collaboration?,AI serves as a decision support tool; human always makes final decision.,"According to the text, augmentation is where “AI serves as a decision support tool; human always makes final decision.” It’s a mode of collaboration where the AI provides advice, but the human retains control and makes the ultimate decision.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Goal: Human-AI complementarities
9
The objective of human-AI collaboration is achieving complementary performance•Humans plus AI outperform either party acting alone for a set of tasks•Complementary strengths can originate from•Access to different data (e.g., soft signals)•Learning of distinct problem-solving strategies (imperfectly correlated errors) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT",5.869114876
What is the mean of the prediction change when considering coalitions of features?,Δ=−0.1,The mean change in prediction (Δ) across all coalitions is -0.1.,"22. Mai 2025
Marginal contributions across coalitionsIdea:•Remove feature across all possible coalitions and compute changes in predictions•Compute a feature’s importance by marginalizing over all its contributionsSimplified example for “savings”:
21
Coalition of 3 features
Δ=−0.07
Coalitions of 2 features
Δ=−0.3
Δ=−0.02
Coalition of 1 feature
Δ=−0.01
Mean: Δ=−0.1
What is the prediction of no feature is present? with modern machine learning methods. This integra -
tion enables the estimation of individual-level differ -
ences between actual outcomes and hypothetical 
outcomes that would have occurred without the inter -
vention (Athey and Imbens 2015 ).
Contrasting with traditional machine learning’s 
focus on predicting outcomes from correlational data 
relationships, CML aims to forecast cause-and-effect 
relationships. This approach enhances the understand -
ing of how interventions affect outcomes of interest 
(Wager and Athey 2018 ). For example, although tradi -
tional machine learning may predict a customer’s likeli -
hood to purchase when exposed to a marketing 
intervention, CML methods determine how the inter -
vention itself alters the customer’s purchase likelihood 
compared with no exposure (i.e., they predict the 
causal effect attributable to the intervention).
Research on the application of CML in practice is still 
in its infancy. To date, only a few studies have applied 
these methods to analyze heterogeneous treatment 
effects. Nevertheless, initial applications are promising. 
For instance, Zhang and Luo ( 2023 ) employ causal for -
ests to explore the varied impact of social media posts availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating.",5.152293444
"According to the document, what is interpretability?",The ability to explain or provide the meaning in understandable terms to humans.,"According to the document, interpretability is “the ability to explain or provide the meaning in understandable terms to humans.”","22. Mai 2025
Some definitionsConfusing nomenclature: explainable / interpretable / transparent models•Interpretability:the ability to explain or provide the meaning in understandable terms to humans•Explainability:explanation as an interface between humans and a decision maker that is both an accurate proxy of the decision maker and comprehensible to humans•Transparency: a model is transparent if by itself it is understandable.Explaining different things:•Predictionsand model behavior (overarching logic the model learned)•Decisions(predictions plus rule that translates into decision)Explanation modes:•Analytic statement:natural language descriptions of elements and context that support decision•Visualizations:highlight parts of data that support the decisions•Cases:give typical/illustrative examples that support the decision•Rejections or alternative choice:counterfactuals that argue against the alternative output
15 des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses betriebswirtschaftlich
ökonomische Entscheidungen zu 
treffen.
Wer sieht das nicht?
Noch mal, was ich letztes Mal 
gesagt habe Wenn Sie
was nicht verstehen, ist es 
nicht Ihre Schuld.
Dann habe ich es einfach 
schlecht erklärt.
Deswegen immer raus damit.
Dann gehe ich davon aus, dass 
jeder verstanden hat.
Aber eine andere Frage haben
wir ja theoretisch auch
noch andere 
Entscheidungsmöglichkeiten,
Parametern, die dieselbe 
Qualität liefern.
Vollkommen richtig. Da wäre 
jetzt die Frage,
ob es eine 
Entscheidungsmöglichkeit,
die zwei Parameter verwendet,
besser als eine auf einem 
Papier.
Das ist sehr, sehr gute Frage.
Da gibt es verschiedene Theorien 
zu.
Grundsätzlich, wie wir sehen 
werden,
für den technischen Teil der 
Vorlesung.
Wir können für das gleiche
Problem verschiedene Regeln 
identifizieren.
Verschiedene Machine Learning 
Modelle trainieren.
Diese Machine Learning Modelle 
können bessere oder
schlechtere Performance 
aufweisen.
Wenn sie aber genau die gleiche 
Performance aufweisen,
dann haben wir natürlich in 
einer gewissen Art und
Weise die Wahl, immer darauf 
achtend,
dass die einzige Performance, 
die wir sehen,
aktuell immer Performance auf 
historischen Daten ist.
Das heißt, wir müssen auf der",6.143040419
What does the EU AI Act Art 86 demand?,Right to Explanation of Individual Decision-Making.,You are correct. The document does not specify the demands of EU AI Act Article 86.,"ethics, and responsibility in AI (Zuiderveen Borgesius,2020). The
Organization for Economic Cooperation and Development
(OECD) has provided recommendations on AI, while the
European Commission has drafted proposals regarding
the inﬂuence of algorithmic systems on human rights. In 2019,
the European Commission established a high-level expert group
on AI, which proposed ethical guidelines and self-regulatory
measures regarding AI and ethics.
Public organizations have played a role in establishing
mechanisms to safeguard algorithmic fairness. The Algorithm
Justice League (AJL) has outlined vital behaviors companies
should follow in a signable agreement. Holding accountable those
who design and deploy algorithms improves existing algorithms
in practice (36KE,2020). After evaluating IBM’s algorithm, AJL
provided feedback, and IBM responded promptly, stating that
they would address the identi ﬁed issue. As a result, IBM
signiﬁcantly improved the accuracy of its algorithm in minority
facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human 22. Mai 2025
EU AI act
10 Retailing 97(4):676–696.
European Union (2016) Regulation EU 2016/679 of the European 
Parliament and of the council of 27 April 2016, article 22. Offi -
cial J. Eur. Union 119:59.
European Union (2021) Proposal for a regulation EU of the Euro -
pean Parliament and of the council of April 21, 2021, laying 
down harmonised rules on artificial intelligence (Artificial Intel -
ligence Act) and amending certain union legislative acts. Official 
J. Eur. Union 119.
Frisch R, Waugh FV (1933) Partial time regressions as compared 
with individual trends. Econometrica 1(4):387–401.
Ghose A, Lee HA, Nam K, Oh W (2023) The effects of pressure and 
self-assurance nudges on product purchases and returns in 
online retailing: Evidence from a randomized field experiment. 
J. Marketing Res. 61(3):517–535.
Goldstein NJ, Cialdini RB, Griskevicius V (2008) A room with a 
viewpoint: Using social norms to motivate environmental con -
servation in hotels. J. Consumer Res. 35(3):472–482.
Goldstein NJ, Griskevicius V, Cialdini RB (2007) Invoking social 
norms: A social psychology perspective on improving hotels’ 
linen-reuse programs. Cornell Hotel Restaurant Admin. Quart. 
48(2):145–150.
Grahl J, Hinz O, Rothlauf F, Abdel-Karim BM, Mihale-Wilson C",5.966392517
What is a 'selective labels problem'?,It occurs when a machine learning model's prediction influences decisions that prevent the collection of data needed to evaluate the model's accuracy.,You are correct. The document does not define a “selective labels problem.”,"Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider
social issues in context. It is crucial to make them aware of
attribute selection’s impact on the algorithm (Yarger et al.,2019).
The algorithm engineer plays a crucial role in the entire system,
from setting goals for machine learning to selecting the
appropriate model and determining data characteristics such as
labels. If inappropriate goals are set, bias may be introduced from
the outset (36KE,2020).
An engineer is responsible for developing the algorithmic model.
If they hold certain beliefs and preconceptions, those personal
biases can be transmitted to the machine (Njoto,2020). Although
the device is responsible for selecting employee resumes, it operates
based on underlying programming. The programmer guides the AI
in making decisions about the best candidate, which can still result
in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to maintained ten years. After applying an initial screening related
to titles, keywords, or abstracts, the literature was selected based
on its relevance to the research topic.
The obtained literature was studied in depth to reveal the
surfaced themes. Several systematic research themes were iden-
tiﬁed, including AI-based recruitment applications and beneﬁts,
causes of algorithmic discrimination, which algorithmic recruit-
ment discrimination exists, and algorithmic recruitment dis-
crimination resolution.
The process applied for the reviews depicted in Fig.1. After
excluding duplicates and less relevant and outdated literature,
only 45 articles could be used as references for this study (referred
to Table1). The literature review shows that most of the research
on algorithmic hiring discrimination has occurred in recent years.
The research trend indicates that algorithmic hiring discrimina-
tion will be a hot research topic in the coming period.
1. The ﬁrst theme is the application of various aspects of
recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the 15. Mai 2025
Measurement of algorithmic fairness
•There is no single agreed-upon measure for discrimination/fairness•There are different types of bias metrics that we can distinguish•Individual-level metrics:ensure that people who have similar qualifications with respect to a task receive similar outcomes•Group-level metrics:ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups
15
Verma & Rubin, 2018",5.397401094
"According to the text, what is one way to control for gender when analyzing a dataset?","By running the dataset through without including gender, but then rerunning it while including gender to compare performance.","According to the text, one way to control for gender when analyzing a dataset is to remove the gender column entirely and only use features for prediction. This was exemplified by the Amazon example, where they excluded the gender column to assess performance differences based on gender.","data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately 
for women. Second, it can be created by unrepresentative, 
or imbalanced data (Barocas & Selbst, 2016; Mehrabi et 
al., 2021). When the dataset underrepresents certain 
groups, such as women, the ML model can make more 
errors when predicting for this group. Imbalances occur 
when labels skew significantly within specific subgroups. 
Third, technical constraints or malpractices during model 
development and training may yield discriminatory 
outputs (Friedman & Nissenbaum, 1996; Williams et al., 
2018). Biases may emerge during data integration due to 
inconsistent data formats or when the integration of 
personal information like ethnicity is mishandled (Schelter 
& Stoyanovich, 2020; Williams et al., 2018). It is critical 
to appreciate the sociotechnological nature of algorithmic 
discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and Datensatz,
in dem man alles angibt,
außer das Geschlecht und lässt 
das durchlaufen.
Aber hierbei würden sie Gender 
mit rein nehmen.
Also in einem ersten Schritt,
um das zu kontrollieren.
Genau in diesem Beispiel von dem 
Amazon,
das haben Sie schon richtig 
erfasst,
war es so die haben Gender 
rausgelassen,
hatten aber eine Spalte im 
Datensatz,
wo Gender noch drin war,
haben die Maschine dann
basierend auf allen Features 
außer Gender die
Vorhersage machen lassen und 
konnten dann natürlich
innerhalb ihrer Daten trotzdem 
vergleichen.
Wie ist die Performance dort in 
den Einträgen,
wo wir Männer,
Frauen nonbinär hatten und haben 
das dann verglichen?
Praktisch ja.
Aber es gibt viele Argumente 
dafür,
dass wir diese Features, die 
protected sind,
dass wir eigentlich größere 
Probleme erzeugen,
wenn wir sie den Maschinen nicht 
zur Verfügung stellen,
weil diese Maschinen so mächtig 
sind,
Korrelationen herauszufinden,
die letztlich ablesen lassen,
ob es welches Geschlecht es ist 
und das implizit lernen.
Und das ist sehr, sehr, sehr 
versteckt ist,
wo eigentlich jetzt eine 
Diskriminierung herkommt.
Wenn wir es aber drin lassen,
gibt es bestimmte Methoden, das 
direkt abzulesen.
Aber die Frage was will man denn 
da, Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D.,",8.890154839
"According to the text, what is one way large language models can exhibit biased tendencies?",They have been optimized to be overly agreeable and to 'say yes to everything.'.,"According to the text, large language models can exhibit biased tendencies by replicating existing biases present in the data they are trained on, and the persistence of biased decision-making by humans in the loop.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” 
initial biases over time given sufficiently diverse and 
unbiased data, human bias es can impede this process. 
This insight is especially relevant in the current era of 
growing political and social divisions in human 
societies (e.g., Allcott et al., 2020). When people 
increasingly adopt biased attitudes and behaviors, 
biased, continuously updated ML models are less inherent to these models, adding a new dimension to 
the algorithmic fairness literature by shifting from 
static, one-time biases to a more temporal perspective: 
a comprehensive understanding of algorithmic 
discrimination and fairness must incorporate not just 
the initial conditions under which a model is trained, 
but also the dynamic environments where these models 
operate and evolve. Our results suggest that continuous 
model updating could serve as a mechanism for 
mitigating bias in ML systems and might act as a force 
to supplement traditional data cleaning and debiasing 
efforts (Shrestha et al., 2019) to reduce discrimination 
risks. 
On the other hand, our simulations reveal that the 
positive impact of continuous updating on algorithmic 
discrimination (and economic efficiency) largely 
hinges on the sociotechnological environment, which 
includes the degree of the system ’s initial bias and, 
perhaps more importantly, the persistence of biased 
decision-making by humans in the loop. We found that 
the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that",7.22135663
What did Google do to address concerns about the representation of nurses in image search results?,Google filtered the results so that both men and women are shown when 'nurse' is entered.,"You are correct. The text describes a shift in Google’s image search results – moving from a system where nurses were predominantly shown as female to one where both men and women are represented. However, it *doesn’t* detail the specific actions Google took to make this change. It focuses on the problem, the historical bias, and the subsequent efforts to clean and adjust the data.","Das hat Google jetzt anscheinend 
durch Filter so gemacht,
dass wenn man Krankenpfleger 
eingibt,
dass sowohl Männer und Frauen 
gezeigt werden.
Aber bis vor zwei Jahren war
es immer noch ein sehr gutes 
Beispiel.
Wenn ich Sie frage, googeln Sie 
mal bitte nach Images.
Dann war Krankenpfleger zu 99 %
weiblich und Arzt zu 99 % 
männlich,
obwohl diese Bilder natürlich 
von beiden
eigentlich schon vorhanden sein 
sollen.
Das heißt, wir haben hier
irgendwelche gewachsenen 
technischen Grundlagen,
Datenbanken, die das einfach 
noch nicht abbilden können.
Having said that, die 
Unternehmen sind
sehr sensibilisiert durch diese 
ganzen PR Probleme,
die sie damit erlitten haben und 
haben angefangen,
diese Daten zu bereinigen.
Aber das erfordert eben massiv 
viel Investitionen
in Datenbereinigung,
wo wir wieder in dem 
Kriegsmodell,
bei dem Data Preparation sind.
Sie müssen sich klar sein Wie 
sind
Ihre Strukturen der Daten 
eigentlich aufgebaut?
Soweit ich weiß es auch manchmal
das Labeling von Menschen 
gemacht.
Richtig? Genau. Es könnte so 
sein,
dass wir haben eine Gruppe von 
Menschen,
die nicht politisch korrekt 
denken.
Und dann würden sie es bewusst 
falsch.
Richtig. Auch ein massives 
Problem.
Wir haben auf der einen Seite discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible gesagt Na ja,
per Gesetz dürfen wir nicht auf 
Gender gucken.
Was also gemacht wurde,
ist man hat die Gendermerkmale,
also die selbst angegebenen 
Geschlecht.
Geschlechtsidentifikation hat 
man rausgenommen.
Es war also nicht Teil des 
Systems.
Trotzdem in diesem Testbereich,
wenn wir in dem Crisp an 
Evaluation denken,
bevor das System wirklich live 
gegangen
ist und massiv eben angefangen 
hat,
die neue Einstellungskriterien 
zu beeinflussen,
haben bestimmte Tests gezeigt.
Naja, obwohl ich nicht angegeben 
habe,
ob diese Person, obwohl das 
Modell nicht in der Lage war,
auf Informationen über die
Geschlechtsidentifikation diese 
Person zuzugreifen,
hat das System systematisch
bei Frauen eine geringere 
Wahrscheinlichkeit ausgespuckt.
Woher kann das kommen? Was 
denken Sie?
Ich habe nicht dem Modell Zugang 
zu dem
eigentlichen 
Genderidentification gegeben
und trotzdem in Tests.
Wenn ich dann gesagt habe, naja,
wie hoch ist denn die 
Vorhersagegenauigkeit für 
Frauen,
wie hoch ist denn für Männer?
Wurde dann gesehen,
dass sie für Frauen systematisch
bei gleichen Qualifikationen
systematisch schlechter war.
Woher kann das kommen,
dass das System das trotzdem 
erkannt hat oder
diesen Bias reproduziert hat,",10.48862314
"According to the document, what was the reason Amazon's AI recruitment system discriminated against women?","It recognized correlations between hobbies, universities attended, and gender due to historical data showing fewer women attending certain universities and pursuing technical fields.","According to the document, Amazon’s AI recruitment system discriminated against women because it was trained on a dataset of CVs that were predominantly male. This led the algorithm to perceive this biased model as indicative of success, resulting in discrimination against female applicants.","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",8.331025362
"According to the lecture, what is the CRISP-DM model and what is its first and most important step?","The CRISP-DM (Cross-Industry Standard Process for Data Mining) is a blueprint for developing machine learning applications. Its first and most important step is Business Understanding, which involves clearly defining the business problem you are trying to solve.","According to the lecture:

The CRISP-DM model is a standard process for structuring AI projects. It consists of six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.

The first and most important step is **Business Understanding**.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 15. Mai 2025
Overview CRISP-DM
9",8.080649137
The lecture describes three levels of integrating machine learning solutions into an organization. What is a 'System Solution' (the third level)?,"A 'System Solution' is the deepest level of integration, where machine learning fundamentally changes the entire organizational structure and business model, requiring a complete rethinking of processes. The example given is Amazon's 'Anticipatory Shipping' patent.","According to the lecture, a ‘System Solution’ is: “A comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems. It’s broad; enterprise-level, and highly integrated with multiple components and data flows. It’s used to transform or optimize an entire organization, like Amazon’s ‘Ship then Shop.’”","System solutions•Definition:Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems.•Scope:Broad; enterprise-level•Integration:Highly integrated with multiple components and data flows.•Use case:Innovation must transform or optimize an entire organization•Example:Amazon’s “Ship then Shop” (Prediction + Inventory + Logistics + …) Und diese Systeme haben ein
Zusammenspiel dann was bestimmt,
ob ich eigentlich das System 
nutze.
Das war eines der fundamentalen 
Ergebnisse,
die Sie hier gefunden haben in 
der Studie.
Und dann Sie haben gesagt okay, 
wenn ich weiß,
dass wir die Menschen in
ihrer Kognition überzeugen 
müssen,
dass sie das System nutzen 
sollten und akzeptieren
sollten entweder über System
eins oder über System zwei 
Aktivierungen,
dann lass uns doch mal drüber 
nachdenken,
welche Faktoren denn hier eine 
besondere Rolle spielen.
Und da haben Sie insgesamt in
diesem Modell vier verschiedene 
Faktoren identifiziert.
Einer der Faktoren nennen Sie 
die Performance Expectancy.
Das können Sie sich vorstellen.
Sie haben ein System vor sich 
und ob
Sie eine Bereitschaft 
grundsätzlich Formen
in ihrer Kognition des System zu 
benutzen,
hängt damit zusammen, ob sie 
glauben,
dass dieses System Ihnen hilft,
bestimmte Ziele zu erreichen.
Wenn Sie sich die verschiedenen 
GPTsysteme,
die aktuell verfügbar sind,
auch in den öffentlichen 
Versionen,
dann sind da ganz viele 
verschiedene.
Dann gibt es aktuell vier 
Punkte, wo ich glaube,
das ist generell ZugReifbar 
mittlerweile.
Dann haben wir drei,
wir haben 4.1 4:05 usw.
Wenn ich die Entscheidung 
treffe, hinzubekommen.
Und auf der einen Seite immer
diese technische Perspektive auf 
die Systeme,
die müssen ausreichend 
performant und gut sein.
Aber wir brauchen eben auch
diese Einbettung der Systeme der
technischen Systeme in das Sozio 
system.
Deswegen sprechen wir 
grundsätzlich dann
von diesen soziotechnischen 
Systemen,
wo diese Systeme wirklich 
genutzt
werden und letztlich 
Veränderungen hervorrufen.
Und viel davon passiert eben 
hier
Information Systems auf Deutsch 
aus
der Perspektive der 
Wirtschaftsinformatik.
Was immer so eine 
Schnittstellendisziplin ist,
wo wir aus ganz verschiedenen 
Blickwinkeln Psychologie,
Management, Ökonomie,
Verhaltensökonomie, Neurologie
auf diese Phänomene draufgucken,
wie tatsächlich Menschen, 
Organisationen,
soziale Systeme mit
diesen technischen Systemen 
interagieren.
Und in diesem Sinne, ich denke, 
diejenigen,
die Wirtschaftsinformatik ich 
weiß gar nicht,
wer es gerade gibt,
Peter oder Olli, einer meiner 
Kollegen der Abteilung.
Wenn Sie Wirtschaftsinformatik 
eins
oder zwei besucht haben,
werden Sie bestimmt schon mal 
über das Unified,
also Unified Theory of 
Acceptance
and Use of Technology gehört 
haben.
Wer hier hat das in der 
Vorlesung schon mal gesehen?",9.763311625
"According to the text, what is one thing machine learning models currently cannot do?",Machine learning models currently cannot provide a valuation of the consequences of a particular state of the world.,"According to the text, one thing machine learning models currently cannot do is “with unstructured data like words simply deal.” They require a lot of “data preprocessing” and cannot easily handle messy, unstructured data.","Sie müssen das sauber machen.
Maschinen, wie wir sehen werden,
können nicht mit 
unstrukturierten
Daten wie Wörtern einfach so 
umgehen.
Wir haben ganz viel 
Datenvorarbeit zu betreiben
und es gibt bestimmte Machine 
Learning Modelle,
die sind sehr schön,
die viel dieser Arbeit durch
bestimmte algorithmische 
Lernverfahren abfangen.
Zum Beispiel der 
Entscheidungsbaum
oder auch der Random Forest.
Aber es gibt andere Methoden und 
insbesondere die
most sophisticated Methoden wie 
neuronale Netze,
die auf jede Kleinigkeit in
den Daten massiv sensitiv 
reagieren,
müssen Daten standardisieren.
Wir müssen auf eine bestimmte 
Art und
Weise sampeln müssen
bestimmte Observationen 
rausschmeißen.
All das, wie Sie hier sehen,
braucht ungefähr 60 bis
80 % der Zeit eines solchen Data 
Science Projekts.
Es ist teilweise über Wochen, 
Monate.
Wenn wir Daten von Unternehmen 
bekommen,
mit denen wir zusammenarbeiten,
um Forschungsprojekte zu machen,
dass wir erst mal eine Idee 
bekommen,
was diese Daten alles bedeuten.
Sie haben bei mir in der im 
Seminar
gesessen letztes Semester diese 
Daten,
die Sie da erhalten haben, von 
der Firma.
Totales Chaos.
Muss man sich erstmal reindenken 
und
das braucht massiv viel Zeit. sind dann in eine Case Study 
gemacht,
wo ich ihnen gezeigt habe,
wie man Machine Learning 
anwenden kann.
Nicht immer nur zu negativen 
Aspekten,
aber diese Predictions,
die Machine Learning uns liefern 
kann,
wie man die auch dazu nutzen 
kann,
zum Beispiel zur Vorhersage,
welcher Kunde wie auf welches 
Messaging reagiert,
um letztlich die
Rücksenderaten von Produkten zu 
verringern.
Letzte Woche sind wir dann mehr 
ins
Detail schon reingesprungen, 
Eingesprungen,
haben uns überlegt, aus einer 
ökonomischen Perspektive,
was Prediction bedeuten könnte,
wenn wir bei General Purpose 
Technologien gesprochen.
Wir haben darüber gesprochen,
dass wir uns, wie
es bei so vielen General Purpose 
Technologien ist,
aktuell auf einen bestimmten 
Teil der Kurve,
die noch befinden,
die Kurve als die Darstellung, 
die fundamentale Idee,
dass wir nicht in große 
Technologie Investitionen habe,
dass sich die nicht sofort 
auszahlt.
Das ist immer so ein bisschen 
Verzögerung gibt,
am Anfang sogar die marginale
Produktivitätsveränderungen 
negativ sein kann.
Das ist so dieser untere 
Halbkreis von dem J.
Und das kann verschiedene Gründe 
haben.
Unter anderem eben, dass diese
Technologien noch nicht 
ausreichend in den
Organisationen mit Stelle sein.
Sie müssen also identifizieren 
welche
Daten welche Variablen sind 
eigentlich da?
Müssen Sie vielleicht neue 
Informationen erheben?
Sind bestimmte Informationen gar 
nicht zugänglich?
Und das Letzte kann tatsächlich 
und findet häufig statt,
dass man sagt Ich habe eine 
Riesenidee,
wie ich eigentlich Machine 
Learning nutzen kann.
Dazu brauche ich aber in einer 
First World ABCDE.
Und dann kommt der Betriebsrat 
und sagt ihnen Nee,
das können wir nicht machen, das 
funktioniert so nicht.
Und dann stehen sie dort und 
sagen Ja gut,
dann macht meine Modellidee,
um dieses Businessproblem zu 
lösen.
So wie ich es mir vorgestellt 
habe,
vielleicht gar keinen Sinn.
Deswegen befindet sich dann 
dieser Pfeil hier zurück.
Und Sie müssen natürlich sagen,
in der First Best World würde 
ich es gerne so haben.
Das sollte immer Ihr 
Ausgangspunkt sein.
Und dann werden Sie mit
der Realität konfrontiert und 
merken,
dass einige Sachen technisch 
einfach nicht da sind,
vielleicht noch nicht da sind.
Vielleicht bedarf es eben 
Änderungen
in der Art und Weise,
welche Informationen erheben 
werden müssen.
Es kann sein, dass Sie zu der 
Entscheidung kommen,
hier in diesem Data 
Understanding, dass Sie sagen 
Hey,",8.200745106
What are two types of bias metrics?,Individual-level metrics and group-level metrics.,"According to the text, two types of bias metrics are:

*   **Individual-level metrics:** These ensure that people with similar qualifications receive similar outcomes.
*   **Group-level metrics:** These ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups.","15. Mai 2025
Measurement of algorithmic fairness
•There is no single agreed-upon measure for discrimination/fairness•There are different types of bias metrics that we can distinguish•Individual-level metrics:ensure that people who have similar qualifications with respect to a task receive similar outcomes•Group-level metrics:ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups
15
Verma & Rubin, 2018 data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",7.314512968
"According to the CRISP-DM framework, what is the approximate percentage of time spent on data preparation within the CRISP-DM process?",Approximately 60-80% of time.,"The provided text doesn’t give an exact percentage of time spent on data preparation. It simply states that the data preparation phase ""is likely to be performed multiple times, and not in any prescribed order.""","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and 15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9",7.439576626
What does the document state about the impact of Generative AI on employee happiness?,"The document states that employees became happier, with less escalation and longer tenure.","You are absolutely correct. The document focuses on algorithmic discrimination in recruitment and selection, not the impact of AI, including generative AI, on employee happiness.","employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender a study on the impact of artiﬁcial intelligence on eliminating hiring bias from
recruitment and selection process. J Comput Theor Nanosci 17(9):4404–4407
Ruwanpura KN (2008) Multiple identities, multiple-discrimination: a critical
review. Fem Econ 14(3):77–105
Samuelson PA (1952) Spatial price equilibrium and linear programming. Am Econ
Rev 42(3):283–303
Shaw J (2019) Artiﬁcial intelligence and ethics. Perspect: Policy Pract High Educ
30, 1–11
Shin D, Park YJJCIHB (2019) Role of fairness, accountability, and transparency in
algorithmic affordance. Perspect: Policy Pract High Educ 98, 277–284
Smith B, Shum H (2018). The future computed. Microsoft
Tilcsik A (2021) Statistical discrimination and the rationalization of stereotypes.
Am Sociol Rev 86(1):93–122
Timmermans S, Tavory I (2012) Theory construction in qualitative research: from
grounded theory to abductive analysis. Sociol Theory 30(3):167–186. https://
doi.org/10.1177/0735275112457914
Upadhyay AK, Khandelwal K (2018) Applying artiﬁcial intelligence: implications
for recruitment. Strateg HR Rev 17(5):255–258
van Esch P, Black JS, Ferolie J (2019) Marketing AI recruitment: the next phase in
job application and selection. Comput Hum Behav 90:215–222. https://doi.",7.221330166
What did Amazon patent in 2014?,A 'Chip Shop' (patent).,You are correct. The document does not provide any information about Amazon’s patents. It focuses on research related to consumer returns in e-commerce.,"work as “Marketing Science. Copyright © 2024 The Author(s). https:/ /doi.org/10.1287/mksc.2022. 
0393, used under a Creative Commons Attribution License: https:/ /creativecommons.org/licenses/ 
by-nc-nd/4.0/.” 
Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE 
and the Deutsche Forschungsgemeinschaft [Grant 449023539]. 
Supplemental Material: The online appendix and data files are available at https:/ /doi.org/10.1287/mksc. 
2022.0393. 
Keywords: electronic commerce • nudging • causal forest • digital footprint • consumer returns • artificial intelligence
1. Introduction
In recent years, global e-commerce has experienced sig -
nificant and sustained growth. E-commerce presents 
numerous advantages for both customers and retailers. 
However, the lenient, and frequently free, return poli -
cies introduce drawbacks that complicate this expan -
sion. In the fashion industry, the share of purchases 
returned can reach a staggering 50% (Dzyabura et al. 
2023 ).
Product returns pose a substantial financial burden 
on retailers (Nageswaran et al. 2020 ). The costs of estab -
lishing return infrastructures (Ofek et al. 2011 ), incur -
ring transportation expenses (Zhou and Hinz 2016 ), profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for 
components of this work owned by others than the Association for Information Systems must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior 
specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, 
GA, 30301-2712 Attn: Reprints, or via email from publications@aisnet.org. They were therefore assured of their comprehension and consent.
Additional information
Supplementary informations The online version contains supplementary material
available at https://doi.org/10.1057/s41599-023-02079-x.
Correspondence and requests for materials should be addressed to Zhisheng Chen.
Reprints and permission informationis available athttp://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory",6.288046837
The lecture identifies two fundamental aspects that make AI different from previous General Purpose Technologies like electricity. What are they?,"First, it is a technology that can improve itself through its own use (self-learning). Second, this self-learning creates massive cross-industry spillover effects, where use in one industry can improve the system for another.","According to the text, the two fundamental aspects that make AI different from previous General Purpose Technologies like electricity are:

1.  **The technology itself can continuously learn and improve** through use – it’s the first time a technology has this capability.
2.  **It sparks waves of follow-on innovations, new business models, and organizational redesigns** (it's a catalyst for complements).","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. some plants. After the mid-1990s, artiﬁcial intelligence software
saw signiﬁcant advances. Until today’s digital economy, AI has
been commonly used in various industries (Hmoud and Laszlo,
2019).
Artiﬁcial intelligence is deﬁned as the ability of something like
a machine to understand, learn, and interpret on its own in a
human-like manner (Johansson and Herranen,2019). Artiﬁcial
intelligence aims “to understand and simulate human thought
processes and to design machines that mimic this behavior.”It is
designed to be a thinking machine with a level of human
Table 1 Research theme, papers, and main content.
Theme Research papers Main content
1. AI-enabled recruitment
application and its beneﬁts.
Bogen and Rieke (2018), Ahmed (2018), Hmoud and Laszlo
(2019), Albert (2019), van Esch et al. (2019), Köchling et al.
(2022), and Chen (2023)
Summarize the recruitment process into four
main steps: sourcing, screening, interviewing, and
selection
Beattie et al. (2012), Newell (2015), Raub (2018), Miasato and
Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal- drauf gucken,
ob es das Richtige ist,
die Kohlekraftwerke wieder 
hochzufahren.
Grundsätzlich kann das aber als
Kerninnovation KI KI braucht.
Das wollen wir alle machen,
braucht aber weitere 
Innovationen,
dass wir es wirklich gut nutzen 
können.
Was glauben Sie denn,
wenn Sie KI mit den vorherigen 
Innovationen vergleichen?
Was macht KI besonders?
Was ist ganz fundamental
unterschiedlich
zu allen vorherigen General 
Purpose Technologies.
Und ich habe es gerade schon 
gezeigt Ja,
man braucht keine neue Hardware 
als Nutzer.
Am Ende doch die.
Also man braucht immer wieder 
andere Hardware.
Irgendwann reiben sich auch 
GPU's zum Beispiel auf,
geht aber in die richtige 
Richtung,
in das Technische zu denken.
Yes, das ist ein Teil davon.
Wir haben zum Ersten Mal Systeme 
oder eine Technologie,
die sich selbst verbessern kann.
Wenn Sie sich überlegen Sie 
haben den Hammer,
was auch eine General Purpose 
Technologie ist,
nur weil Sie dreimal den Hammer 
benutzen,
wird der Hammer per se nicht 
besser.
If anything wird der Hammer.
Irgendwie über die Zeit 
schlechter.
Wir haben aber jetzt eine 
Technologie.
Dadurch, dass wir sie nutzen,
wir neue Daten kreieren und
diese neuen Daten wieder in
die Maschine reingefüttert 
werden können,",10.73033905
"From a purely economic perspective, what is the single most important function that machine learning technology provides?","It is a technology that massively lowers the cost of making high-quality, accurate predictions.","From a purely economic perspective, machine learning technology provides the single most important function of **improved prediction and decision-making based on data.**

The text highlights this by stating that ML allows for “data-based decisions” and “predicting market trends.” This core ability – to analyze vast datasets and generate more accurate forecasts and insights – directly translates to better resource allocation, optimized operations, and ultimately, increased profitability for businesses.","Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 24. April 2025
However, beyond the hype what does Machine Learning actually deliver?",9.809930801
"According to the text, what does the 'J-curve' represent?","The 'J-curve' represents a phenomenon where, with many technologies, productivity initially decreases before eventually increasing.","The provided text does not contain any information about a ""J-curve."" It discusses data mining, machine learning models, and data input, but it doesn't mention a J-curve.","The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase
w hich phase, or w hich particular task of a phase, has to be perform ed next.
The outer circle in Figure 2 sym bolizes the cyclic nature of data m ining itself. D ata m ining is not
finished once a solution is deployed. The lessons learned during the process and from  the
deployed solution can trigger new , often m ore focused business questions. Subsequent data
m ining processes w ill benefit from  the experiences of previous ones (cf. the virtuous cycle of
(B erry and Linoff, 1997)). dieser Gleichung hier 
ausdrücken.
Wir haben drei verschiedene 
Komponenten.
Wir haben hier dieses Hütchen,
wir haben das fix und wir haben 
hier dieses Y Hütchen.
Meine erste Frage an Sie Was war 
das? F Hütchen.
Yes, genau richtig.
Wir haben hier das erlernte 
Machine Learning Modell,
was die Muster, die wir in 
historischen
Daten durch Algorithmen erkannt 
haben, abspeichert.
Hier in diesem F. Steckt unser 
gesamtes Wissen drin,
was wir irgendwie extrahieren
und wie genau wir das 
extrahieren.
Das wird der zweite Teil unserer 
Vorlesung sein.
Wichtig hierbei Hütchen ist 
immer dafür ein Indikator,
dass wir von einer Approximation 
sprechen.
Das ist ein Modell.
Das ist nicht der wahre 
unterliegende Prozess,
der die Daten generiert.
Das ist nicht die Landschaft von 
Frankfurt sozusagen,
sondern es ist die Landkarte.
Es ist ein explizit abstrakteres 
Modell,
eine Darstellung der Realität.
Und da können eben Fehler drin 
sein.
Es ist ganz wichtig, sich
immer wieder vor Augen zu 
halten.
Dann haben wir die zweite 
Komponente.
Das war das XML. Was ist das?.X. 
Jemand eine Idee?
Jetzt dahinten? Genau das sind 
die Daten,
die wir in diese Maschine 
reintun.
Wir haben zum Beispiel, wenn Sie 
überlegen,
Sie interagieren mit Chat GPT 10. Juli 2025
Gradient descent: the intuition
18
Imagine a data set with three observations:
Can you spot the relationship between 𝑥and 𝑦?We train a perceptron with one weight 𝑤and no bias
Assume 𝑓𝑤⋅𝑥=𝑤⋅𝑥(linear activation function). What is the optimal 𝑤?
𝑥 𝑦3 -0.21-5 0.359 -0.63
𝑓(𝑤⋅𝑥)𝑥 A𝑦",7.243177891
What is a potential new attack vector related to large language models?,"Systems can be hacked, leading to false or harmful code being written into company codebases.","The provided text doesn't offer information about potential attack vectors related to large language models. It discusses:

*   The interplay between continuous model updating and algorithmic discrimination.
*   The origins and impacts of algorithmic feedback loops.
*   Details about the Word2Vec model and transformer architecture.

To answer your question about potential attack vectors, I would need additional context or information.","affecting ML systems’ functionality by explaining the 
interplay b etween continuous model updating and 
algorithmic discrimination. Despite its critical importance 
in developing countermeasures and refining policy 
recommendations, the complex interplay between 
algorithmic discrimination and continuous ML model 
updating remains an underexplored area in the literature.  
Second, our paper complements the nascent body of 
studies exploring the origins and impacts of 
algorithmic feedback loops (Burghardt & Lerman, 
2022; Cowgill, 2018). For instance, Lum and Isaac 
(2016) revealed that the iterative updates of a 
predictive poli cing system led to increased 
discrimination against minority groups. Similarly, 
Ensign et al. (2017) showed how feedback loops could 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
A game changer
27
The transformer architecture1.Transform words into numbers –Word embeddings2.Control word positioning of words –Positional encoding3.Represent words in the context of other words –Self-attention4.Predict the next word using a feed-forward neural net",8.925376654
"According to the text, what is a problem with relying on a perfect machine learning model to predict future outcomes?","It would shift us into a situation where there appears to be no risk, but this is conceptually problematic.","According to the text, a problem with relying on a machine learning model is that “the Y [output] is never necessarily a prediction, a prediction that doesn’t necessarily have to be right.” It highlights that the model’s output (predicted “Y”) isn’t guaranteed to accurately reflect the actual outcome.","1933 ), uses machine learning functions to flexibly esti -
mate residuals in treatment and outcome prediction 
equations, enhancing the robustness of our treatment 
effect estimates and aligning with accepted practices 
(detailed further in Online Appendix A.6). We further 
conform to data science standards by dividing our data 
into training (80%) and testing (20%) sets (Hastie et al. 
2009 ). We fine-tune the causal forest’s hyperparameters 
through a grid search with fivefold crossvalidation on 
the training data, focusing on out-of-sample R-score per -
formance (Schuler et al. 2018 ). We provide hyperpara -
meter tuning details in Online Appendix A.7.
4.1.3. Evaluation of the CML Model. Our evaluation of 
the CML model is based solely on test data and follows 
two steps. First, we assess the CML model’s predictions 
through group average treatment effects following 
Chernozhukov et al. ( 2018 ), where we separately esti -
mate the treatment effects for individuals who the 
CML model would treat and those who it would not 
treat. Second, we evaluate the CML-based treatment 
policy through off-policy evaluation following Hitsch 
et al. ( 2024 ). Off-policy evaluation estimates the effect 12. Juni 2025
Remember the following case study from lecture 1.(a) Explain in your own words what the central machine learning prediction helps to do. (b)Provide a brief discussion about the potential dangers associated using ML models for such purposes in the context of misinformation.
5 Das ist unsere Prädiktion,
auf Englisch auch wieder 
indiziert durch das Hütchen.
Das Hütchen zeigt uns an dieser 
Stelle,
dass wir nicht ganz explizit 
wissen,
ob das eigentlich das reale 
Outcome ist.
Wenn Sie eine Machine Learning 
System haben,
was Ihnen basierend auf den
Soziodemografischen Faktoren 
einer Person
eine Vorhersage gibt darüber,
ob diese Person tatsächlich 
kreditwürdig ist oder nicht.
Ist dieser Output der
Maschine das Y immer eine 
Prediction,
eine Vorhersage, die nicht
notwendigerweise richtig sein 
muss.
Wir hoffen, dass sie so akkurat 
wie möglich ist.
Das ist aber nicht zwangsläufig 
gegeben.
Und wer erinnert sich an das 
Ziel von
Machine Learning aus einer 
konzeptionellen Perspektive?
Wenn wir über die overarching 
Objectives reden,
was wollen wir damit erreichen?
Wofür ist dieses ganze Modell 
gebastelt etc.
da? Die bestmöglichen 
Vorhersagen?
Genau, Und für welche Daten?
Ja, und eine Sache will ich noch 
rauskitzeln.
Was sind die Inputdaten am 
besten?
Daten, die das Modell noch nicht 
kennt? Genau das.
Es geht uns nicht darum,
dass wir ein Modell haben,
was uns optimal erklärt,
was in der Vergangenheit alles 
passiert ist.
Aber wir wollen keinen 
Vorhersagemodell
über den Stockmarket Price",9.467935324
How much have the costs for inference (using GPT or other Large Language Models) decreased in the last two years?,The costs have decreased by a factor of 34.,"According to the text, the costs for inference (using GPT or other Large Language Models) have decreased by a factor of 34 in the last two years.","Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen.
Wenn wir über GPT Systeme 
sprechen,
weil die da sehen wir natürlich 
merklich unterschiedlich,
ob ich 60 Gigabyte Text
reinknalle oder das halbe 
Internet.
Dann habe ich natürlich schon 
unterschiedliche
Kommunikationskosten.
Wobei jetzt in dem neuen Report
war zum Beispiel angezeigt,
dass die Kosten zumindest für 
die Inferenz,
also wenn Sie GPT benutzen oder
andere Large Language Models,
die Kosten sind seit zwei zwei 
Jahren Haben
die sich bei einem Faktor 34 
verringert?
Ja, das wird massiv effizienter.
Wir sehen jetzt so ein bisschen 
die
Laws of Scaling einsetzen.
Ganz kurz noch hat das Ihre 
Frage.
Das ist die beantwortet mit 
Ockhams Razor.
Wenn wir jetzt nur nach 
Temperatur entscheiden,
wäre nach diesem Gesetz kein 
Gesetz,
das vorsichtig sein. Das ist so 
eine.
Ich würde es als Daumenregel 
bezeichnen.
Deswegen hat man auch davon 
abgesehen,
dass irgendwie Rule zu nennen.
Aber grundsätzlich so ein 
bisschen aus der
Physik motiviert zu sagen naja,
ein einfacheres Modell, was mir 22. Mai 2025
A frequent trade-off
Model complexity, prediction accuracy, and model interpretability•Linear models simple to interpret as input-output relationships can be quantified in magnitude and direction, see, e.g., OLS coefficients•More complex models typically more accurate in predictions•Often a tradeoff; but not always (Sometimes “Occam’s Razor” seems to apply)
12
Model complexity / ability to capture pattern
Interpretability
BayesLinear regressionDecision tree
Deep NNEnsemble methods (e.g., RF)SVM
NOTE: Graph just an illustration without precise scales; exact relationship highly dependent on actual problem 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18",7.514164686
"What is the idea behind using simpler models, according to Ockhams Razor?",To prefer a model that needs less information to make a good prediction.,"According to Ockham’s Razor, the idea behind using simpler models is to favor models that require less information to make a good prediction. Essentially, it’s about minimizing complexity and assuming the simplest explanation is usually the best. The text states that “dass man versuchen sollte, ein Modell so einfach wie möglich zu belassen.”","Ockhams Razor aber dafür,
dass man versuchen sollte,
ein Modell so einfach wie 
möglich zu belassen.
Bedeutet in unserem Beispiel ich 
sollte ein Modell
laut diesem Ockhams Razor 
bevorzugen,
was weniger Informationen 
benötigt,
um eine gute Vorhersage zu 
machen.
Wäre das nicht auch teilweise 
ökonomischer?
In dem Modell, eine Menge 
Energie zu verbrauchen,
würden vielleicht weniger Daten
das gleiche Ergebnis bringen.
Das ist auch ein wichtiger Punkt 
grundsätzlich,
den wir aber in den Datenmengen,
über die wir alle sprechen,
wenn wir über tabulare Daten
insbesondere sprechen, 
ignorieren können,
weil die Kosten dafür so gering 
sind,
dass das keine Rolle spielt,
ob ich einen oder zwei Features 
mehr nehme.
Es gibt Unternehmen, die viel 
mehr Daten.
Das stimmt. Das ist 
grundsätzlich richtig.
Es kommt auf den 
Trainingsprozess an,
es kommt auf die Art des Modells
an für unsere Purpose is.
Aber das meine ich damit Für 
unsere Datensätze,
die wir nutzen, ist das eine 
Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen. die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell
oder auch ein paar mehr kleinere 
Erweiterungen noch,
die es dann mehr auf
die Konsumentenperspektive 
erweitert haben.
Aber das Grundmodell und das 
ist,
was ich Ihnen beibringen möchte.
Und ich dachte eigentlich,
ich könnte es relativ schnell 
machen,
aber weil es so wichtig ist,
sprechen wir doch ein paar 
Minuten drüber.
Dieses Grundmodell, das ist 
wichtig zu verstehen.
Auf der einen Seite wurde 
argumentiert,
ob meine Mitarbeiter in
meinem Unternehmen eine 
Technologie,
die ich bereitstelle,
benutzen und das gilt auch 
grundsätzlich für Kunden
mit ein paar Abwandlungen ist,
ob es sogenannte Facilitating 
conditions gibt.
Habe ich einen Hilfscenter,
habe ich in meiner Organisation 
ein einen Lehrgang,
nenne ich es jetzt einfach mal
for lack of a better world,
wo mir beigebracht wird,
wie ich mit dem System 
interagiere.
Habe ich einen IQ, habe ich 
Gruppen Arbeitsgruppen,
die sich mit dem System 
beschäftigen,
habe ich die Möglichkeit,
bestimmte Fehler von diesem 
System",9.940217495
"In the paper 'The Productivity J-Curve' by Brynjolfsson et al., what does the Fortune 500 enterprise software company specialize in?",The company specializes in providing business process software for small and medium-sized U.S. businesses.,"The provided text doesn't contain information about ""The Productivity J-Curve"" by Brynjolfsson et al. or any details about a Fortune 500 enterprise software company. It only describes a lecture course on Machine Learning for Management Decisions.","Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 8. Mai 2025
MachineLearning für Management Entscheidungen (MLME)Lecture2Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz 23. Juli 2025
MachineLearning für Management Entscheidungen (MLME)Lecture11Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz",8.284939528
"According to the document, what is the economic decision rule for an investor?","'Invest'  if  p ⋅U (15,15)+ (1−p) ⋅ U(0,20) ≥U (10,0). 'Not invest' otherwise.","According to the document, the economic decision rule for an investor is:

“Invest” iff “𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)” “Not invest” otherwise.

Where:

*   𝑝 is the probability that the borrower would make a re-transfer.
*   𝑈15,15+1 is the utility of investing 15 million € and the borrower making a repayment.
*   𝑈0,20 is the utility of investing 0 € and the borrower making a repayment.
*   𝑈(10,0) is the utility of investing 10 mil € and the borrower making a repayment.","8. Mai 2025
Data driven decision-makingWhat you implicitly did (probably) are two parts:•Compared how they feel about certain outcomes•Assigned a probability 𝑝 that the borrower would make a re-transfer Choose the strategy that maximizes your „utility“ (whatever this is)Economic decision rule for investor: „Invest“  iff  𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)  „Not invest“ otherwise
12
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup : 20 mil €
Investor: 10 mil €Startup : 0 mil € human decisions from a nother prior empirical study 
(Study A, see below for more details) where we used 
the strategy method to measure participants ’ choices 
assuming the investor initially made the investment. 
That is, we kn ew whether each individual borrower 
would or would not make a repayment if an investor 
were to initially invest 10 MU with them. Importantly, 
due to the strategy method , we kn ew the borrower ’s 
conditional repayment decision regardless of the actual 
investor decision, i.e., we kn ew counterfactual 
outcomes. Game outcomes were determined by pairing 
a simulated investor decision with the borrower ’s 
actual conditional decision. 
Where do the ML system, selective labels problem, 
and continuous updating come into play? There are 
two pertinent aspects of the investment game. First, 
investors make initial decisions under uncertainty and 
form beliefs about the likelihood of borrowers making 
a repayment. Here, an ML  model prediction can 
decrease information asymmetries (Agrawal et al., 
2019) by providing individual-level predictions on the 
borrower’s repayment likelihood, increasing the 
potential for economically efficient outcomes. Second, 
investors only observe borrower behavior if they 8. Mai 2025
Data driven decision-making: an exampleConsider the following economic transaction
11
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup: 20 mil €
Investor: 10 mil €Startup: 0 €
What would you do, and why?",13.99036121
"According to the document, what are two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses?",A short-run dip and a long-run rebound.,"According to the document, the two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses are:

1.  **Initial Dip:** A short-run dip in productivity is measured, often due to heavy up-front investment.
2.  **Long-Run Rebound:** Productivity accelerates once complementary investments are in place, driven by learning effects, scale economies, and network externalities.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind",11.04088449
What is the date of the first lecture?,24. Apr 25,"The date of the first lecture is April 24, 2025.","24. April 2025
General informationLecture and tutorialLecture to introduce topics and cover theoryTutorial to work on applications (case studies & coding)Six tutorials by Charlotte Knickrehm How can you ask questions?Always ask questions on the runSend an e-mailOffice hours by appointmentWhat is expected of you?Prepare readingsIn class participation (lecture & tutorial)
20
This course is discussion-driven and relies a lot on your participation. 24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung 24. April 2025
Any questions on course structure?",4.918763876
What is the purpose of the tutorial sessions?,To work on applications (case studies & coding).,"The purpose of the tutorial sessions is to:

*   **Help participants implement ML projects** using the Python programming language.
*   **Develop a solid understanding of ML tools.**
*   **Recognize the value of ML tools for management decisions.**
*   **Teach students how to use generative AI models like GPT** through targeted prompting to address management questions and support decision-making.","23. Juli 2025
Clear & Specific
10
Before prompting•understand the context and the task you use the LLM for•think about what you want to achieveEffective prompts•clearly define objectives•feature direct and specific instructions•Give context•separate inputs and instructions consistently•Break up complex sentences Trainieren
selber gar nicht lange und
wir können sehr viele Kniffe 
benutzen,
wie wir dieses diesen
Trainingsprozess in einer 
gewissen Art
und Weise automatisieren können,
indem wir zum Beispiel
die Modellstruktur, die 
Modellauswahl,
sogar die Auswahl der relevanten
Inputinformationen durch 
bestimmte sogenannte
Cross Validation Verfahren durch
die Daten selber identifizieren 
lassen.
Ich hasse diesen Satz,
aber hier passiert dann sehr 
häufig dieses Let
the data speak so wenig anhaben 
wie möglich machen.
Wir versuchen einfach, das beste 
Modell zu finden.
Und auch das sei wieder betont, 
ist fundamental.
Uns geht es darum, ein Modell zu 
finden,
was nicht unsere historischen
Daten so gut es geht erklärt,
sondern sobald das Modell da 
ist,
möchten wir ein Modell haben,
was es uns ermöglicht,
Vorhersagen für die Zukunft zu 
machen,
also Vorhersagen für Beispiele,
Observationen, die das Modell 
noch nicht gesehen hat.
Denken Sie an das 
EiscremeBeispiel zurück.
Es ging nicht darum, perfekt zu 
erklären,
wie sich in der
Vergangenheit unsere 
Eisverkäufer abhängig
von bestimmten 
Wetterinformationen dargestellt 
haben,
sondern es ging darum Ich habe 
einen bestimmte
Logik identifiziert aus den 
historischen Daten. Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen.",9.427310944
"According to the document, what is a Class 2 customer?",A Profitable customer.,"The document does not define or mention ""Class 2 customer."" It describes customer data points and segments, but not customer classifications.","integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - 24. April 2025
Data to characterize customers
30
Initial cartBrowsing""Digital footprint"" (Berg et al. 2020)•Cart value•# of products•# of eco-friendly products•# of duplicate products
•Visited category ""Women""•Visited category ""Kids""•Visited category ""Sale""•Visited category ""Sport""•Visit during working hours
•Operating system•Internet Browser•Internet Provider•State (IP geolocation)",6.470340252
What companies were mentioned as having faced discrimination and harassment allegations in 2020?,"Amazon, McDonald’s, and Pinterest.","According to the provided text, Amazon, McDonald’s, and Pinterest were some of the companies that faced discrimination and harassment allegations in 2020.","against major companies like Amazon , 
McDonald’s, and Pinterest: These are some of 
the year ’s high -profile legal battles.  Business 
Insider. https://www.businessinsider.com/
every-company-that-was-sued-discrimination-
and-harassment-lawsuits-2020-2021-1  
Srivastava, M., Heidari, H., & Krause, A. (2019). 
Mathematical notions vs. human perception of 
fairness: A descriptive approach to fairness for 
machine learning. Proceedings of the 25th 
ACM SIGKDD International Conference on that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",6.348515511
What is the URL for the document by Cowgill and Tucker (2019)?,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3361280,"I am sorry, but the provided text does not contain the URL for the document by Cowgill and Tucker (2019).","DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org. ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - work as “Marketing Science. Copyright © 2024 The Author(s). https:/ /doi.org/10.1287/mksc.2022. 
0393, used under a Creative Commons Attribution License: https:/ /creativecommons.org/licenses/ 
by-nc-nd/4.0/.” 
Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE 
and the Deutsche Forschungsgemeinschaft [Grant 449023539]. 
Supplemental Material: The online appendix and data files are available at https:/ /doi.org/10.1287/mksc. 
2022.0393. 
Keywords: electronic commerce • nudging • causal forest • digital footprint • consumer returns • artificial intelligence
1. Introduction
In recent years, global e-commerce has experienced sig -
nificant and sustained growth. E-commerce presents 
numerous advantages for both customers and retailers. 
However, the lenient, and frequently free, return poli -
cies introduce drawbacks that complicate this expan -
sion. In the fashion industry, the share of purchases 
returned can reach a staggering 50% (Dzyabura et al. 
2023 ).
Product returns pose a substantial financial burden 
on retailers (Nageswaran et al. 2020 ). The costs of estab -
lishing return infrastructures (Ofek et al. 2011 ), incur -
ring transportation expenses (Zhou and Hinz 2016 ),",6.671740294
"In the 'Feedback Loops in Machine Learning' study, what did the authors note about the batch learning methodology?",This technique retrains the learning algorithm on all the currently available training data.,"The provided text doesn’t detail any specific observations about a “batch learning methodology” within the “Feedback Loops in Machine Learning” study. It primarily focuses on simulations built from data collected through Study A and Study B, which involved incentivized field studies and online experiments.","15. Mai 2025
(Unwanted) feedback loops
A core benefit of machine learning: continued learning•Tackles data drift issues •Update model using newly collected data regularly•Retraining v. incremental learning proceduresHowever, ML predictions influence decisions•Selective labels problems(Lakkarajuet al. 2017)•Can endogenize dataavailable for updates19
DataML ModelPredictionDecision
Feedback Feedback Loops in Machine Learning 
 
817 
Despite this result, the left panel in Figure 5 indicates 
that continuous updating could not entirely restore 
economic efficiency to the level it would have attained 
in the absence of initial label imbalances in the training 
data. Nevertheless, our findings underscore significant 
and noteworthy improvements in economic efficiency 
concerning female borrowers. For male borrowers, the 
development of both economic efficiency measures was 
nearly identical, regardless of the presence or absence of 
initial label imbalances for women in the training data. 
In presenting results for the most extreme label 
imbalance scenario —where no positive (repaying) 
examples exist for women —a question naturally 
arises concerning the role of the initial degree of label 
imbalance. Does the initial degree of label imbalance 
influence the ability of continuous updating to 
mitigate algorithmic discrimination and enhance 
economic efficiency?  
We address this question using regression analyses 
reported in Table 1 (See Figures A1-A6 in the Appendix 
for a graphical illustration). Columns (1) and (2) use the 
gender gap in the ML model ’s share of positive Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were",8.429867744
"According to the document, what is one practical implication for organizations regarding continuously updated ML models?",Organizations should invest in comprehensive anti-discrimination and unconscious bias training for their decision makers.,"According to the document, one practical implication for organizations is that they should “invest in comprehensive anti -discrimination and unconscious bias training for their decision makers.”","ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers. availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. long term. Organizations adopting a dynamic 
sociotechnological perspective might succeed in 
creating a more equitable, effective, and efficient use 
of ML models. This approach aligns with societal 
expectations and regulatory requirements. 
From a policymaker ’s perspective, our findings 
regarding human involvement become particularly 
significant in light of impending regulations that 
mandate human participation in ML decision -making 
processes. These regulations pertain to final decisions in 
high-risk scenarios such as loan approval and hiring 
processes, as outlined in Article 22 of the General Data 
Protection Regulation and the European Commission’s 
Artificial Intelligence Act. However, our findings 
indicate a potential risk when humans have the final say 
in ML -supported decision -making processes. If 
individuals engage in discriminatory practices and the 
models update continuously, there is a risk of 
unintentionally perpetuating or amplifying algorithmic 
discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by",6.345328331
"According to the document, what is one impact of human discrimination on the self-correction capabilities of continuously updated ML models?",It impedes the self-correction abilities.,"According to the document, one impact of human discrimination on the self-correction capabilities of continuously updated ML models is that “the greater the extent of human discrimination in a dynamic system where ML models are continuously updated, the less likely it is that self-correcting mechanisms will occur.” Furthermore, the models “unbiased models learn biased behaviors from humans over time.”","imbalance combined with high or very high levels of 
human discrimination (see Columns 4 and 5).  
In sum, these results emphasize that the capacity of 
continuous ML model updating to mitigate algorithmic 
discrimination critically relies on the absence of 
discriminatory behavior by human decision makers. If 
such behaviors persist —potentially giving rise to the 
initial label imbalance in the training data—continuous 
updating has a minimal effect on improving the ML 
model’s behavior toward the discriminated group. Our 
findings even suggest that human discrimination can 
cause ML models, originally trained on balanced data and 
displaying no algorithmic discrimination, to adopt 
increasingly discriminatory behaviors, as measured by 
statistical parity. In the context of our loan approval 
example, the results reveal that the efficacy of 
continuously updated ML models notably declines if a 
loan officer persistently discriminates against female 
applicants, regardless of the creditworthiness assessment 
generated by the ML model. Consequently, the ML 
model’s prediction may persistently demonstrate gender 
disparity across two key dimensions: the ML model ’s 
propensity to deem female applicants as creditworthy and the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",9.272341967
"According to the document, what is the approximate percentage point change in the share of positive predictions for female borrowers when there is no human discrimination and the maximum label imbalance is present?",22%,"According to the document, the share of positive predictions for female borrowers increases from 0% in Iteration 1 to 22% in Iteration 100 when there is no human discrimination and the maximum label imbalance is present.","investors’ probability to invest with female borrowers. 
Figure 6 shows that the share of positive predictions  
(the two panels at the top) and false-negative rates (two 
panels at the bottom) for female borrowers depends on 
the level of human discrimination and the degree of the 
initial label imbalance . To illustrate the change over 
time in a parsimonious way, we pre sent the 
visualization for the first and the last iteration only.  
Figure 6 reveals that the effectiveness of continuously 
updating an ML model in mitigating algorithmic 
discrimination diminishes in the presence of 
discrimination by human decision makers. Specifically, 
the extent of human discrimination negatively impacts the 
growth of positive predictions for female borrowers 
across iterations, regardless of the initial label imbalance. 
For instance, a comparison of the upper two panels shows 
that at the maximum label imbalance (x-axis), the share 
of positive predictions (y-axis) for women increases from 
0% in Iterations 1 (left panel) to 22% in Iteration 100 
(right panel) if there is no human discrimination (z-axis). 
However, this growth is significantly curtailed, reaching 
merely 1.2%, 6.9%, 14.3% under very high, high, and perfect information benchmark (for female borrowers) is 
consistent across conditions (approx. +0.2 percentage 
points per iteration). However, the initial degree of 
economic efficiency diminishes with increasing label 
imbalance. Regarding the optimal investment with 
repaying female borrowers (Column 3), the positive time 
trend is higher in medium, high, and maximum label 
imbalance conditions (respectively +0.09, +0.13, +0.15 
percentage points) than in situations with low or no initial 
label imbalance (reference category). 
In summary, our findings highlight that the observed self-
correction properties of continuous updating are 
moderated by the degree of initial label imbalance in the 
training data. This holds, in particular, for algorithmic 
discrimination as measured by the gender gap in false-
negative error rates. These results, when interpreted in the 
context of our loan approval example, suggest that the 
extent of historical discrimination against female 
applicants, as reflected in the training data, influences the 
effectiveness of continuous model updating in mitigating 
the gender gap in the ML prediction model. This result 
holds for both the gap in the likelihood that the model will investments with repaying borrowers in the lower two 
panels) for female borrowers from Iteration 1 to 100 , 
conditional on combinations of initial label imbalances 
(x-axis) and the extent of persistent human 
discrimination ( z-axis).15 Echoing our results on the 
role of human discrimination in mitigating algorithmic 
discrimination over time, we f ound that the ability of 
continuous updating to enhance economic efficiency 
decreases as human discrimination intensifies. For 
instance, a comparison of the upper two panels shows 
that when the initial label imbalance is at its maximum 
(x-axis), the share of game outcomes with female 
borrowers that match the perfect information 
benchmark (y-axis), grows from 42.3% In Iteration 1 
(left panel)  to 62.5% in Iteration 100 (right panel)  if 
there is no human discrimination ( z-axis). A similar 
comparison of the two panels at the bottom reveals that 
the share of investments with repaying borrowers also 
grows from 5.7% in Iteration 1 (left panel) to 34.9% in 
Iteration 100 (right panel). 
 
15 We present corresponding figures for male borrowers in 
Figures A9 and A10 in the Appendix. 
Conversely, under conditions of medium, high, and",10.37897491
"In the 'Feedback Loops in Machine Learning' study, what was the share of game outcomes matching the perfect information benchmark for female borrowers in the first iteration?",42.3%.,"The text states that with a high initial label imbalance, the false-negative error rate for female borrowers decreased from 55.4% in Iteration 1 to 35.8% in Iteration 100 if there was no human discrimination. It doesn’t directly state the share of game outcomes matching the perfect information benchmark in the first iteration.","perfect information benchmark (for female borrowers) is 
consistent across conditions (approx. +0.2 percentage 
points per iteration). However, the initial degree of 
economic efficiency diminishes with increasing label 
imbalance. Regarding the optimal investment with 
repaying female borrowers (Column 3), the positive time 
trend is higher in medium, high, and maximum label 
imbalance conditions (respectively +0.09, +0.13, +0.15 
percentage points) than in situations with low or no initial 
label imbalance (reference category). 
In summary, our findings highlight that the observed self-
correction properties of continuous updating are 
moderated by the degree of initial label imbalance in the 
training data. This holds, in particular, for algorithmic 
discrimination as measured by the gender gap in false-
negative error rates. These results, when interpreted in the 
context of our loan approval example, suggest that the 
extent of historical discrimination against female 
applicants, as reflected in the training data, influences the 
effectiveness of continuous model updating in mitigating 
the gender gap in the ML prediction model. This result 
holds for both the gap in the likelihood that the model will Feedback Loops in Machine Learning 
 
819 
 
 
Note: We illustrate how the mean share of positive predictions and mean false-negative error rates for female borrowers in Iterations 1 and 100 
depend on the initial label imbalance in the training data and the degree of human discrimination by the investor. A comparis on of panels for 
Iterations 1 and 100 for the two measures of algorithmic discrimination provides insights into their conditional development across iter ations. 
Abbreviations: False-neg. rate = false-negative rate; Init. lab. imb. = initial label imbalance; Human discr. = human discrimination. 
Figure 6. Development of the Mean Share of Positive Predictions and False-Negative Error Rates 
Figure 6 also shows that an increase in human 
discrimination undermines the reduction in false -
negative error rates for female borrowers over time. 
For instance, the lower two panels show that with a 
high initial label imbalance (x-axis), the false-negative 
error rate (y-axis) for female borrowers decreases from 
55.4% in Iteration 1 (left panel)  to 35.8% in Iteration 
100 (right panel)  if there is no human discrimination  
(z-axis). However, under very high human 
discrimination, this rate only drops to 52.2%  in fold to 34.8%, signifying an impressive gain in 
efficiency (see dashed line with marker in Panel i). In 
our loan approval example, continuous updating of the 
ML model increase d the number of repaying female 
applicants who could secure credit. 
 
Note: We depict the development of the mean share of game outcomes that are equal to outcomes under perfect information and mean shares of 
games where investors invest in repaying borrowers. We show results separately for female and male borrowers. The left Panel (i) depicts results 
for the condition where the initial label imbalance for the minority class was at the maximum, whereas the right Panel (ii) depicts results for the 
condition where there was no initial label imbalance. 
Figure 5. Development of the Mean Share of Game Outcomes",11.49132943
"In the ""Feedback Loops in Machine Learning"" study, what type of model was utilized to predict whether an investment would occur in the investment game simulation?",A gradient boosted forest.,The study utilized a trained machine learning model to predict whether an investment would occur. The model leveraged 31 features to predict whether a given investor would invest with a given borrower.,"Feedback Loops in Machine Learning 
 
813 
Armed with this information, the investor decided 
whether to invest. Notably, in our simulation , we 
mimicked investor decisions using a trained model (see 
below for more information). If an investment was 
made, the borrower decided on repayment, and a new 
observation encapsulating a borrower’s specific 
personal traits (features) and actual repayment decision 
(label) was appended to the dataset used to train the ML 
model predicting repayment. 
Importantly, to avoid inappropriate overfitting and 
spillover effects, this observation was only included if 
the dataset didn’t already contain a record of this exact 
borrower. Conversely, if an investor declined to invest, 
the individual game conclude d without eliciting a 
repayment decision from the borrower or appending a 
new observation to the training data. In other words, the 
growth of the training data was endogenous and 
selectively include d borrowers who received an 
investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were The second random part of our Study B data populated 
our pool of investors for our simulation (303 unique 
participants from Study B), whose observations we 
excluded from the model’s training. Each investor was 
characterized by their 20 personal attributes, and their 
investment decisions for a specific borrower were 
simulated. When a certain investor from the pool was 
randomly selected in an iteration and paired with a 
borrower, the trained model predict ed whether an 
investment would occur. The prediction of the 
likelihood of an investment occurring was based on the 
investor’s 20 personal characteristics, the borrower ’s 
10 characteristics, and a repayment forecast for the 
borrower, i.e., the model leverage d 31 features to 
predict whether a given investor w ould invest with a 
given borrower. An investment took place only if the 
predicted probability surpasse d the threshold of 50 % 
(in the condition where there was no human 
discrimination). When using the borrowers the 
individuals from our investor pool actually i nteracted 
with in Study B as a true out -of-sample test set and a 
50% threshold, we found that, on average, the trained 
model correctly replicated investor decisions in",8.413955688
What are some contexts in which algorithmic discrimination has been observed?,"Algorithmic discrimination has been observed in recidivism risk assessments, education, recruitment, predictive policing, health risk evaluations, targeted advertising, crowdlending scenarios, and facial recognition tasks.","Based on the provided text, here are some contexts where algorithmic discrimination has been observed:

*   **Algorithmic Recruitment:** Bias can arise in the development of rules for algorithmic recruitment.
*   **Forecasting:** Historical inequalities are projected into the future by algorithms.
*   **Development of Algorithmic Rules:** Bias can be introduced during the development of rules by engineers and when annotators handle unstructured data.

The text suggests that this bias can occur in various stages, including data collection, model construction, and feature selection.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",10.83729005
What is algorithmic discrimination?,Algorithmic discrimination refers to discriminatory hiring practices that arise from algorithmic bias.,"According to the text, algorithmic discrimination is:

“Discrimination involves statements or actions that unjustifiably disadvantage members of particular social groups. These actions typically stem from conscious or subconscious attitudes, prejudices, or emotional associations tied to characteristics such as gender, age, disability, race, language, religion, or sexual identity.”

More simply, it’s when algorithmic systems (based on ML models) produce outputs that disproportionately benefit or disadvantage specific groups, even when there’s no intended justification for that disparity. It can arise from biases in the training data itself or from the way engineers design the algorithms.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately Despite algorithms aiming for objectivity and clarity in their
procedures, they can become biased when they receive partial
input data from humans. Modern algorithms may appear neutral
but can disproportionately harm protected class members, posing
the risk of“agentic discrimination”(Prince and Schwarcz,2019).
If mishandled, algorithms can exacerbate inequalities and
perpetuate discrimination against minority groups (Lloyd,2018).
Within the recruitment process, algorithmic bias can manifest
concerning gender, race, color, and personality.
Research methodology
The primary research strategy was a literature review approach.
This review aimed to assess current research on recruitment
supported by artiﬁcial intelligence algorithms. The systematic
review process included gathering and evaluating the selected
studies’ literature and topics. Driven by the direction of the
research, studies focusing on algorithmic discrimination in
recruitment over the past 10 years were included unless past
literature was worth reviewing. This is because this is a relatively
new phenomenon that has become prominent over the past 10
years. In deﬁning the “algorithmic and hiring discrimination”
literature, a fairly broad approach was taken based on article discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible",11.51142979
"According to the document, what software was used as an auxiliary tool to clarify ideas and improve work efficiency?",Nvivo 12.0 Plus qualitative analysis software.,You are absolutely correct. The text does not mention any specific software used for clarification or efficiency.,"her sich zog.
Das ist ein Beispiel für so ein 
Application Solution.
Fragen bis hierher irgendwas
unklar auf diesen verschiedenen 
Leveln.
Wir fangen bei keiner 
Integration an,
einzelne Prediction gehen 
Schritt weiter.
Wir integrieren das System 
besser in unsere Workflows.
Wir erkennen notwendige andere 
Veränderungen
innerhalb des Workflows.
Ja. Zum Beispiel, wenn wir jetzt 
also sagen wir mal,
im Bewerbungsverfahren,
eine automatisierte Bewerbung 
nach Sibirien oder
sowas durchsucht werden.
Wie ist es, wenn also der
Spieler dem System quasi 
konfrontiert ist mit
Menschen zu predigten,
aber die sind in der Produktion.
Also wie sinnvoll ist dann so 
ein System? Sehr gut.
Ich würde Ihre Frage ein
paar Minuten nach hinten 
schieben,
weil das ein Teil der
zweiten Hälfte der Vorlesung 
heute ist,
wo wir über Probleme sprechen 
Und was Sie ansprechen,
nennt man ein Feedbackloop bzw 
einen
sogenannten Data Drift.
Sobald Menschen oder auch
ganze Systeme von dem
Einsatz von Machine Learning 
Systemen
wissen, könnte es sein,
dass Menschen ihre Art und 
Weise,
sich zu verhalten, ändern,
um dieses das System 
auszutricksen,
indem bestimmte neue 
Fachbegriffe,
wo man denkt, das ist, wo die 
Maschine drauf anspricht. nachzudenken,
ist diese Dekomposition von 
Workflows und die
Organisation von solchen 
Projekten
kann stattfinden über diese 
Scrips Modell.
Und letztlich haben wir 
unterschiedliche Level der
Integration von diesen Machine 
Learning Innovationen
innerhalb der Organisation.
Hört sich alles super an!
Problem ist da kann massiv viel 
schiefgehen.
Selbst wenn ich diesen ganzen 
Prozess durchgedacht habe,
sind es an vielen Ecken immer 
noch Probleme,
die sich beispielsweise hier bei 
Amazon vor einigen
Jahren trotz hochintelligenter 
Leute,
trotz super Strukturen,
trotz tollen technologischen 
Möglichkeiten
ist es da zu einem großen 
Skandal gekommen,
die man von diesem Skandal 
damals gehört.
Müsste ungefähr in die Zeit, wo 
sie alle
Abitur gemacht haben, gewesen 
sein.
Ja, das war weil die.
Also ich bin mir nicht mehr 
sicher.
Ich glaube, da wurden die 
früheren
Recruitingdaten von Amazon 
genommen,
in eine AI eingespeist.
Und da die früheren Recruiter
biased gegenüber Frauen waren,
hat die AI das Muster erkannt 
und ebenso übernommen.
Genau.
Wir haben ein System
gehabt und ich würde gar nicht 
mal sagen,
dass die Recruiter per se 
gebeizt waren.
Ich wahrscheinlich auch ein 
Teil.
Ich will den Leuten da nichts 
unterstellen. den besten Bewerber.
Finde aus einer größeren Menge 
von möglichen Bewerbern.
Ich denke also wieder mit Was 
sollte sich innerhalb
meines Prozesses
innerhalb dieses Workflows auch 
verändern?
Integriert mehr Integration?
Und ein Beispiel hierfür ist 
Ada. Kennt das jemand?
Hat jemand schon mal gehört,
es war ein Startup,
was in
der Pre Curve Zeit ein Customer 
Service Angebot hatte?
Die haben die Automatisierung
damals auch nicht mitgenerativen 
KI Systemen.
Aber ein sehr
clever zusammengestelltes 
Applikationssystem,
wo Customer Compliance gehandelt 
worden sind.
Das wurde aufgekauft innerhalb 
der
kurze Zeit von einem uns
allen bekannten großen Provider 
von digitalen Calls,
nämlich Zoom Zoom hat 
festgestellt,
als alle in die digitalen
Räume zu kurze Zeit gegangen 
sind,
dass es sehr viele Beschwerden
insbesondere von bestimmten 
Altersgruppen gab,
die mit dieser Technologie nicht 
klargekommen sind,
was dazu geführt hat,
dass dieser Ansturm, dieser 
Komplements,
die gesamte Service und
die gesamte Marketing Abteilung 
von Zoom
eigentlich den ganzen Tag auf 
Trab gehalten hatten.
Sie konnte trotzdem nicht alles
abarbeiten und dieses System,
was sie installiert haben,
war jetzt nicht einfach nur Ich 
habe ein System,",7.300182104
What was the result of Microsoft revising their dataset for training the Face API?,A 20-fold reduction in the recognition error ratio between men and women with darker skin tones.,"According to the text, Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones and a 9-fold reduction for women.","uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold
reduction for women by balancing factors such as skin color, age,
and gender (Grabovskyi and Martynovych,2019).
Integrating “small data” and “big data” can enhance accuracy
(36KE, 2020). Data should not solely rely on extensive collections
but also focus on precision. While big data analysis tends to
emphasize correlations, which can lead to errors when inferring
causation, small data, which is more user-speciﬁc, offers detailed
information and helps avoid such mistakes. Combining the
vastness of big data with the precision of small data can help
somewhat mitigate hiring errors (Kitchin and Lauriault,2015).
Biases in datasets can be identi ﬁed through autonomous
testing. The inaccuracies stemming from incomplete past data
can be addressed through “oversampling” (Bornstein, 2018).
Researchers from MIT demonstrated how an AI system called
DB-VEA (unsupervised learning) can automatically reduce bias
by re-sampling data. This approach allows the model to learn one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation,
determining decision boundaries, and detecting algorithmic
fairness through interactive visual interfaces. Additionally, Face-
book has developed Fairness Flow, an emerging tool for
correcting algorithmic bias. Fairness Flow automatically notiﬁes
developers if an algorithm makes unfair judgments based on race,
gender, or age (Kessing,2021).
Improving the algorithm ’s ethics from a management
perspective
Internal ethics governance. Several major technology companies
have published AI principles addressing bias governance, sig-
naling the start of self-regulation (36KE, 2020). Microsoft has
formed an AI and ethical standards committee to enforce these
principles, subjecting all future AI products to ethics scrutiny
(Smith and Shum,2018). Google has responded by introducing a
Model Card function, similar to an algorithm manual, that
explains the employed algorithm, highlights strengths and
weaknesses, and even shares operational results from various
datasets (Mitchell et al.,2019). a data -generating process distinct from subsequent 
simulation iterations, then the ML model’s bias against 
women essentially emerges from a concept dri ft 
(Widmer & Kubat, 1996). This means the initial ML 
model approximates a once-accurate but now outdated 
data-generating process. The observed feedback loop, 
where the ML model progressively becomes less 
biased, then demonstrates its ability to overcome 
concept drifts when updated with new training data it 
influences. Conversely, persistent human 
discrimination shifts the training data ’s gender 
composition, increasingly diverging from the borrower 
pool over iterations. However, the core relationship 
between borrower attributes and their repayment 
propensity remains unchanged. Thus, ongoing human 
discrimination aligns with a n ongoing  data drift 
(Mallick et al. , 2022), as the incoming data ’s 
composition on which the model predicts, deviates 
from its training data. With biased decision-making by 
humans in the loop  undermining the discriminating 
ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that",8.487809658
What did Amazon do with their ML-based hiring tool?,Amazon withdrew the tool and developed a new unbiased algorithm.,"According to the text, Amazon withdrew their ML-based hiring tool after it exhibited gender bias. They then developed a new, unbiased algorithm.","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not nachzudenken,
ist diese Dekomposition von 
Workflows und die
Organisation von solchen 
Projekten
kann stattfinden über diese 
Scrips Modell.
Und letztlich haben wir 
unterschiedliche Level der
Integration von diesen Machine 
Learning Innovationen
innerhalb der Organisation.
Hört sich alles super an!
Problem ist da kann massiv viel 
schiefgehen.
Selbst wenn ich diesen ganzen 
Prozess durchgedacht habe,
sind es an vielen Ecken immer 
noch Probleme,
die sich beispielsweise hier bei 
Amazon vor einigen
Jahren trotz hochintelligenter 
Leute,
trotz super Strukturen,
trotz tollen technologischen 
Möglichkeiten
ist es da zu einem großen 
Skandal gekommen,
die man von diesem Skandal 
damals gehört.
Müsste ungefähr in die Zeit, wo 
sie alle
Abitur gemacht haben, gewesen 
sein.
Ja, das war weil die.
Also ich bin mir nicht mehr 
sicher.
Ich glaube, da wurden die 
früheren
Recruitingdaten von Amazon 
genommen,
in eine AI eingespeist.
Und da die früheren Recruiter
biased gegenüber Frauen waren,
hat die AI das Muster erkannt 
und ebenso übernommen.
Genau.
Wir haben ein System
gehabt und ich würde gar nicht 
mal sagen,
dass die Recruiter per se 
gebeizt waren.
Ich wahrscheinlich auch ein 
Teil.
Ich will den Leuten da nichts 
unterstellen.",7.224973202
"According to the document, what is “agent discrimination”?","When partial human data is provided to a machine, it will eventually lead to the risk of “agent discrimination.”","The document does not define “agent discrimination.” It outlines four theoretical groups explaining discrimination in algorithmic recruitment:

1.  Competitive market theory
2.  Monopoly model of discrimination
3.  Statistical theory of discrimination
4.  Antecedent market discrimination hypothesis

It focuses on broader theories of discrimination rather than specific types like “agent discrimination.”","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human
Rights (ECHR) guarantees the rights and freedoms outlined in
the Convention, prohibiting direct and indirect discrimination
(Zuiderveen Borgesius, 2020). Non-discrimination laws, particu-
larly those about indirect discrimination, serve as a means to
prevent various forms of algorithmic discrimination. The EU
General Data Protection Regulation (GDPR), implemented in
May 2018, addresses the impact of ML algorithms and offers a
“right to explanation” (e.g., Articles 13–15) (Peña et al., 2020),
enabling individuals to request explanations for algorithmic
decisions and demand measures to avoid discriminatory
inﬂuences when handling sensitive data. The GDPR mandates
organizations to conduct a Data Protection Impact Assessment
(DPIA), with each EU member state must maintain an
independent data protection authority vested with investigative
powers. Under the GDPR, a data protection authority can access
an organization’s premises and computers using personal data
(Zuiderveen Borgesius, 2020).
Investigation and analysis technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender",8.620527983
"According to the document, what are some characteristics used in discrimination theory?","Race, ethnicity, color, and gender.","According to the document, here are some characteristics used in discrimination theory:

*   **Race, ethnicity, color, and gender** (as defined by the ILO’s Convention 111)
*   **Ethnic origin, gender, skin color, and age**
*   **Inadequate information** (as a non-objective variable)

The document also outlines four broad theoretical groups for understanding discrimination:

1.  Competitive market theory
2.  Monopoly model of discrimination
3.  Statistical theory of discrimination
4.  Antecedent market discrimination hypothesis","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",11.85112095
What is the generic C R ISP-D M process model useful for?,"Planning, documentation and communication.","According to the text, the generic CRISP-DM process model is useful for:

*   **Planning**
*   **Communication** within and outside the project team
*   **Documentation**","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",7.692582846
"According to the document, what can AI-based recruitment reduce?",Costs.,"According to the document, AI-based recruitment can reduce:

*   **Costs**
*   **Time**
*   **Transactional workload**","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal-
Chérif et al. (2021)
Suggests that AI support can save time
Upadhyay and Khandelwal (2018), Johansson and Herranen
(2019)
Propose that AI support can reduce transactional
workload
2. Causes of algorithmic
discrimination
McFarland and McFarland (2015), Mayson (2018), Raso et al.
(2018), Raub (2018), Raghavan et al. (2020), Njoto (2020),
Zixun (2020), and Jackson (2021)
Suggests that the reason for algorithmic
discrimination is related to data selection
Yarger et al. (2019), Miasato and Silva (2019), and Njoto
(2020)
Propose that discrimination is due to the
designer-induced selection of data features
3. Algorithmic recruitment
discrimination
Correll et al. (2007), Kay et al. (2015), O’neil (2016), Raso et al.
(2018), Miasato and Silva (2019), Langenkamp et al. (2019),
Faragher (2019), Ong (2019), Fernández and Fernández (2019),
Beneduce (2020), Jackson (2021), Yarger et al. (2023), and
Avery et al. (2023)
In recruitment, algorithmic bias can manifest in
gender, race, skin color, and personality",7.483771801
"According to the document, what is the aim of the C R ISP-D M process model?","To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the document, the aim of the CRISP-DM process model is to specify a process that can be reliably and efficiently repeated by different people and adapted to different situations. It’s designed to be used by both experienced data mining people and those with lower technical skills.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",7.905348301
