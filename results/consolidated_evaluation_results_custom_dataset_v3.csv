question,ground_truth_answer,generated_answer,retrieved_context,response_time_seconds,correctness_score,correctness_reasoning,faithfulness_score,faithfulness_reasoning,relevance_score,relevance_reasoning
"In the paper about the Centaur foundation model, what happens to the model's internal representations after being fine-tuned on the Psych-101 dataset?",The model's internal representations become more aligned with human neural activity.,The provided text doesn’t contain information about the Centaur foundation model or what happens to its internal representations after being fine-tuned on the Psych-101 dataset. It discusses a different machine learning model and a general approach to understanding how such models work.,"die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen. eher das Modell nehmen, was 
einfacher ist.
Der Gedanke, der dahinter 
steckt,
ist, dass wir vermeiden wollen,
dass die zufälligen Schwankungen 
in den
Daten mit höherer 
Modellkomplexität
in dem Modell erkannt werden als 
wichtige Muster.
Time ist jetzt auch ein bisschen 
drauf gucken,
dass ich meinen Time Management 
richtig betreibe,
weil wie Sie vielleicht gemerkt 
haben,
habe ich die Tendenz, viel zu 
reden.
Kommen Sie gerne nach der 
Vorlesung,
dann können wir darüber 
sprechen.
Wenn wir das verallgemeinern und 
jetzt nicht nur
über unser EiscremeBeispiel 
nachdenken,
sondern ganz, ganz, ganz High 
Level aus einer fast schon.
Und Uwe Walz würde sich freuen,
wenn diese Formel hier angezeigt 
wird.
Wahrscheinlich aus einer 
mikroökonomischen
Perspektive kann man
natürlich fragen Warum brauchen 
wir Vorhersagen?
Und aus einer ökonomischen
Sicht Wenn ich bestimmte 
Entscheidungsprobleme,
Nutzenmaximierungsprobleme,
Gewinnmaximierungsprobleme 
aufschreibe, dann ist immer so
ein bisschen in
fast allen Optimierungsproblemen 
uncertainty mit drin.
Bestimmte Sachen weiß ich 
einfach nicht.
Es kommt so ein bisschen darauf 
an,
was in der Zukunft passieren 
wird.
Wenn ich heute investiere,
wenn ich eigentlich mein",20.28423309,0,"The generated answer claims the information is not in the provided text, while the ground truth provides a specific, factual answer to the question. The two are in complete opposition.",1,"The generated answer is a faithful summary of the provided context. The context discusses a model by ""Venkatesh and colleagues"" and general machine learning concepts, and it does not contain any information about the ""Centaur foundation model"" or the ""Psych-101 dataset.""",0,"The context does not mention the ""Centaur foundation model"" or the ""Psych-101 dataset."" It discusses a different model related to technology adoption and general machine learning concepts, making it entirely irrelevant to the specific question asked."
"In the study on capturing human cognition, how does the Centaur model's performance compare to existing cognitive models?",Centaur captures the behaviour of held-out participants better than existing cognitive models.,"According to the text, the Centaur model, developed by Venkatesh and colleagues, achieved approximately 70% accuracy in explaining the variation in whether people use technology. The text highlights this as ""massively"" significant, especially within sociotechnological systems where human factors are crucial. 

However, the text doesn't directly compare the Centaur model’s performance to *other* existing cognitive models. It simply states that its 70% accuracy is a substantial result. 

Do you want me to try and infer something about its performance relative to other models based on the information provided?","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",11.55007291,0,"The generated answer directly contradicts the ground truth. The ground truth states that the Centaur model performs better than existing cognitive models, whereas the generated answer explicitly claims that the text does not make such a comparison.",0.8,"The generation is largely faithful, accurately reporting that a model by Venkatesh and colleagues explained about 70% of variation in technology adoption, which the text calls ""massiv."" It also correctly states that the text does not compare this model to others. However, it unfaithfully introduces the name ""Centaur model,"" which is not mentioned in the provided context.",0.3,"The context discusses the high performance of a model by ""Venkatesh and colleagues"" in explaining technology adoption (70% variation explained), which might be the model the user is referring to as ""Centaur."" However, it does not explicitly name the model ""Centaur"" and, more importantly, it does not provide any comparison to *other* existing cognitive models as requested by the question. The focus is on technology adoption, which is a subset of human cognition."
"According to the paper on the Centaur foundation model, what is it designed to predict and simulate regarding human cognition?",It can predict and simulate human behaviour in any experiment expressible in natural language.,"The text doesn't provide information about a ""Centaur foundation model."" It describes the use of a mathematical model of a brain cell (a perceptron) and a modified investment game to simulate economic transactions. It focuses on the ethical and practical challenges of studying human cognition and doesn't detail a specific model like ""Centaur.""","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization robust and widely  accepted tool across multiple 
scientific fields, facilitating the analysis of dynamic 
processes (Kahalé, 2020). Such simulations also 
enable the generation of proofs of concept for novel 
ideas (Wong & Kwong, 2018). We opt for this research 
methodology primarily because the exogenous 
introduction of discrimination, particularly by humans, 
is not only ethically untenable but also practically 
unfeasible in experimental or field settings. 
Our simulation employs a modified version of the 
investment game (Berg et al., 1995). This game 
emulates the incentives and informational structure 
inherent in numerous economic transactions that 
transpire in environments lacking robust enforcement 
mechanisms (Dufwenberg & Kirchsteiger, 2004; Feh r 
& Fischbacher, 2003). In our game, two participants, 
an investor and a borrower, each begin with an 
endowment of 10 monetary units (MU). The investor 
first chooses whether to retain or invest the entire 10 
MU with the borrower. If the investor retains the sum, 
the game ends and both parties keep their initial 
endowment of 10 MU, representing a non -
transactional scenario, e.g., where a bank loan officer 
does not approve a loan. Alternatively, if the investor Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",8.308384657,0,"The generated answer states that the requested information about the ""Centaur foundation model"" is not available in the text. The ground truth provides a specific answer about what the model is designed to do. The two answers are completely different.",1,"The generated answer is completely faithful to the context. It correctly states that the ""Centaur foundation model"" is not mentioned and accurately summarizes the actual content, which includes discussion of a perceptron, a modified investment game, and the ethical reasons for using simulations.",0,"The provided context does not mention the ""Centaur foundation model"" at all. It discusses a simulation using a modified ""investment game"" to study economic transactions and generally explains the purpose of machine learning models to make predictions on new data. Therefore, the context is irrelevant to the question."
"For the Centaur model, what is the name and scale of the dataset used for fine-tuning to capture human cognition?","Psych-101, which covers trial-by-trial data from over 60,000 participants in 160 experiments.","The text doesn't provide the name or scale of the dataset used for fine-tuning the Centaur model. It mentions that Venkatesh and colleagues applied the model to ""many empirical data"" but doesn't specify the dataset's name or size.","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen. die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell",7.786422491,0,"The generated answer claims the information about the dataset's name and scale is not in the text, whereas the ground truth provides the exact name and scale. The two are contradictory.",1,"The generated answer is fully faithful. It correctly states that the context does not provide a specific name or scale for the dataset and accurately reports that the model was applied to ""many empirical data,"" which is a direct translation of the German phrase ""ganz viele empirische Daten"" found in the source.",0.2,"The context mentions that the model by ""Venkatesh and colleagues"" was applied to ""ganz viele empirische Daten"" (a lot of empirical data), which hints at the use of a dataset. However, it completely fails to provide the specific name and scale of the dataset requested in the question and does not even confirm that the model is the ""Centaur"" model."
In the paper 'A foundation model to predict and capture human cognition' what is the name of the computational model introduced?,Centaur.,The paper introduces a mathematical model of a brain cell: a **perceptron**.,"10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization 12. Juni 2025
Please answer the following questions:(a) Label the components of the following equation according to our definition in the lecture. (b) Explain each component in 1-2 short sentences in your own word. (c) Based on the components, explain the overarching goal of machine learning models.
4
Learned modelFrom historic data
The objective of ML is to make predictions !𝒚given new data points 𝒙for which we do not know the true outcome 𝒚
!𝑓(𝑥) &𝑦Input information:Tabular dataImagesText…
Prediction:Loan defaultStock priceCustomer retention…
=
?
? ? Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",4.652276993,0,"The generated answer and the ground truth provide two completely different and conflicting names for the computational model introduced in the paper (""perceptron"" vs. ""Centaur"").",1,"The generated answer is a direct and accurate statement taken from the provided context. The context includes a slide titled ""A mathematical model of a brain cell: perceptron.""",1,"The context directly answers the question by providing the name of the computational model introduced in the document. The text explicitly states, ""A mathematical model of a brain cell: perceptron."""
The paper 'Algorithmic Bias?' concludes that optimizing for what factor can lead to apparently discriminatory outcomes in ad delivery?,An algorithm that simply optimizes for cost-effectiveness.,"According to the paper, apparently discriminatory outcomes in ad delivery can be caused by nonobjective variables, such as inadequate information.","that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that 15. Mai 2025
Consequences of algorithmic biasIndividual consequences include•rejection or discontinuance of ML tools.•customers paying higher prices than usual Organisational consequences include •violating equal opportunity policies•high customer churn rate Societal-level consequences include •increased gaps between historically disadvantaged groups and others•Fostered polarization and human discrimination
17 significant consequences, such as in healthcare, loan 
approval, hiring, or criminal justice systems. 
Mitigating algorithmic discrimination : The 
expansive literature on algorithmic discrimination 
proposes various methods to identify and alleviate 
such biases. According to d ’Alessandro et al. (2017), 
we can divide these mitigation strategies into three 
categories: pre -processing, in -processing, and post -
processing. Pre -processing strategies strive to 
eliminate disparities inherent in the dataset. Methods 
such as those proposed by Kamiran and Calders (2012) 
and Zafar et al. (2017) employ data pre -processing 
techniques like feature massaging and reweighting to 
balance the dataset based on social group 
characteristics. In -processing strategies involve 
conventional learning algorit hms during model 
training. Researchers often use techniques like naive 
Bayes and decision tr ees for debiasing (Calders & 
Verwer, 2010; Zhang et al., 2018). Additional research 
suggests placing constraints on the classification 
model to mandate the satisfaction of a proxy 
(Woodworth et al., 2017). Lastly, post -processing 
strategies aim to adjust the output to counter biases. 
For example, Corbett -Davies et al. (2017)",5.392766953,0.5,"The generated answer identifies ""nonobjective variables, such as inadequate information,"" as the cause, which is a related but broader concept than the more specific ground truth answer, ""An algorithm that simply optimizes for cost-effectiveness."" While related, they are not semantically equivalent.",1,"The generated answer is faithful as it directly paraphrases a statement from the context. The context states, ""that nonobjective variables, such as inadequate information, contribute to biased outcomes (Dickinson and Oaxaca, 2009)""[cite: 89].",1,"The context directly supports the idea that optimizing for cost-effectiveness can lead to discriminatory outcomes. It states that statistical discrimination arises from the ""cost constraint of obtaining information"" and is ""efï¬cient under imperfect information and aligns with the employerâ€™s proï¬t maximization goal,"" which is the core concept behind optimizing for cost-effectiveness."
"In the context of the STEM career ad study, what was the difference in click behavior between men and women who were shown the ad?","Among those who saw the ad, women were more likely to click on it than men.","According to the provided text, Lambrecht and Tucker’s study found that apparent gender-based discrimination was evident in the display of STEM career ads. Specifically, women showed less interest in the ads compared to men. 

Here’s the key detail: “women showed less interest in the ads compared to men.”","Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)
from an organizational behavior perspective. While recent studies
have primarily discussed discrimination theory in the traditional
economy’s hiring market, future theoretical research should
consider how advanced technology affects equity in hiring within
the digital economy.
Conclusion
The study concludes that the fo urth industrial revolution
introduced technological innovations signiﬁcantly affecting the
recruitment industry. It extends the analysis of statistical dis-
crimination theory in the digital age and adopts a literature
review approach to explore four themes related to AI-based
recruitment. The study argues that algorithmic bias remains an
issue while AI recruitment tools offer beneﬁts such as improved
recruitment quality, cost reduction, and increased ef ﬁciency.
Recruitment algorithms ’bias is evident in gender, race, color,
and personality. The primary source of algorithmic bias lies in
partial historical data. The personal preferences of algorithm
engineers also contribute to algorithmic bias. Technical mea- according to the trends of other minority or female applicants,
which may differ from the features identi ﬁed as successful
representatives.
Word embedding. Microsoft researchers found that words
exhibit distinct associations in news and web data. For instance,
words like “fashion” and “knitting” are more closely related to
females, while “hero” and “genius” are more closely related to
males (36KE, 2020). Microsoft suggests a simple solution by
removing the gender-speciﬁc measures in word embedding to
reduce “presentation bias.”
Differential testing. Scientists at Columbia University devel-
oped Deep Xplore, a software that highlights vulnerabilities in
algorithmic neural networks via “coaxing” the system to make
mistakes (Xie et al., 2018). Deep Xplore utilizes discrepancy
testing, which involves comparing several systems and observing
their outputs’differences. A model is considered vulnerable if all
other models consistently predict a particular input while only
one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation,",8.69923377,0,"The generated answer directly contradicts the ground truth. It states that women showed ""less interest,"" while the ground truth states that women were ""more likely to click on it than men.""",0,"The generated answer is not faithful. It makes a specific factual claim (""women showed less interest in the ads compared to men"") that is not mentioned anywhere in the provided context. The context consists of a reference list and unrelated text about algorithmic bias tools.",0,"The context is irrelevant. Although it includes a bibliographic reference to the study in question, the actual text provided does not contain any information about the click behavior of men versus women in that study. The snippets discuss other topics like word embeddings and bias detection tools."
"In the empirical study of gender-based discrimination in STEM ad display, what reason is given for why fewer women saw the advertisement?","Younger women are a prized demographic and are more expensive to show ads to, so a cost-optimizing algorithm shows the ad to them less often.","According to the text, the reason fewer women saw the STEM career ads was because the models correlated hobbies and universities attended – specifically, universities with historically higher male enrollment – which were identified as successful representatives.","Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., according to the trends of other minority or female applicants,
which may differ from the features identi ﬁed as successful
representatives.
Word embedding. Microsoft researchers found that words
exhibit distinct associations in news and web data. For instance,
words like “fashion” and “knitting” are more closely related to
females, while “hero” and “genius” are more closely related to
males (36KE, 2020). Microsoft suggests a simple solution by
removing the gender-speciﬁc measures in word embedding to
reduce “presentation bias.”
Differential testing. Scientists at Columbia University devel-
oped Deep Xplore, a software that highlights vulnerabilities in
algorithmic neural networks via “coaxing” the system to make
mistakes (Xie et al., 2018). Deep Xplore utilizes discrepancy
testing, which involves comparing several systems and observing
their outputs’differences. A model is considered vulnerable if all
other models consistently predict a particular input while only
one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation, mit einem bestimmten Geschlecht.
Ich kann über ganz viele andere
korrelative Maße ein sehr
guter Einschätzung davon 
bekommen,
ob sie sich mit
einem bestimmten Geschlecht 
identifizieren.
Hier in diesem Fall war es 
tatsächlich die
Hobbys und die Universitäten,
an denen bestimmte Abschlüsse 
gemacht worden sind,
weil bestimmte Universitäten in 
diesen bestimmten
Universitäten waren die 
Männerquoten massiv viel höher.
Und bei diesen Universitäten mit 
Harvard,
Stanford, da wurde mehr 
rekrutiert.
Historisch gesehen sind aber die 
Anteile
der Frauen in
diesen Universitäten deutlich 
geringer gewesen.
Das ändert sich zum Glück jetzt.
Aber wenn ich historische Daten 
nehme,
um ein Modell zu bauen,
erkennt dieses Modell 
Korrelationen,
die eigentlich gar nicht dort 
sein dürften.
Per Gesetz gibt es viele andere 
Beispiele.
Ein sehr extremes Beispiel ist,
dass zu einem gewissen Zeitpunkt 
die
Google KI Bei automatisierten
Labeln von Bildern angefangen 
hat.
Personen mit dunklerer Hautfarbe 
häufiger als
etwas anderes zu erkennen.
In diesem sehr extremen Beispiel 
hier wurde
es jetzt als Affe betitelt,
was natürlich hochgradig 
rassistisch ist.
Was denken Sie, ist hier schief 
gelaufen?
Wenn eine Awareness dafür 
schaffen,",6.743628263,0,"The generated answer and the ground truth provide two completely different reasons for why fewer women saw the ad. The generation cites correlations with hobbies and universities, while the ground truth cites the higher cost of advertising to the demographic of younger women.",1,"The generated answer is faithful. [cite_start]It accurately summarizes the information provided in the context, which states that the model used correlations with ""Hobbys und die UniversitÃ¤ten"" [cite: 156] [cite_start]and that historical data from universities with higher male enrollment rates [cite: 156, 158] led to this outcome. [cite_start]The phrase ""successful representatives"" is also found in the context[cite: 146].",1,"The context directly and explicitly answers the question. It states that the reason fewer women saw the ad was that the model identified correlations in the historical data related to ""Hobbys und die UniversitÃ¤ten"" (hobbies and universities), where male enrollment had been historically higher at certain recruited-from universities."
"Regarding the display of STEM career ads, what was the empirical result of the ad's delivery in the 'Algorithmic Bias?' paper?",Fewer women saw the ad than men.,"According to the provided text, the empirical result of the ad’s delivery in the “Algorithmic Bias?” paper (Lambrecht & Tucker, 2019) was that apparent gender-based discrimination was evident in the display of STEM career ads. Specifically, the study found that women were less likely to see ads for STEM careers compared to men.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)
from an organizational behavior perspective. While recent studies
have primarily discussed discrimination theory in the traditional
economy’s hiring market, future theoretical research should
consider how advanced technology affects equity in hiring within
the digital economy.
Conclusion
The study concludes that the fo urth industrial revolution
introduced technological innovations signiﬁcantly affecting the
recruitment industry. It extends the analysis of statistical dis-
crimination theory in the digital age and adopts a literature
review approach to explore four themes related to AI-based
recruitment. The study argues that algorithmic bias remains an
issue while AI recruitment tools offer beneﬁts such as improved
recruitment quality, cost reduction, and increased ef ﬁciency.
Recruitment algorithms ’bias is evident in gender, race, color,
and personality. The primary source of algorithmic bias lies in
partial historical data. The personal preferences of algorithm
engineers also contribute to algorithmic bias. Technical mea-",9.845435143,1,"The generated answer (""women were less likely to see ads... compared to men"") and the ground truth (""Fewer women saw the ad than men"") are semantically identical. They both state the same empirical result.",0.5,"The generation is partially faithful. [cite_start]It correctly identifies from the paper's title [cite: 174, 176] [cite_start]and the general text [cite: 191] that the study deals with gender-based discrimination. However, it makes a specific claim that ""women were less likely to see ads for STEM careers compared to men,"" which is not stated in the provided context.",0,"The provided context is a list of references and a general conclusion about algorithmic bias. While it cites the specific paper mentioned in the question, it does not contain the empirical result from that study regarding the ad's delivery."
"In the study on gender-based discrimination in STEM career ads, what was the explicit intention for the ad's delivery?",The ad was explicitly intended to be gender-neutral in its delivery.,"According to the provided text, the study on gender-based discrimination in STEM career ads aimed to **empirically study apparent gender-based discrimination in the display of STEM career ads.** 

Specifically, the study investigated whether algorithms were displaying gender-based biases in the advertisements.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",8.138091087,1,"The generated answer and the ground truth are semantically consistent. Stating that the study aimed to ""empirically study apparent gender-based discrimination"" implies that the experimental condition (the ad delivery) was intended to be neutral to see if bias would emerge, which is the same core meaning as the ground truth.",1,"The generated answer is faithful. [cite_start]The core claim, that the study's aim was to ""empirically study apparent gender-based discrimination in the display of STEM career ads,"" is a direct quote from the title of the paper cited multiple times in the context[cite: 202, 204].",1,"The context strongly supports the answer. The title of the repeatedly cited paper is ""Algorithmic bias? an empirical study of apparent gender-based discrimination..."". The very nature of conducting an ""empirical study"" on ""apparent"" discrimination implies that the ad's delivery was intended to be neutral to observe whether bias would emerge algorithmically. The surrounding text about mitigating bias further supports this."
The findings of the 'Feeback Loops in Machine Learing' paper underscore the complex nature of algorithmic discrimination as what kind of phenomenon?,A sociotechnological phenomenon.,"According to the text, the findings underscore algorithmic discrimination as a **dynamic, sociotechnological phenomenon** that can diminish or emerge due to the interaction between ML systems and human users in environments where continuous model updating is essential.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",7.089416981,1,The generated answer is semantically identical to the ground truth. They both identify the phenomenon as 'sociotechnological'.,1,The generated answer is fully faithful. [cite_start]The claim that the findings underscore the 'sociotechnological nature of algorithmic discrimination' is a direct quote from the provided context[cite: 231].,1,"The context directly and explicitly answers the question. It contains the sentence: ""Theoretically, our findings underscore the sociotechnological nature of algorithmic discrimination."""
What type of setting was used in the simulations of the 'Feedback Loops in Machine Learning' paper to examine the ML model's evolution?,"A setting that mirrors sequential interactions, such as loan approval decisions.","The simulations in the “Feedback Loops in Machine Learning” paper were based on a setting that mirrored sequential interactions, such as loan approval decisions. Specifically, they utilized data from two incentivized studies:

*   **Study A:** Involved first-semester economics students acting as borrowers.
*   **Study B:** Involved participants acting as investors who could access an ML model predicting borrowers’ repayment likelihood.

These studies created a dynamic environment where the ML model’s predictions influenced the creation of new training data, allowing researchers to examine the model’s evolution and the potential for algorithmic discrimination.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. Feedback Loops in Machine Learning 
 
805 
generate predictions (Parisi et al., 2019). However, in 
dynamic, nonstationary environments, the underlying 
data-generating process that ML models aim to 
approximate can change over time. This phenomenon, 
referred to as drift (Lu et al., 2018), can lead to  
progressively poorer predictions. The decrease in 
prediction performance can originate from changes in 
the underlying relationship between the input features 
and the target variable —concept drift (Widmer & 
Kubat, 1996)—or changes in the distributions of t he 
input features or the target variable—data drift (Mallick 
et al., 2022). As a consequence, to maintain or even 
improve prediction accuracy over time, it is imperative 
to continuously update ML models on recent data that 
reflect new patterns in the data -generating process 
(Gama et al., 2014). 
The efficacy of continuous updating inherently links to 
the quality and structure of the collected data. Ideally, 
updating processes should empower models to learn 
from their past mistakes —that is, identify cases for 
which they generate inaccurate predictions. However, 
ML models that support or automate human decisions 
typically do more than merely provide neutral Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were",12.45401454,1,The generated answer and the ground truth are semantically identical. They both describe the setting as one that mirrors sequential interactions like loan approval decisions.,1,"The generated answer is fully faithful. [cite_start]The context explicitly states that the simulations used 'a setting that mirrors sequential interactions, such as loan approval decisions' [cite: 242] [cite_start]and were built on data from two studies involving an 'investment game'[cite: 254, 256].",1,"The context directly answers the question by stating, ""...we examine the dynamic evolution of an ML modelâ€™s fairness and economic consequences in a setting that mirrors sequential interactions, such as loan approval decisions."""
What is the key finding regarding the continuous updating of ML models?,Continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time.,"According to the text, a key finding is that continuous updating of ML models can help mitigate algorithmic discrimination and enhance economic efficiency *over time*. However, the text also highlights a crucial caveat: human decision-makers who override ML predictions can actually impede self-correction and even induce initially unbiased models to become discriminatory over time.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers. generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical 
exploration of the long -term dynamic effects of 
continuously updating ML models  is lacking , 
particularly in terms of the role of human decision-
making based on these predictions. We aim to fill this 
research gap. Regarding contexts where a continuously 
updated ML system both supports human decision -
making and shapes the data available for updating, we 
pose the following questions: 
RQ1: How does continued updating affect the fairness 
and economic efficiency of an ML model that 
initially discriminates against a certain social 
group due to a label imbalance in the training 
data? 
RQ2: How does persistent discrimination by human 
decision makers against a certain social group 
affect the impact of continuous updating on 
model fairness and economic efficiency?
 
 
Figure 1. Process Where Machine Learning Predictions Endogenously Influence the Availability of Data 
for Future Retraining of the Model",7.502393484,1,The generated answer and the ground truth are semantically identical. They both state that continuous updating can mitigate discrimination and enhance efficiency over time.,1,The generated answer is faithful. [cite_start]It accurately states the main finding that 'continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time' [cite: 265] [cite_start]and also includes the crucial caveat mentioned in the text about the negative impact of human overrides[cite: 266].,1,"The context directly provides the key finding in the sentence: ""Our findings indicate that continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time."""
How do ML models endogenously shape their future training data?,"By influencing human decisions, which then generate new data points (or a lack thereof) for future model training.","According to the text, ML models endogenously shape their future training data by:

*   **Generating new data based on their predictions:** The models’ predictions themselves create new data points that are then used to update the model.
*   **Reinforcing discriminatory patterns:** When human decision-makers override ML predictions, this can actually *increase* the likelihood of the model becoming discriminatory over time.","endogenous composition of future training data, a 
problem referred to as selective labeling (Lakkaraju et 
al., 2017; Little & Rubin, 2019), can maintain or even 
exacerbate inaccurate or discriminatory predictions. 
In this paper, we explore the dynamic interplay 
between the continuous updating of ML models and 
algorithmic discrimination when ML predictions 
endogenously shape the composition of additional data 
available for model updating. In particular, our work 
focuses on the role of ongoing discrimination by 
human decision  makers who have the final say in an 
ML-supported decision-making task. A steady stream 
of previous work has examined how algorithmic 
discrimination can reproduce and perpetuate 
discriminatory patterns ingrained in societal structures 
(Lambrecht & Tucker, 2019; Morse et al., 202 2; 
Teodurescu et al., 2021). Some studies have developed 
theoretical arguments concerning the emergence of 
feedback loops when ML models continually learn 
from the discriminatory outcomes they help ed to 
generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. endogenously help generate, can enhance 
performance, alleviate algorithmic discrimination, and 
boost the economic efficiency of ML models initially 
grappling with label imbalances in the original training 
dataset. The ability for self -correction inversely 
depends on the degree of initial label imbalance: the 
more biased the ML model ’s predictions are at the 
outset, the le ss capacity it has for recovery through 
continuous updating over time. Crucially, our research 
indicates that persistent discrimination by humans —
who make the final decisions in ML-supported tasks—
significantly impedes the self -correction abilities 
associated with the continuous updating of ML 
models. Our findings even suggest that ongoing human 
discrimination can lead ML models, originally trained 
on balanced data and demonstrating no algorithmic 
discrimination, to exhibit increasingly discriminatory 
behavior over time. 
5.2 Theoretical Contributions and 
Implications  
Our study responds to recent calls for empirical 
investigations into the sociotechnological nature of 
algorithmic discrimination in environments where ML 
models continue to learn based on data they help to 
generate (e.g., Kordzadeh & Ghasemaghaei, 2022;",8.687135935,1,"The generated answer and the ground truth are semantically equivalent. The generation's explanation of how models' predictions create new data points, which are then used for updates, captures the core meaning of the ground truth.",1,"The generated answer is faithful. [cite_start]It correctly synthesizes information from the text, explaining that models shape future data when 'outcomes of algorithmic decisions provide new endogenous training data' [cite: 227] [cite_start]and that this is influenced by 'human decision makers... who have the final say'[cite: 280].",1,"The context directly answers the question. It explains that feedback loops ""arise when the outcomes of algorithmic decisions provide new endogenous training data for subsequent model updates"" and that this process is influenced by ""human decision makers who have the final say in an ML-supported decision-making task."" This clearly describes how the model's output influences the data it will be trained on next."
What effect can human decision-makers have on the self-correction of discriminatory models when they are kept in the loop?,Human decision-makers in the loop may impede the self-correction of discriminatory models and can even cause initially unbiased models to become discriminatory over time.,"According to the text, human decision-makers can significantly hinder the self-correction of discriminatory models when they are kept in the loop. Here’s a breakdown:

*   **Learning Biased Behaviors:** Humans can unintentionally learn biased behaviors from the ML model’s predictions and then perpetuate those biases in their final decisions.
*   **Interference with Data:** Human interference can reduce the chance of the model being supplemented with observations that would improve its predictions for disadvantaged groups.
*   **Reinforcing Discrimination:** Their decisions can directly influence the behavior of continuously updated ML models, reinforcing discriminatory practices.

In essence, the text suggests that human biases within the decision loop can counteract the self-correcting mechanisms of ML models.","discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by 
organizational decision makers. These measures could 
serve as essential complementary elements to ensure that 
continuously updated ML models demonstrate desirable 
behaviors when organizations implement regulatory 
requirements that involve humans in the decision loop. 
5.4 Limitations and Future Research  
Like any study, this study also has limitations—which, 
we believe , provide valuable directions for future 
research to enhance our understanding of how 
algorithmic discrimination evolves in environments 
where ML models undergo continuous updates. One the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” Journal of the Association for Information Systems 
 
824 
Indeed, self-correcting mechanisms may require time 
to take effect, during which disadvantaged social 
groups may continue to endure discriminatory 
practices. Rather, we perceive our findings as a 
testament to the substantial learning capabilities of 
modern ML models undergoing continuous updating. 
We argue that these capabilities can complement other 
interventions aimed at addressing algorithmic 
discrimination, such as the proper preprocessing of 
training data. This approach is particularly effective 
when organizations take measures to ensure that their 
employees do not engage in (subconscious) 
discriminatory practices during ML -supported 
decision-making processes. However, overreliance on 
the self -correcting aspects of continuous updating 
processes could cause significant damage to both an 
organization’s economic performance and reputation if 
ML systems are deployed hastily without rigorous ly 
testing for and addressing algorithmic discrimination.",13.30015707,1,The generated answer is semantically identical to the ground truth. Both state that human decision-makers can impede the self-correction of models and even introduce new biases.,1,"The generated answer is fully faithful. [cite_start]It correctly states that human decision-makers 'may impede the self-correction of discriminatory models and even induce initially unbiased models to become discriminatory with time,' a claim directly supported by the context[cite: 266, 291]. [cite_start]The bullet points about learning biased behaviors [cite: 301][cite_start], interfering with data [cite: 302][cite_start], and reinforcing discrimination [cite: 305] are all directly supported by the provided text.",1,"The context provides a direct and comprehensive answer. It explicitly states that human decision-makers ""may impede the self-correction of discriminatory models"" and ""can even cause initially unbiased models to become discriminatory over time."" It further explains that ""unbiased models learn biased behaviors from humans over time."""
What is a key challenge with opaque algorithmic designs in recruitment?,"They hinder the detection and correction of biases, making it difficult for HR teams to ensure equitable hiring.","According to the text, a key challenge with opaque algorithmic designs in recruitment is that they can amplify future hiring inequalities. The text states: ""Predicting future hiring outcomes by observing historical data can amplify future hiring inequalities."" This suggests that if the data used to train the algorithm reflects existing biases, the algorithm will perpetuate and potentially worsen those biases in its hiring decisions.","technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data
selection. Data collection tends to prefer accessible,“main-
stream” organizations unequally dispersed by race and
gender. Inadequate data will screen out groups that have
been historically underrepresented in the recruitment
process. Predicting future hiring outcomes by observing
historical data can amplify future hiring inequalities. Yarger
et al. (2019), Miasato and Silva (2019), and Njoto (2020)
propose that discrimination is due to the designer-induced
selection of data features.
3. The third theme is which algorithmic recruitment dis-
crimination exists. According to Correll et al. (2007), Kay
et al. (2015), O’neil (2016), Raso et al. (2018), Miasato and
Silva (2019), Langenkamp et al. (2019), Faragher (2019),
Ong (2019), Fernández and Fernández (2019), Beneduce
(2020), Jackson (2021), Yarger et al. (2023), and Avery et al.
(2023), when partial human data is provided to a machine,
so the algorithm is biased, it will eventually lead to the risk",8.673582315,1,"The generated answer is semantically identical to the ground truth. Both state that opaque designs hinder the detection and correction of biases, making it difficult to ensure equitable hiring.",1,The generated answer is faithful. [cite_start]The claim that predicting future outcomes from historical data can 'amplify future hiring inequalities' is a direct quote from the provided context[cite: 332].,1,"The context directly addresses the question. It states that ""Predicting future hiring outcomes by observing historical data can amplify future hiring inequalities."" This is a key challenge of opaque designs, as the lack of transparency hinders the ability to detect and correct the biases learned from such historical data, which in turn prevents equitable hiring."
"According to some studies cited, what are two benefits of AI-supported recruitment?",It can reduce costs and save time.,"According to the provided text, two benefits of AI-supported recruitment are:

1.  **Reduced costs**
2.  **Saves time** 

Additionally, it reduces transactional workload.","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly",6.67088604,1,The generated answer and the ground truth are semantically identical. Both identify cost and time savings as key benefits of AI-supported recruitment.,1,"The generated answer is faithful. [cite_start]The context explicitly lists studies that state AI-supported recruitment can 'reduce costs' [cite: 342] [cite_start]and 'saves time' [cite: 344][cite_start], and also 'reducing transactional workload'[cite: 344].",1,"The context explicitly lists multiple benefits of AI-supported recruitment. It clearly states that studies indicate it can ""reduce costs"" and ""saves time,"" directly answering the user's question."
What are two recommended categories of measures to mitigate algorithmic bias in recruitment?,Technical measures (like unbiased dataset frameworks and improved transparency) and management measures (like internal ethical governance and external oversight).,"According to the text, the two recommended categories of measures to mitigate algorithmic bias in recruitment are:

1.  **Technical measures:** This includes things like “unbiased dataset frameworks and improved algorithmic transparency.”
2.  **Management measures:** This encompasses “internal corporate ethical governance and external oversight.”","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender Theme IV. How decreasing algorithmic recruitment
discrimination
Changes should be made at the technical and regulatory levels to
ensure that AI algorithms do not replicate existing biases or
introduce new ones based on the provided data (Raub,2018).
Building fair algorithms from a technical perspective
Constructing a more unbiased dataset. Unfair datasets are the root
cause of bias. Therefore, a direct approach to addressing algo-
rithmic bias is reconﬁguring unbalanced datasets. Using multiple
data points can yield more accurate results while carefully elim-
inating data points that re ﬂect past biases. However, this
approach incurs signiﬁcant costs (Bornstein,2018).
Another method is to correct data imbalances by using more
equitable data sources to ensure fair decision-making (36KE,
2020). Understanding the underlying structure of training data
and adjusting the signi ﬁcance of speci ﬁc data points during
training based on known latent distributions makes it possible to
uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold",7.32905674,1,The generated answer and the ground truth are semantically identical. Both identify technical measures (like unbiased datasets and transparency) and management measures (like ethical governance) as the two recommended categories.,1,"The generated answer is faithful. [cite_start]The context explicitly recommends implementing 'technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight'[cite: 364].",1,"The context provides a direct and explicit answer to the question. It states that to mitigate algorithmic bias, it is ""recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight."""
"Besides gender and race, what other characteristic is mentioned as a basis for discriminatory hiring practices by AI?",Personality traits.,"According to the text, personality traits are also mentioned as a characteristic that can lead to discriminatory hiring practices by AI systems.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- Thirdly, concerns are raised regarding potential hiring
discrimination perpetrated by machines. This can arise from AI
tools, such as partial source data, or users unfamiliar with user
interfaces and operations.
Fourthly, intrinsic factors like personality and IQ, as well as
extrinsic factors like gender and nationality, have been observed
to inﬂuence the accurate identi ﬁcation and judgment of AI
systems concerning hiring discrimination.
Fifthly, respondents offer recommendations for combating
discrimination by machines, including technical and non-
technical approaches.
Recommendations for future studies
This study conducted a literature review to analyze algorithmic
recruitment discrimination’s causes, types, and solutions. Future
research on algorithmic recruitment discrimination could explore
quantitative analysis or experimental methods across different
countries and cultures. Additionally, future studies could examine
the mechanics of algorithmic recruitment and the technical rules
that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)",5.432776451,1,The generated answer is semantically identical to the ground truth. Both identify 'personality traits' as another basis for discrimination.,1,"The generated answer is faithful. [cite_start]The context explicitly states that algorithmic bias can result in 'discriminatory hiring practices based on gender, race, color, and personality traits'[cite: 382].",1,"The context directly answers the question by explicitly listing ""personality traits"" as a basis for discriminatory hiring practices alongside gender, race, and color."
"According to the study of the paper 'Ethics and discrimination in artificial intelligence-enabled recruitment practices', what are two primary sources from which algorithmic bias stems?",Algorithmic bias stems from limited raw data sets and biased algorithm designers.,"According to the study, the two primary sources from which algorithmic bias stems are:

1.  **Limited raw data sets** – particularly those lacking diverse representation.
2.  **Biased algorithm designers** – who may prioritize specific features or variables based on their own biases.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not",8.218610764,1,The generated answer and the ground truth are semantically identical. Both identify limited/biased data and biased designers as the two primary sources of algorithmic bias.,1,The generated answer is faithful. [cite_start]The context explicitly states that 'algorithmic bias stems from limited raw data sets and biased algorithm designers'[cite: 407]. The two bullet points are direct reflections of this statement.,1,"The context directly and explicitly answers the question in the abstract, stating: ""The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers."""
What does the CRISP-DM User Guide provide that the Reference Model does not?,"The User Guide gives more detailed tips and hints for each phase and task on how to do a data mining project, whereas the Reference Model describes what to do.","The Reference Model provides a general process, but the CRISP-DM User Guide “proved to be much more advantageous than we originally anticipated for communication both within and outside the project.”","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",5.308125734,0,"The generated answer is incorrect. It claims the User Guide proved advantageous for communication, which contradicts the ground truth's statement that the User Guide provides detailed 'how-to' information, while the Reference Model describes 'what to do'.",0,"The generated answer is not faithful because the context provided is the generated answer itself, not the original source material. It's impossible to verify the claims against the actual context.",0,"The provided context is an excerpt from the generated answer itself, not the source document. It does not contain the original information distinguishing the User Guide from the Reference Model. Therefore, it is impossible to evaluate the relevance of the source context for the question."
Which task is mentioned as often being the lengthiest in the Data Preparation phase?,The 'Clean data' task.,"The text doesn't specify which task is the longest in the Data Preparation phase. It only mentions that there were tight deadlines created due to a misunderstanding of the generic CRISP-DM model, leading to sub-optimal solutions.","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and G eneric
Tasks
Specialized
Tasks
C ontextC ontext
R eference M odel U ser G uide
• check lists
• questionaires
• tools and techniques
• sequences of steps
• decision points
• pitfalls
Phases
Process
Instances
Figure 1: Four Level B reakdow n of the C R ISP-D M  M ethodology for D ata M ining
The C R ISP-D M  m ethodology distinguishes betw een the Reference M odel and the U ser G uide.
W hereas the R eference M odel presents a quick overview  of phases, tasks, and their outputs, and
describes w hat to do in a data m ining project, the U ser G uide gives m ore detailed tips and hints
for each phase and each task w ithin a phase and depicts how  to do a data m ining project.
3.2 T he G eneric C R ISP-D M  R eference M odel
The C R ISP-D M  reference m odel for data m ining provides an overview  of the life cycle of a data
m ining project. It contains the phases of a project, their respective tasks, and their outputs.
The life cycle of a data m ining project is broken dow n in six phases w hich are show n in Figure 2.
The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase creative process w ith m any parallel activities. In our case, w e em ployed external service
providers for som e tasks. Therefore, there m ust be firm  deadlines for a task to be com pleted to
ensure tim ely com pletion and proper usage of resources. B ut w hen is a task com plete in a data
m ining project? This is a question that cannot be answ ered by a generic process m odel. This m ust
be addressed by the specialized process m odel. B ut even there, it is not trivial to com e up w ith
useful criteria. So far, w e have not yet a com plete, satisfactory and operational set of criteria in
our application.
In our first case studies, w e encountered an unexpected difficulty w ith the generic process m odel.
A lthough in the C R ISP-D M  docum ents it is stated at several places, that the phases and tasks are
not supposed to be strictly sequential, the clear and obvious presentation of the process m odel
inevitably created this im pression in our decision m akers. D espite our argum ents, w e found
ourselves forced to very tight deadlines, w hich in the end let to sub-optim al solutions. In our
case, this w as not a serious problem  because w e still learned enough about the problem  and about",6.446878433,0,"The generated answer is incorrect. It claims the text doesn't specify the longest task, while the ground truth identifies the 'Clean data' task as the lengthiest.",1,The generated answer is faithful. It accurately states that the text does not specify the longest task. The context discusses data preparation and tight deadlines but never singles out one task as being the lengthiest.,0,"The context is not relevant to the question. It describes the Data Preparation phase and discusses issues with project deadlines but does not specify which task, such as 'Clean data', is the lengthiest."
What is the stated aim of the CRISP-DM process model?,"To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the text, the stated aim of the CRISP-DM process model is to be reliably and efficiently repeated by different people and adapted to different situations.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",4.734779119,0,"The generated answer states the aim is to be 'reliably and efficiently repeated', which is different from the ground truth's more comprehensive goal of making projects 'less costly, more reliable, more repeatable, more manageable, and faster'.",1,"The generated answer is faithful. [cite_start]The context states that the aim of the project was 'to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations'[cite: 452], which directly supports the generated answer.",1,"The context directly answers the question. It explicitly states that the aim of the project was ""to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations."""
What does CRISP-DM stand for?,Cross Industry Standard Process for Data Mining.,CRISP-DM stands for Cross Industry Standard Process for Data Mining.,"15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",3.12629509,1,The generated answer and the ground truth are semantically identical. They both correctly state that CRISP-DM stands for Cross Industry Standard Process for Data Mining.,1,The generated answer is fully faithful. [cite_start]The context contains a slide that explicitly displays the text 'Cross Industry Standard Process for Data Mining' directly underneath the 'CRISP-DM' acronym[cite: 461].,1,"The context provides a slide that explicitly contains the phrase ""Cross Industry Standard Process for Data Mining"" directly associated with the acronym ""CRISP-DM."""
What are the six phases of the CRISP-DM process model?,"The six phases are Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.","The six phases of the CRISP-DM process model are:

1.  Business Understanding
2.  Data Understanding
3.  Data Preparation
4.  Modeling
5.  Evaluation
6.  Deployment","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining D ata
U nderstanding
D ata
Preparation
M odelling
D ataD ataD ata
B usiness
U nderstanding
D eploym ent
Evaluation
Figure 2: Phases of the C urrent C R ISP-D M  Process M odel for D ata M ining
In the follow ing, w e outline each phase briefly:
• Business U nderstanding
This initial phase focuses on understanding the project objectives and requirem ents from  a
business perspective, and then converting this know ledge into a data m ining problem
definition, and a prelim inary project plan designed to achieve the objectives.
• D ata U nderstanding
The data understanding phase starts w ith an initial data collection and proceeds w ith activities
in order to get fam iliar w ith the data, to identify data quality problem s, to discover first
insights into the data, or to detect interesting subsets to form  hypotheses for hidden
inform ation.
There is  a close link betw een B usiness U nderstanding and D ata U nderstanding. The
form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be 15. Mai 2025
Overview CRISP-DM
9",5.521406412,1,"The generated answer and the ground truth are semantically identical, both correctly listing the six phases of the CRISP-DM model.",1,"The generated answer is faithful. [cite_start]The context contains a diagram that explicitly lists the six phases of the CRISP-DM model: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment[cite: 466].",1,"The context directly answers the question with a diagram that lists the six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment."
What three characteristics define a General Purpose Technology (GPT)?,"They are pervasive, improve over time, and lead to complementary innovation.","Based on the text, here are three characteristics that define a General Purpose Technology (GPT):

1.  **Applicable in Various Sectors:** GPTs can be used “in a specific domain, in a specific area, but also in different sectors.”
2.  **General Purpose:** The text explicitly uses the term “General Purpose” as a defining characteristic, and refers to it as “GPT.”
3.  **Driven by User Interaction:** The text highlights that “the willingness of the user to interact with the systems” is central to how a technology can ultimately influence social life, economic processes, and productivity.","in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben. Studierende machen sich mit der Funktionsweise von generativen KI-Modellen wie GPT vertraut. Sie lernen, diese durch gezieltes Prompting für Managemententscheidungen einzusetzen und kritisch zu bewerten, insbesondere unter wirtschaftlichen und ethischen Gesichtspunkten. gemacht hat,
das User Interface war
die reine Technologie dieser 
großen Sprachmodelle.
Die gibt es theoretisch schon 
seit 2017.
Die wurden natürlich immer 
besser.
Wir hatten am Ende die Version 
drei Punkt irgendwas,
was dann integriert wurde.
Aber es wurde integriert.
Dieses fundamentale KI Modell in
eine ausreichend kompatible und
nutzerfreundliche Umgebung,
nämlich Chat GPT, was
noch mal das Grundmodell GPT
plus weitere Systeme sind, die 
hier zusammenspielen.
Das heißt Forgeat GPT.
Gab es diese großen 
Sprachmodelle schon?
Die haben sich wie gesagt immer 
weiter verbessert.
Grundsätzlich haben
wir aber alle erst damit 
angefangen und
haben eine ganze Industrie 
eigentlich damit angefangen,
es kommerziell zu benutzen.
Ab dem Moment, wo dieses
User Interface richtig 
hinbekommen wurde
und die User des Systems 
hochgegangen ist.
Das ist eine der Kern messages
die Bereitschaft des Nutzers,
mit den Systemen zu 
interagieren.
Generell, wo wir
häufig von Adoption reden 
Technologie use.
Wenn wir ganz, ganz breit sind.
Das ist zentral dafür,
wie eine Technologie tatsächlich
letztlich Einfluss haben kann 
auf unser Sozialleben,
auf unsere ökonomischen 
Prozesse,
auf Unternehmen
und letztlich natürlich auf 
Produktivität.",11.0857029,1,"The generated answer is semantically consistent with the ground truth. While phrased differently, 'Applicable in Various Sectors' corresponds to 'pervasive', and 'Driven by User Interaction' is a key aspect of how GPTs lead to 'complementary innovation'.",1,"The generated answer is faithful. The three characteristics are all supported by the context. 1. [cite_start]Applicable in various sectors ('in verschiedenen Sektoren' [cite: 471]). 2. [cite_start]General Purpose (The text explicitly uses the term 'General Purpose' and its abbreviation GPT [cite: 473, 474]). 3. [cite_start]Driven by user interaction (The text highlights 'die Bereitschaft des Nutzers, mit den Systemen zu interagieren' as central to a technology's influence [cite: 488, 490]).",1,"The context directly provides the three defining characteristics. It mentions a GPT can be applied ""in verschiedenen Sektoren"" (pervasive), that they get ""Ã¼ber die Zeit immer besser genutzt"" (improve over time), and that they lead ""zu weiteren komplementÃ¤ren Innovationen"" (lead to complementary innovation)."
"In the paper 'THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES', by how much higher was the adjusted measure of Total Factor Productivity (TFP) compared to official measures at the end of 2017?",The adjusted TFP was 15.9% higher than official measures at the end of 2017.,"I understand you're asking about a specific figure from a paper titled ""THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES."" However, the provided context doesn't contain the answer to your question. It describes a J-Curve productivity pattern, Receiver Operating Characteristic, and a discussion about machine learning. It does not include any data or figures related to TFP measurements in 2017.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 3. Juli 2025
What then?
Consider technical performance metric that is independent of threshold
• Solution: Receiver Operating Characteristic
• Simulate thresholds from 0 to 1 and calculate TPR and FPR for every threshold value
• Plot every (TPR, FPR) tuple with TPR being on the y-axis and FPR on the x-axis
18",11.28702903,0,"The generated answer claims the information is not in the text, which directly contradicts the ground truth that provides a specific numerical answer (15.9% higher).",1,The generated answer is faithful. It correctly states that the provided context does not contain the specific data point requested. The context describes the J-Curve and other concepts but includes no figures related to TFP measurements in 2017.,0,"The context is irrelevant. It describes the concept of the productivity J-Curve in general terms but contains no specific data, percentages, or any mention of Total Factor Productivity (TFP) measurements for the year 2017."
What did Robert Solow's productivity paradox state?,That one 'can see in the computer age everywhere but in the productivity statistics',The provided text doesn't contain information about Robert Solow's productivity paradox. It discusses a phenomenon where productivity changes can initially be negative due to investments needed to integrate new technologies into organizations.,"der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, sample field experiment. Review of Economics 
and Statistics, 96(1), 119-134. 
Falk, A. & Kosfeld, M. (2006). The hidden costs of 
control. American Economic Review , 96(5), 
1611-1630. 
Favaretto, M., De Clercq, E., & Elger, B. S. (2019). 
Big Data and discrimination: perils, promises 
and solutions. A systematic review . Journal of 
Big Data, 6(1), 1-27.  
Fazelpour, S., & Danks, D. (2021). Algorithmic bias: 
Senses, sources, solutions. Philosophy 
Compass, 16(8), Article e12760. Produktivitätsveränderungen 
negativ sein kann.
Das ist so dieser untere 
Halbkreis von dem J.
Und das kann verschiedene Gründe 
haben.
Unter anderem eben, dass diese
Technologien noch nicht 
ausreichend in den
Organisationen mit
komplementären Investitionen 
integriert worden sind.
Und dann sind wir noch 
technischer
geworden aus einer 
mikroökonomischen Perspektive.
Wir haben kurz über die Trust 
Game Struktur gesprochen,
wo es darum geht.
Ich mache Entscheidungen, 
ökonomisch
relevante Entscheidungen unter 
Unsicherheit.
Und in diesen Szenarien kann 
Machine Learning uns helfen.
Wenn Sie sich wieder vorstellen,
Sie sind hier der Investor.
Sie haben 10 Millionen € zur 
Verfügung,
Sie entweder in ein Startup 
investieren
können oder eben auch nicht.
Wenn Sie nicht investieren, 
haben Sie
10 Millionen Cash und das 
Startup hat nichts.
Wenn Sie es investieren,
dann kann das Startup entweder 
sich anstrengen,
viel arbeiten, kann diese Summe,
die Sie investieren,
verdreifachen und gibt Ihnen die 
Hälfte zurück.
Oder aber das Startup strengt
sich ein bisschen an und läuft 
weg.
Dann haben wir gesagt Wenn wir 
uns
das als 
Nutzenmaximierungsstrategie
aufschreiben aus dieser 
mikroökonomischen,
schrägstrich spieltheoretischen",6.00255084,0,"The generated answer is incorrect. It claims the text doesn't contain information about the paradox, while the ground truth provides Solow's famous quote which defines the paradox.",1,"The generated answer is faithful. It correctly states that the provided text does not mention Robert Solow's productivity paradox. The context describes the J-curve phenomenon where productivity initially dips due to necessary investments, which is related, but it never mentions Solow or the paradox by name.",0.8,"The context is highly relevant as it describes the economic phenomenon at the heart of Solow's paradoxâ€”that initial productivity can decrease due to the large, complementary investments required by new technologies. However, it does not explicitly mention Robert Solow or his famous quote, so it doesn't directly answer the question as stated."
What are some examples of complementary investments required by General Purpose Technologies (GPTs) like AI?,"Business process redesign, co-invention of new products and business models, and investments in human capital.","According to the text, some examples of complementary investments required by General Purpose Technologies (GPTs) like AI are:

*   **Sensors and connected devices:** These are new inputs that are spurred by the rise of AI.
*   **Synthetic data:** Another new input.
*   **“Orgware”:** This refers to things like data pipelines and AI-first processes – essentially, the tools and processes needed to support and utilize the AI.
*   **Adapting organizational structures:** The text highlights the need to adjust organizations, just as factories were rebuilt with electricity after the introduction of the steam engine.","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind Studierende machen sich mit der Funktionsweise von generativen KI-Modellen wie GPT vertraut. Sie lernen, diese durch gezieltes Prompting für Managemententscheidungen einzusetzen und kritisch zu bewerten, insbesondere unter wirtschaftlichen und ethischen Gesichtspunkten.",11.50169277,0.8,"The generated answer is partially correct but less comprehensive than the ground truth. It lists some technical complements like sensors and 'orgware' but omits the broader and more crucial examples from the ground truth, such as 'business process redesign' and 'co-invention of new products and business models'.",1,"The generated answer is faithful. The context provides direct examples of complementary investments. [cite_start]A slide lists 'New inputs: sensors, connected devices, synthetic data' and 'New â€œorgwareâ€: data pipelines, ML-ops, AI-first processes'[cite: 524]. [cite_start]The text also mentions the need to adapt organizational structures, similar to how factories were rebuilt after the introduction of electricity[cite: 530].",1,"The context directly answers the question by providing specific examples of complementary investments. A slide lists ""New inputs: sensors, connected devices, synthetic data"" and ""New â€œorgwareâ€: data pipelines, ML-ops, AI-first processes"" as complements catalyzed by AI."
"According to the text, what is the 'Productivity J-Curve' phenomenon?","It is a phenomenon where, as firms adopt a new General Purpose Technology (GPT), total factor productivity (TFP) growth is initially underestimated because resources are spent on unmeasured intangible capital. Later, measured productivity growth overestimates true productivity as the benefits from those intangibles are realized.","According to the text, the “Productivity J-Curve” phenomenon is a two-phase productivity pattern that emerges when investing in General Purpose Technologies (like GPT). It’s characterized by:

*   **Initial Dip:** Productivity initially falls or stalls due to heavy up-front investment in data, skills, process redesign, and complementary tech. This is like being on the “lower half-circle” of the J.
*   **Long-Run Rebound:** After this initial dip, productivity accelerates once the complementary investments start paying off. This happens due to learning effects, scale economies, and network externalities.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, mussten und überall Kolben die 
Energie
eigentlich weiterbringen 
mussten.
Das heißt,
die Dampfmaschine hat nicht nur 
eine Effizienz,
die Elektrizität, als wir damit 
angefangen haben,
hat nicht nur eine effizientere
Energieproduktion geschaffen,
sondern sogar wenn wir dem
Faktor Produktion Beispiel 
bleiben,
hat es ganze Fabriken im Design 
verändern.
Das hat dazu geführt, dass wir 
länger
die Straßen beleuchten konnten.
Das wiederum hat Effekte darauf 
gehabt,
wie viel wir bauen konnten,
wie lange wir unterwegs sein 
können,
was Mobilität erhöht hat usw usw
alles ausgehend auf dieser 
Kerninnovation,
die wir haben oder hatten damals 
nämlich Elektrizität.
Ja, jeder nimmt Elektrizität 
jetzt verkantet
einfach hier an den Stecker 
gehen
und Elektrizität Energie ziehen.
Und das ist natürlich ein 
massiver Treiber von Wachstum,
von Produktivität.
War aber nicht immer so.
Am Anfang hat es super lange 
gedauert,
bis diese Elektrizität irgendwas
gebracht hat und die
Frage werde ich ihn gleich 
stellen,
Wie lange sie denn glauben,
dass es gedauert hat, wenn wir 
auf der nächsten
Slide zu den JWs sind
auch wieder mikroökonomisch 
motiviert?
Ja, alles. Aber lassen Sie uns
ganz kurz noch mal darüber 
nachdenken.",11.95941639,1,"The generated answer is semantically identical to the ground truth. Both describe the J-Curve as an initial dip or underestimation of productivity growth, followed by a rebound or overestimation as the benefits of intangible investments are realized.",1,The generated answer is faithful. [cite_start]The context explicitly defines the J-Curve as a 'two-phase productivity pattern that emerges when a GePuT diffuses'[cite: 539]. [cite_start]The bullet points in the generated answer accurately reflect the 'Short-run dip' [cite: 539] [cite_start]and 'Long-run rebound' [cite: 539] described in the source.,1,"The context directly and explicitly defines the 'Productivity J-Curve'. A slide is titled ""Expect a J-Curve"" and describes it as a ""two-phase productivity pattern"" with a ""Short-run dip"" followed by a ""Long-run rebound,"" which is a perfect explanation of the phenomenon."
"What societal benefit, in terms of CO2 emissions, would a 2.6% return decrease translate to based on U.S. figures from 2022?","It would result in a reduction of 624,000 metric tons of CO2 emissions.","Based on the provided text, a 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions.","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation 24. April 2025
Summary and impact: what ML does
https://www.readycloud.com/info/14-facts-about-ecommerce-product-returns
up to 4.6% decrease
•Our green nudges can be easily implemented at low costin all online shops •Our ""smart"" implementation uses open-source libraries and may predict impact on individual-level profit of any nudge•Our return reduction on global scale would save…•…up to 200,000 tons of CO2 emissions •…up to 120,000 tons of waste
Annual impact for industry partner: in the six figures",7.586772203,1,"The generated answer and the ground truth provide the exact same numerical value, making them semantically identical.",1,"The generated answer is faithful. [cite_start]It is a direct quote from the provided context, which states, 'From a societal per - spective, based on U.S. figures from 2022, the 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions (Calma 2023)'[cite: 556].",1,"The context contains the exact sentence needed to answer the question: ""From a societal per - spective, based on U.S. figures from 2022, the 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions (Calma 2023)."""
What is the estimated annual savings in processing costs for the industry partner due to the green nudge?,"The effect corresponds to annual savings in processing costs of approximately $340,000.","The estimated annual savings in processing costs for the industry partner due to the green nudge is approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than annually. This amount is roughly equivalent to the 
annual emissions from the electricity consumption of 
121,000 U.S. homes.
3
Notably, the actual savings in 
emissions are likely to be even higher both as the vol -
ume of returns continues to grow further (Calma 2023 ) 
and because emissions through packaging and the pro -
duction process of discarded goods are not included.
From a business perspective, according to internal 
figures of our industry partner, the estimated annual 
profit increase quickly amortizes costs associated with 
the development, implementation, and maintenance of 
green nudging. In fact, the developed dual nudge 
turned out to be so successful that our industry partner 
opted to keep the green nudge running in the German 
online shop. The project’s success is affirmed by a prac -
tice executive who corroborates the effectiveness of the 
implemented dual-nudge approach by stating that
[t]he reduction in returns without a negative impact 
on revenue was huge for us. So huge, in fact, that we 
opted for keeping the experiment live and showing 
the successful treatment to all of our German online 
customers. —Practice partner executive
Apart from the immediate business benefits through effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge",6.05389452,1,"The generated answer and the ground truth provide the exact same numerical value ($340,000), making them semantically identical.",1,"The generated answer is faithful. [cite_start]It is a direct quote from the provided context which states, 'the effect corresponds to annual savings in processing costs of approximately $340,000'[cite: 582].",1,"The context directly answers the question. It states, ""For our industry partner, the observed effect translates into an estimated annual reduction in processing costs of about $340,000..."""
"In the 'Smart Green Nudging' paper, what was the sample size (n) of the randomized field experiment discussed?","The experiment had a sample size of 117,304 participants (n = 117,304).","The randomized field experiment in the “Smart Green Nudging” paper involved 73,235 visitors who made a purchase.","The rest of the paper is structured as follows. Section 
2 provides the theoretical background by reviewing 
related literature. Section 3 presents the setting and 
results of our randomized field experiment. Section 4
details the configuration and results from our causal 
machine learning model, along with the evaluation of 
smart green nudging. Section 5 concludes by discuss -
ing the practical implications of our work, its limita -
tions, and potential avenues for future research.
2. Related Literature
Our work relates to three streams of literature, namely 
(i) product returns, (ii) nudging, and (iii) treatment het -
erogeneities and causal machine learning.
2.1. Product Returns
The surge in product returns and the corresponding busi -
ness and environmental repercussions have prompted 
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
2 Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 
Downloaded from informs.org by [141.2.113.143] on 24 April 2025, at 02:36 . For personal use only, all rights reserved. significant variability in how individuals respond to 
nudging (Hummel and Maedche 2019 ), rendering the 
outcomes of green nudging unpredictable (Lehner et al. 
2016 ). Additionally, few researchers have explored the 
impact of green nudges on a large scale (see He et al. 
2023 for an exception), leaving a gap in our understand -
ing and a need to reconcile these divergent findings.
We extend prior research by conducting a large- 
scale, randomized field experiment to test the effective -
ness of green nudges in reducing product returns. Our 
green nudge effectively leverages customers’ desire to 
sustain a positive self-image through environmentally 
friendly behavior (Schubert 2017 ). It heightens aware -
ness of the negative environmental impact linked to 
returning products during both the prepurchase and 
postpurchase phases of the customer journey.
2.3. Treatment Heterogeneities and Causal 
Machine Learning
There often exists a notable heterogeneity in customer 
reactions to marketing strategies (El Kihal and Shehu 
2022 ), especially nudging techniques (Lehner et al. 
2016 , Mirsch et al. 2017 , Hummel and Maedche 2019 ). 
Accordingly, relying solely on average treatment nudge group (cart and reminder prompt). During the 
study, millions of visitors browsed the website. Our 
randomization checks (see Table 5 in Online Appendix 
A.2) confirmed the success of the randomization pro -
cess across various characteristics, including the inter -
net browser, the state associated with the IP 
geolocation, and the product count in the initial (i.e., 
preintervention) cart. Of the visitors across the three 
groups, 73,235 made a purchase. The conversion rate 
aligns with typical figures in the fashion retail industry. 
It is important to note that customers can view their 
cart at any time but need to do so at least once to com -
plete a purchase.
In our field experiment, we tracked several customer 
outcomes with our industry partner and its service pro -
viders, including sales, returns, and duplicate product 
purchases, as detailed in Table 1 . The key variable of 
interest is the customer’s decision to return at least one 
purchased product. This approach is driven by our pri -
mary scientific goal to reduce environmental externali -
ties, such as packaging waste and greenhouse gas 
emissions, associated with product returns. Specifi -
cally, our industry partner mandates returning all",6.1340096,0,"The generated answer (73,235 visitors who made a purchase) is factually different from the ground truth (117,304 participants), representing a subset of the total sample size rather than the sample size of the experiment itself.",1,"The generated answer is faithful. [cite_start]The context states that 'millions of visitors browsed the website' [cite: 602] [cite_start]and that 'Of the visitors across the three groups, 73,235 made a purchase'[cite: 603]. The generated answer correctly reflects the number of visitors who made a purchase.",0.7,"The context contains conflicting numbers. The abstract states the experiment was conducted with ""n = 117,304"", while the main body mentions ""73,235 made a purchase."" The question asks for the sample size of the experiment, which is stated in the abstract. However, the presence of the other number could cause confusion, making the context less than perfectly clear."
What are the two types of returns mentioned in the context of online shopping?,'Real returns' (where the customer intended to keep the item) and 'opportunistic returns' (where the customer orders multiple options with the intent to return some).,"According to the context, the two types of returns mentioned are:

1.  **Returns arising from discrepancies between expectations and reality** (often referred to as returns due to dissatisfaction).
2.  **Returns related to the environmental impact of returns** (the “green nudging” approach).","Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation marketing researchers and practitioners to examine dri -
vers of product returns (e.g., Harris 2008 , Pei and Paswan 
2018 ) and to seek solutions to curb them (P˚ alsson et al. 
2017 , Abdulla et al. 2019 ). Particularly problematic are 
opportunistic return behaviors. An example for such 
behavior is the practice of “bracketing,” where customers 
purchase multiple variants of an item with the intention 
of returning some (Altug et al. 2021 ). A plausible initial 
approach to address this issue is the adaptation of return 
policies. The literature differentiates between lenient poli -
cies, such as “no-questions-asked” full money-back guar -
antees, and more restrictive ones that, for example, 
impose restocking fees or nonrefundable handling 
charges (Su 2009 , Nageswaran et al. 2020 ). Although for -
going lenient policies can decrease returns (Shehu et al. 
2020 ), they also pose business drawbacks. For example, 
money-back guarantees can serve as quality signals 
(Moorthy and Srinivasan 1995 , Altug et al. 2021 ) to boost 
sales and profits (Davis et al. 1995 ). These business ramifi -
cations call into question the efficacy of exclusively rely -
ing on restrictive return policies from a practitioner’s 
standpoint. returns arising from discrepancies between expecta -
tions and reality (Zhou and Hinz 2016 ). Examples of 
customer-based interventions include virtual try-on 
sessions, customer reviews, and online consultations 
(Minnema et al. 2016 ). In essence, monetary and proce -
dural interventions operate in the postorder phase, 
whereas customer-based instruments work before and 
during the order process (Walsh and M ¨ohring 2017 ). 
Accordingly, monetary and procedural interventions 
are rather reactive measures, trying to motivate indivi -
duals to keep products that have already been pur -
chased. By contrast, consumer-based instruments are 
preemptive, aiming to avoid purchases of products 
that are highly likely to be returned.
Our study contributes to the literature on product 
returns by introducing a simple intervention that high -
lights the environmental impact of returns to custo -
mers. This nudging approach, a customer-based 
instrument, is distinct in its minimally invasive, low- 
cost nature, seeking simply to inform customers about 
the repercussions of their postpurchase actions. Closely 
related to our research, Ghose et al. ( 2023 ) recently 
assessed the effectiveness of pressure-oriented and self-",7.739594936,0,The generated answer identifies two different types of returns ('returns due to dissatisfaction' and 'returns related to environmental impact') which are not the same as the two types mentioned in the ground truth ('Real returns' and 'opportunistic returns').,1,"The generated answer is faithful. [cite_start]It accurately identifies the two types of returns described in the context: 'returns arising from discrepancies between expectations and reality' [cite: 624] [cite_start]and opportunistic returns, such as the practice of 'bracketing'[cite: 618].",1,"The context clearly distinguishes between two types of returns. It describes ""opportunistic return behaviors"" like ""bracketing"" (ordering multiple items with the intent to return some) and ""returns arising from discrepancies between expectations and reality"" (the item wasn't what the customer expected). These directly correspond to the two types mentioned in the ground truth."
"According to the paper, by how much did the smart green nudge decrease product returns compared to having no nudge at all?",The smart green nudge resulted in a 6.7% decrease in product returns compared to no nudge at all.,"According to the paper, the smart green nudge decreased product returns by 5.19% compared to having no nudge at all.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than our test data and report the results in Table 4 .
Table 4 shows return shipments, the return value, 
the conversion rate of the viewed carts, and (gross) 
sales. Additionally, we depict the “Share treated” col -
umn presenting the proportion of customers subjected 
to green nudging.
We observe a pronounced improvement in effective -
ness of smart green nudging in reducing returns; com -
pared with the naive approach of nudging all 
customers, smart green nudging can approximately 
double the impact, curbing returns by an additional 
3.15%. When compared against no nudging, the reduc -
tion is 5.19%. This decline in returns is not only mean -
ingful from a business standpoint but also holds 
considerable ecological weight.
Although the results from Table 4 look promising, 
we now follow Athey et al. ( 2023 ) and examine varying 
shares of treated customers with accompanying confi -
dence intervals for a more thorough examination. 
Figure 4 presents the mean returned shipments (on the 
vertical axis) as we vary the policy π by applying the 
intervention to different shares of the test data (on the 
horizontal axis). In essence, the curve shows the overall 
efficacy of smart green nudging as we progressively effective for customers who would otherwise return at 
an above-average level (see Table 8 in Online Appendix 
A.8). Conversely, for customers who would typically 
return less than average, green nudging seems to back -
fire. A possible reason is that green nudging may unin -
tentionally remind these customers of the return 
option, thereby inadvertently leading to more returns.
The analysis of group average treatment effects indi -
cates the CML model’s effectiveness in identifying het -
erogeneous treatment effects on unseen test data, and 
thus, it underscores its utility as an effective tool for 
personalization. However, the degree to which a CML- 
based “smart green nudging” policy amplifies the over -
all decrease in returns warrants further investigation, 
which we address in the subsequent analysis.
4.2.2. Gain from Smart Green Nudging. How effec -
tively does the “smart” CML-based policy reduce 
returns? We explore the overall effectiveness by evalu -
ating the IPS estimator from Equations ( 3 ) and ( 4 ) in 
our test data and report the results in Table 4 .
Table 4 shows return shipments, the return value, 
the conversion rate of the viewed carts, and (gross) 
sales. Additionally, we depict the “Share treated” col -",6.086755753,0,"The generated answer provides a figure of 5.19%, which is factually different from the 6.7% stated in the ground truth.",1,"The generated answer is faithful. It accurately reports the number directly from the provided context. [cite_start]A table in the context shows that under the 'Smart green nudging' policy, the 'Difference from no nudging, %' for 'Returns (shipment)' is -5.19%[cite: 643].",1,"The context directly answers the question, even though the value is different from the ground truth. A table in the provided text clearly shows the result for ""Smart green nudging"" and calculates the ""Difference from no nudging, %"" for ""Returns (shipment)"" as -5.19%."
"In the 'Smart Green Nudging' study, which variable was identified by SHAP values as the most pivotal for the prediction?",The initial cart value.,"According to the study, SHAP values indicated that the initial cart value was the variable most pivotal for the prediction.","contribute to the predictions (Senoner et al. 2022 ). In 
our case, SHAP values indicate that the initial cart 
value is the variable most pivotal for the prediction. We 
report SHAP values from our CML model in Online 
Appendix A.10. However, it is crucial to differentiate 
between variables that predict the treatment effect and 
those that act as causal moderators. We encourage 
future research to rigorously investigate the causal role 
of variables in moderating the effectiveness of green 
nudging.
Our study also underscores the effectiveness of using 
customer carts and digital footprint data to personalize 
marketing interventions. Our analyses reveal multiple 
characteristics that induce heterogeneous treatment 
effects, including the type of internet browser, the inter -
net provider, the federal state, and the weekday.
7
For 
example, weekend shoppers are more effectively 
nudged than weekday shoppers, possibly because of 
lower stress levels, aligning with similar literature find -
ings (Grahl et al. 2023 ). Similarly, the internet browser 
and other characteristics of the digital footprint can 
hint at underlying customer traits and demographics 
(Berg et al. 2020 ). Incorporating these insights, practi - task. In the following, we show SHAP values for initially biased and unbiased ML models. Each figure contains 
information on the overall importance of each feature for the prediction (higher position in the vertical ranking) and 
the features’ marginal effects for predictions about individual instances (position on the x-axis). Each dot represents a 
SHAP value for a feature and an instance. The color of the dots depicts the value of the feature from low (blue dots) 
to high (red dots). Plots jitter identical SHAP values vertically, providing insights into the SHAP value distribution per 
feature. Overall, Figures C1 and C2 show the relationship between the features (borrower characteristics) and the 
prediction (likelihood of this person mak ing a repayment) by revealing their marginal contribution to predictions (in 
terms of probability) conditional on feature values. 
 
Figure C1. SHAP Values, Example of Biased ML Model (max. initial label imbalance) 
The figure for the biased ML reveals that prior to any retraining, the gender attribute (0 = female, 1 = male) is the most 
relevant feature to predict whether a borrower will make a repayment. Our findings indicate that being female always 22. Mai 2025
Shapley scores: weighted average
•Local accuracy: attributions sum (exactly) to the prediction difference•Consistency: if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.•Missingness: unused features get SHAP value of 0 x*=0→𝜙+•Additivity: SHAP values can be added to show the joint contribution of several features to a prediction
22
𝜙%𝑓,𝑥=	2&!⊆	)!𝑧*!𝑀−𝑧*−1!𝑀! (𝑓)𝑧*−𝑓)𝑧*\i)Shapley value for i= savings
Model f
For individual x with a specific combination of features
Weighted sum over allSubsets z’of (transformed) data point x; whereM is total number of features in full set
Difference inModel prediction
𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛!=𝐸(-𝑓𝑋)	+	3""𝜙!",5.663115025,1,"The generated answer and the ground truth are semantically identical, both identifying the 'initial cart value' as the most pivotal variable.",1,"The generated answer is faithful. [cite_start]The context contains the direct statement: 'In our case, SHAP values indicate that the initial cart value is the variable most pivotal for the prediction'[cite: 657].",1,"The context directly and explicitly answers the question with the sentence: ""In our case, SHAP values indicate that the initial cart value is the variable most pivotal for the prediction."""
"In the field experiment from the 'Smart Green Nudging' paper, what was the initial aim of the academic team members regarding the prompts they were testing?",The initial aim was to isolate the effects of the cart prompt and the reminder prompt.,"The initial aim of the academic team members was to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects.","Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con -
strained our options, permitting only the testing of one 
of the design elements in isolation. Ultimately, our 
industry partner opted to test the prompt displayed 
after consumers finished their purchasing. This choice 
was based on the belief that the reminder displayed 
after the purchase posed a lower risk of negatively 
impacting sales.
3.1.3. Study Details. We conducted a large-scale, ran -
domized field experiment over seven weeks in our 
industry partner’s German online shop to test the dual- 
nudge and single-nudge interventions.
2
The study ran -
domly assigned website visitors using stationary 
devices to one of three groups upon their first view of 
the shopping cart: the control group (no nudging), the 
single-nudge group (reminder prompt), or the dual- 
nudge group (cart and reminder prompt). During the 
study, millions of visitors browsed the website. Our 
randomization checks (see Table 5 in Online Appendix 
A.2) confirmed the success of the randomization pro - significant variability in how individuals respond to 
nudging (Hummel and Maedche 2019 ), rendering the 
outcomes of green nudging unpredictable (Lehner et al. 
2016 ). Additionally, few researchers have explored the 
impact of green nudges on a large scale (see He et al. 
2023 for an exception), leaving a gap in our understand -
ing and a need to reconcile these divergent findings.
We extend prior research by conducting a large- 
scale, randomized field experiment to test the effective -
ness of green nudges in reducing product returns. Our 
green nudge effectively leverages customers’ desire to 
sustain a positive self-image through environmentally 
friendly behavior (Schubert 2017 ). It heightens aware -
ness of the negative environmental impact linked to 
returning products during both the prepurchase and 
postpurchase phases of the customer journey.
2.3. Treatment Heterogeneities and Causal 
Machine Learning
There often exists a notable heterogeneity in customer 
reactions to marketing strategies (El Kihal and Shehu 
2022 ), especially nudging techniques (Lehner et al. 
2016 , Mirsch et al. 2017 , Hummel and Maedche 2019 ). 
Accordingly, relying solely on average treatment The rest of the paper is structured as follows. Section 
2 provides the theoretical background by reviewing 
related literature. Section 3 presents the setting and 
results of our randomized field experiment. Section 4
details the configuration and results from our causal 
machine learning model, along with the evaluation of 
smart green nudging. Section 5 concludes by discuss -
ing the practical implications of our work, its limita -
tions, and potential avenues for future research.
2. Related Literature
Our work relates to three streams of literature, namely 
(i) product returns, (ii) nudging, and (iii) treatment het -
erogeneities and causal machine learning.
2.1. Product Returns
The surge in product returns and the corresponding busi -
ness and environmental repercussions have prompted 
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
2 Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 
Downloaded from informs.org by [141.2.113.143] on 24 April 2025, at 02:36 . For personal use only, all rights reserved.",6.342064857,1,The generated answer and the ground truth are semantically identical. Both state that the initial aim was to isolate the effects of the two different prompts.,1,"The generated answer is faithful as it is a direct quote from the context. [cite_start]The context states, 'Notably, the academic team members initially aimed to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects'[cite: 675].",1,"The context provides a direct and verbatim answer to the question: ""Notably, the academic team members initially aimed to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects."""
"In the 'Smart Green Nudging' paper, what were the estimated annual savings in processing costs for the industry partner as a result of the green nudging intervention?","The estimated annual savings in processing costs were approximately $340,000.","According to the paper, the estimated annual reduction in processing costs for the industry partner as a result of the green nudging intervention was approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than variation in treatment effects across different states (see 
Online Appendix A.9). However, excluding indivi -
duals from Baden-W ¨urttemberg who, on average, 
exhibit a notable adverse reaction to the green nudge 
results in a modest improvement of 0.1 percentage 
points in the effectiveness of the green nudge.
Overall, the results for smart green nudging offer an 
optimistic outlook. From a business perspective, imple -
menting smart green nudging could potentially double 
the cost savings compared with the naive strategy. 
Equally significant is the environmental benefit. 
Applied across the United States in logistics, this 
approach could double the effect on CO
2 
emissions to 
an impressive total of more than 1 million tons annu -
ally (Calma 2023 ). These dual advantages underscore 
Table 4. Summary Statistics from the Analyses of Smart Green Nudging on Test Data
Share treated, %
Returns (shipment) Returns (value) Cart conversion Sales
Mean (SE) Mean (SE) Mean (SE) Mean (SE)
No nudging 0.0 0.262 (0.005) 18.61 (0.58) 0.628 (0.006) 51.96 (0.90)
Naive green nudging 100.0 0.256 (0.005) 18.22 (0.56) 0.631 (0.005) 52.49 (0.87)
Difference from no nudging, % � 2.10 � 2.10 0.51 1.01 Kihal et al. 2021 , Pfrang and Spreer 2022 ). This paper 
aims to contribute to this discussion by empirically 
evaluating the effectiveness of an informational inter -
vention, referred to as green nudging, in decreasing 
product returns.
In this study, we partnered with a leading European 
fashion retailer. Like many in the industry, the retailer 
grapples with the challenges and costs arising from 
online product returns. Seeking to align profitability 
with environmental responsibility, we collaborated 
with our partner to craft a strategy that not only curtails 
product returns but also, that resonates with the retai -
ler’s sustainable ethos and is economically viable. The 
jointly developed green nudge comprises two prompts: 
informing online customers about the negative envi -
ronmental impact of product returns throughout 
(shopping cart prompt) and at the conclusion of their 
online shopping journey (reminder prompt). We con -
ducted an extensive field experiment with 117,304 par -
ticipants in the retailer’s online shop to empirically 
validate the green nudge. We then apply modern 
causal machine learning (CML) methods (Athey et al. 
2019 ) to analyze treatment heterogeneities based on",7.256868362,1,"The generated answer and the ground truth provide the exact same figure ($340,000), making them semantically identical.",1,"The generated answer is faithful. [cite_start]It accurately reports the figure mentioned in the context, which states, 'the observed effect translates into an estimated annual reduction in processing costs of about $340,000'[cite: 697].",1,"The context directly answers the question with the phrase: ""...the observed effect translates into an estimated annual reduction in processing costs of about $340,000..."""
Where is INFORMS located?,"INFORMS is located in Maryland, USA.","INFORMS is located in Maryland, USA.","This article was downloaded by: [141.2.113.143] On: 24 April 2025, At: 02:36
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA
Marketing Science
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org
Smart Green Nudging: Reducing Product Returns
Through Digital Footprints and Causal Machine
Learning
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian
Speicher, Oliver Hinz
To cite this article:
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian Speicher, Oliver Hinz (2024)
Smart Green Nudging: Reducing Product Returns Through Digital Footprints and Causal Machine Learning.
Marketing Science
Published online in Articles in Advance 08 Aug 2024
. https://doi.org/10.1287/mksc.2022.0393
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The Informationen zur Vorlesung: Gehalten am Do. 12:00 bis 16:00; wöchentlich vom 24.04.2025 bis 24.07.2025; Ort: Hörsaalzentrum Westend - HZ 12 DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org.",3.088027,1,"The generated answer and the ground truth are semantically identical, both stating that INFORMS is located in Maryland, USA.",1,"The generated answer is faithful. [cite_start]The context explicitly states, 'INFORMS is located in Maryland, USA'[cite: 713].",1,"The context explicitly states the location in the publisher information section: ""INFORMS is located in Maryland, USA."""
What programming language will be used in the course?,Python.,The course will use the programming language Python.,"24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen. 23. Juli 2025
Yourhelpisappreciated
2
Survey on LearnMateChatbot for this course",2.779830217,1,"The generated answer and the ground truth are semantically identical, both stating that Python is the programming language used.",1,"The generated answer is faithful. [cite_start]The context clearly states, 'Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt fÃ¼r Schritt an die Umsetzung von ML-Projekten herangefÃ¼hrt' (With the help of the programming language Python, participants will be guided step-by-step through the implementation of ML projects)[cite: 718].",1,"The context directly answers the question in the sentence: ""Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt fÃ¼r Schritt an die Umsetzung von ML-Projekten herangefÃ¼hrt"" (With the help of the programming language Python, the participants will be guided step-by-step to the implementation of ML projects)."
"According to the text, what are four useful applications of Large Language Models (LLMs) in everyday life?","Four useful applications of LLMs are: synthesizing information (such as summarizing and translating), acting as an argumentative sparring partner to identify weak points in arguments, brainstorming creative ideas, and assisting with code-related tasks like documentation and translation between programming languages.","The text doesn’t list four specific “everyday life” applications of Large Language Models (LLMs). However, it does highlight these potential uses:

1.  **Synthesizing information:** Summarizing and translating blocks of information.
2.  **Argumentative sparring partner:** Pointing out weak points in arguments.
3.  **Brainstorming:** Co-creation of creative ideas.
4.  **Code documentation or transformation:** Generate docs for software and translate across languages.

It also emphasizes the importance of user interaction (""the Bereitschaft des Nutzers, mit den Systemen zu interagieren"") as a key factor in the impact of LLMs on society and productivity.","23. Juli 2025
Useful in everyday life
20
•Synthesizing information: Summarizing and translating blocks of information, e.g., bullet points, into structured text•Argumentative sparring partner: pointing out weak points in arguments•Brainstorming: co-creation of creative ideas•Code documentation or transformation: generate docs for software and translate across languages Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. gemacht hat,
das User Interface war
die reine Technologie dieser 
großen Sprachmodelle.
Die gibt es theoretisch schon 
seit 2017.
Die wurden natürlich immer 
besser.
Wir hatten am Ende die Version 
drei Punkt irgendwas,
was dann integriert wurde.
Aber es wurde integriert.
Dieses fundamentale KI Modell in
eine ausreichend kompatible und
nutzerfreundliche Umgebung,
nämlich Chat GPT, was
noch mal das Grundmodell GPT
plus weitere Systeme sind, die 
hier zusammenspielen.
Das heißt Forgeat GPT.
Gab es diese großen 
Sprachmodelle schon?
Die haben sich wie gesagt immer 
weiter verbessert.
Grundsätzlich haben
wir aber alle erst damit 
angefangen und
haben eine ganze Industrie 
eigentlich damit angefangen,
es kommerziell zu benutzen.
Ab dem Moment, wo dieses
User Interface richtig 
hinbekommen wurde
und die User des Systems 
hochgegangen ist.
Das ist eine der Kern messages
die Bereitschaft des Nutzers,
mit den Systemen zu 
interagieren.
Generell, wo wir
häufig von Adoption reden 
Technologie use.
Wenn wir ganz, ganz breit sind.
Das ist zentral dafür,
wie eine Technologie tatsächlich
letztlich Einfluss haben kann 
auf unser Sozialleben,
auf unsere ökonomischen 
Prozesse,
auf Unternehmen
und letztlich natürlich auf 
Produktivität.",12.46545911,1,"The generated answer's four points (synthesizing info, sparring partner, brainstorming, code assistance) are semantically identical to the four applications listed in the ground truth.",1,"The generated answer is faithful. [cite_start]The context provides a bulleted list of applications [cite: 728] that directly match the four uses mentioned in the answer: synthesizing information, acting as an argumentative sparring partner, brainstorming, and code documentation/transformation.",1,"The context provides a bulleted list of applications that directly correspond to the four useful applications mentioned in the question: synthesizing information, acting as an argumentative sparring partner, brainstorming, and assisting with code-related tasks."
For what types of tasks are basic chat models the most suitable choice?,Basic chat models are most suitable for low-stake tasks where a fast response is required.,"According to the text, basic chat models are most suitable for tasks that are “rather low stake and fast response is required.”","23. Juli 2025
Many models, but which one should you use?
18
Reasoning model: Choose when task is rather complex, results are important, and time to receive response
Basic chat model: Choose when task is rather low stake and fast response is required.
Example 1: •Which vegetable has the highest vitamin C amount?•Act as a nutritionist and create a vegetable salad with at least 500𝜇𝑔vitamin C and 200𝜇𝑔vitamin D.Example 2: •What is an LLM?•Act as a computer scientist and explain to me what the advantage of a reasoning model over a chat model is. 15. Mai 2025
Recap: a task-based approach
4
Task 1Task 2... Task N-1 Task N
Workflow
Decomposition of workflowsto identify suitable individual tasks that AI can perform•Evaluate AI applicability based on task attributes (routine, analytical, standardized)•Identify (hidden) prediction problemsin tasks, e.g., customer needs, optimal ad textRethink processesand imagine what AI systems can help with•Installation of individual AI often entail further changes, e.g., new tasks for humans•Understand what is necessary for AI systems to work properly•Notion of complements and substitutesin organizational structures 23. Juli 2025
Give examples whenever possible (few-shot)
13
Humans often rely on previous examples to work on tasks•Examples can also be useful for LLMs (example = shot)•Help show LLMs the structure of desired output•Zero shot-prompting may be ineffective if response needs to be very specificIf you want specific styles and formats try giving examples•Add example(s) as part of prompt•Instruct LLM to follow examples•Beware too many examples!",4.439547062,1,"The generated answer and the ground truth are semantically identical. Both state that basic chat models are best for low-stake, fast-response tasks.",1,The generated answer is faithful. [cite_start]The context contains a slide that directly states for a 'Basic chat model: Choose when task is rather low stake and fast response is required'[cite: 743].,1,The context directly answers the question. A slide explicitly states for a 'Basic chat model: Choose when task is rather low stake and fast response is required.'
"According to the document, what is the population of Munich?",1578567,"According to the document, the population of Munich is 1,578,567.","23. Juli 2025
Human v. LLM ”Thinking”
7
Is the following statement true? “Munich’s population is 2.12 times that of Frankfurt.”A potential Human (cognitive) process:• Think about knowing these numbers; most likely not• Use Wikipedia tool: get both of these populations•Munich: 1,578,567•Frankfurt: 773,068• Try to divide the numbers in your head; rough idea bit not exact number• Use calculator tool: 1,578,567/ 773,068 = 2.042• Reflection about correctness of result; number sounds reasonable• Forming an answer• Start writing an answer: “Yes, the statement is correct.”• Checking the entire solution process, the answer sentence and maybe rewrite it to “The statement is true.”
This is not how LLMs “think”! decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were 
recruited to participate in an ongoing field study. The study 
was conducted by a subset of this paper ’s authors and 
aimed to examine correlations between academic 
performance, personal characteristics, and behavioral 
measurements, including outcomes of sequential social 
dilemmas such as the investment game. The study, 
accessible via a link sent to students ’ email addresses, 
comprised a comprehensive survey on personal 
demographics, socioeconomic background, cognitive 
abilities, personality traits, and, crucially for our 
simulations, a sequential social dilemma game 
incorporating our previously described investment game (a 
detailed description of this game can be found in Appendix 
B.1). Participants’ decisions were incentivized, allowing 
for the measurement of revealed preferences, an approach 
superior to simple self -reported measures due to its 
potential to reduce presentation and demand effects 
(Camerer and Hogarth, 1999). The incentives involved 
earning real money based on the game outcomes with other 15. Mai 2025
Accurate predictions are often not enough
13
Laws against discrimination in many domains includingCredit (Equal Credit Opportunity Act),Education (Civil Rights Act of 1964; Education Amendments of 1972), Employment (Civil Rights Act of 1964), Housing (Fair Housing Act), Public Accommodation (Civil Rights Act of 1964)
§Born and raised in Germany§90% of population§data describes them accurately §accurate predictions (95% accurate)
§Migrated to Germany in past years§10% of population§data describes them poorly§poor predictions (50% accurate)
90.5% accurate prediction about creditworthiness
Consider an example where machine learning model predicts individual creditworthiness
ML model",4.420750141,1,"The generated answer and the ground truth state the exact same population figure, making them semantically identical.",1,"The generated answer is faithful. [cite_start]The context includes a slide that directly states the population of Munich as 1,578,567[cite: 748].",1,"The context provides a slide that directly states the population of Munich as 1,578,567, which is used in a sample cognitive process."
How many parameters did GPT-2 have?,1.5B parameters.,GPT-2 had 1.5B parameters.,"17. Juli 2025
Evolution of OpenAI’s GPT modelsGPT:117M parameters (Radford et al., 2018)•Transformer decoder with 12 layers•Trained on BooksCorpus: over 7000 unique books (4.6GB text)GPT-2:1.5B parameters (Radford et al., 2019)•Same architecture as GPT but more flexible•Also: trained on much more data: 40 GB of internet text dataGPT-3:175B parameters (Brown et al., 2020)•Over 600GB of text dataGPT-4:1.76T parameters (undisclosed)•Mixture of Expert: different expert models with >200B parameter each•10 trillion words ~ 40 Billion pages (exact size unknown)
35 17. Juli 2025
GPT training
34
Source: State of GPT | BRK216HFS
Stage 1Stage 2 Stage 3 17. Juli 2025
GPT
33
GPT Architecture
It is “straightforward” to build a GPT model…~100 lines of code would do it. But training it is another story!",2.97255969,1,"The generated answer and the ground truth state the exact same number of parameters, making them semantically identical.",1,The generated answer is faithful. [cite_start]The context includes a slide that explicitly lists 'GPT-2: 1.5B parameters'[cite: 757].,1,"The context provides a slide titled ""Evolution of OpenAIâ€™s GPT models"" which clearly lists ""GPT-2: 1.5B parameters""."
What is the intuition behind how an 'attention head' works in a language model?,"An attention head represents a word as a weighted combination of itself and all the words that came before it. The weights in this combination are based on the similarity between the words, which helps the model understand the context of the sentence.","Okay, let's break down the intuition behind ""attention heads"" in language models, based on the provided text and general knowledge. It’s a complex concept, but here’s a simplified explanation:

**The Core Idea: Focusing on Relevance**

Imagine you're reading a sentence. You don't process every word equally. You instinctively focus on the words that are most *relevant* to understanding the meaning. Attention heads mimic this process.

**How it Works (Simplified):**

1. **Each Word Gets a ""Score"":**  When a language model processes a word, it calculates a ""score"" for *every other word* in the input sequence. This score represents how much attention the model should pay to that other word when processing the current word.

2. **Multiple ""Heads"" – Different Perspectives:**  Instead of just *one* set of scores, a language model uses *multiple* ""attention heads."" Each head learns a *different* set of scores. Think of it like having multiple people reading the same sentence, each focusing on a different aspect of its meaning.

   * **Example:** One head might focus on grammatical relationships between words. Another might focus on semantic similarity (words with related meanings). A third might focus on the order of events.

3. **Weighted Sum:** Each head produces a weighted representation of the input sequence. These weighted representations are then combined to create a final representation of the current word, taking into account the relevant information from all the other words.

**Connecting to the Text's Context:**

The text emphasizes that these attention heads are crucial for understanding complex relationships within a sequence.  It highlights that the model isn't just looking at the immediate neighbors of a word, but considering its entire context.  The ""moderators"" mentioned in the text (like age or experience) would influence *how* these attention heads operate – perhaps making certain relationships more or less important.

**Analogy:**

Think of it like a detective investigating a crime. One detective might focus on fingerprints, another on witness statements, and another on forensic evidence.  Each ""attention head"" is like a different detective, bringing a specialized perspective to the investigation.

**Important Note:** The text doesn't go into the mathematical details of how these scores are calculated, but this is the fundamental intuition behind attention heads – allowing the model to selectively focus on the most relevant parts of the input sequence.

Do you want me to delve deeper into any specific aspect of this explanation, such as:

*   The mathematical formulas involved (though they are quite complex)?
*   How attention heads contribute to tasks like translation or question answering?","des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses die Entscheidungsgewalt haben,
der Mensch in diesem Loop
würde nie auf die Maschine 
hören.
Alle möglichen Effizienzgewinne, 
die wir hätten,
werden gar nicht realisiert.
Es kommt immer auf die Nutzung 
anderer
will ich sie heute 
sensibilisieren.
Und da gibt es eine 
Riesenliteratur zu
ganz viel aktive Forschung,
wie wir eigentlich erreichen 
können,
dass Menschen vernünftig mit
diesen Maschinen interagieren.
Richtig hinterfragen,
welche Informationen sie wie 
gewichten sollen.
Und wenn Sie jetzt mal an Chat 
GPT
denken und andere große 
Sprachmodelle,
wo Sie der Nutzer sind Sie 
machen Eingaben,
Sie kriegen Vorschläge an 
Ausgaben.
Wir sind noch nicht bei KI 
Agenten,
wo ich sage ich möchte einen 
Urlaub gebucht haben.
Klick macht das alles, sondern 
diese Systeme.
Sie können hingehen und fragen 
wo
kann ich guten Urlaub machen?
Dieses und jenes sind meine 
Präferenzen.
Eine solche und solche Art von 
Urlaub möchte ich machen.
Und dann kriegen Sie einen 
Vorschlag von diesem System.
Ihre Rolle dabei ist dann 
letztlich zu entscheiden
Ist diese Information gut oder 
schlecht?
Ist es eine Information,
die Sie nutzen sollten in
Ihrer Entscheidungsfindung oder 
nicht?
Und da sehen wir mehr und mehr,
dass ich eben diese Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",39.54837894,0.8,"The generated answer is overly complex and detailed compared to the concise ground truth. While it captures the core idea of focusing on relevant words, its explanation of scores, multiple heads, and weighted sums goes far beyond the simple, intuitive explanation in the ground truth, making it only partially aligned.",0.5,"The generated answer is only partially faithful. [cite_start]While the context discusses related concepts like 'moderators' that influence relationships between variables[cite: 782], it does not explicitly define or explain the mechanism of an 'attention head'. The analogy is an inference by the model, not a direct statement from the context. The core claims about how an attention head works (calculating scores, using multiple heads, creating a weighted sum) are not present in the provided text.",0.4,"The context discusses related concepts like 'moderators' that influence relationships between factors, which is analogous to how attention works by weighting the influence of different words. However, it does not explicitly define or explain the mechanism of an 'attention head' in a language model. The connection is conceptual but indirect."
What is Word2Vec and what is its function?,Word2Vec is a simple neural network that is used to generate word embeddings from a text corpus.,"Based on the provided context, here’s what we can gather about Word2Vec:

**What is Word2Vec?**

Word2Vec is a simple neural network used to generate word embeddings. 

**Its Function:**

*   **Creates Vector Representations of Words:** It takes a text corpus (a large collection of text) and learns to represent each word as a numerical vector.
*   **Similar Words Have Similar Vectors:** The key idea is that words that appear in similar contexts will have similar vectors. This allows you to capture semantic relationships between words.
*   **Fixed Dictionary of Vectors:** It provides a fixed dictionary of vectors that you can use for various natural language processing tasks.

**How it works (as described in the context):**

*   **Prediction:** Word2Vec predicts words from their surrounding context (or predicts the context from a word itself - skipgram).
*   **Learned Weights:** The weights learned during training are used as the word embeddings.

Do you want to know more about a specific aspect of Word2Vec, like how it’s trained or the different prediction methods (CBOW or Skipgram)?","17. Juli 2025
Learning word embeddings with Word2VecWord2Vec: simple neural networkto generate word embeddings given a text corpus•Idea: predict words from their surrounding context•Learned weights are embeddings•Through optimization: words in similar context have similar embeddings•Gives us a fixed dictionary of vectors for words we can utilizeIntuitive example: assume we have a text corpus with two sentences
15
Training dataPizza is awesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layer Output layer 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
Word2Vec example (continuous bag of words version)
16
Training dataPizzaisawesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1001
Words àOne-hot Vector One-hot VectoràWords0100
Training dataPizza is awesomePizzaisgreatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1010
Words àOne-hot Vector One-hot VectoràWords0100",18.08851957,1,The generated answer and the ground truth are semantically identical. Both define Word2Vec as a simple neural network used to generate word embeddings from a text corpus.,1,"The generated answer is faithful. [cite_start]The context explicitly describes Word2Vec as a 'simple neural network to generate word embeddings given a text corpus'[cite: 809]. [cite_start]It explains that its function is to 'predict words from their surrounding context' [cite: 809] [cite_start]and that the 'Learned weights are embeddings' [cite: 809][cite_start], which provides a 'fixed dictionary of vectors'[cite: 809].",1,"The context directly explains the function of Word2Vec. A slide is titled ""Learning word embeddings with Word2Vec"" and the bullet points explain its idea: ""simple neural network to generate word embeddings given a text corpus"" and ""predict words from their surrounding context"" where ""Learned weights are embeddings."""
What is one potential problem with representing words as indexed tokens?,Lacking interpretability of distance.,"According to the text, a potential drawback of representing words as indexed tokens is “Lacking interpretability of distance.”","17. Juli 2025
TokenizationMost simple view: words are discrete and independent tokens•Represent words as indexed tokens
What is a potential problem here?•Lacking interpretability of distance •Idea: Employ one-hot-encoding so that distances are all the same
10
WordIndex tokenHello 0… …King 145Queen156… …
WordIndex tokenOne-hot vectorHello 0 [1,0,…,0,0,…,0]… … …King 145 [0,0,…,1,0,…,0]Queen156 [0,0,…,0,1,…,0]… … … 23. Juli 2025
Current LLMs
5
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token
Input sentence 17. Juli 2025
Current LLMs
28
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token",4.845431805,1,"The generated answer and the ground truth are semantically identical, both identifying the lack of interpretable distance as a key problem.",1,The generated answer is faithful. [cite_start]The context directly states that a potential problem with representing words as indexed tokens is a 'Lacking interpretability of distance'[cite: 811].,1,"The context directly answers the question. A slide presents the method of representing words as indexed tokens and explicitly asks, ""What is a potential problem here?"" followed by the answer, ""Lacking interpretability of distance."""
"According to the gradient descent, what two steps are taken to decrease a model's error during training?","To decrease the error, the model's weights (w) are changed in the direction of the negative slope, and the size of this change is scaled by the learning rate (η).","According to the provided text, the two steps taken during training to reduce a model’s error using gradient descent are:

1.  Compute gradients: ,ℒ,-, ,ℒ,.(through back-propagation algorithm)
2.  Update weights and biases: 𝑤=𝑤−𝜂⋅,ℒ,-, 𝑏=𝑏−𝜂⋅,ℒ,.","10. Juli 2025
Gradient descent: the optimization algorithm
27
We solve the following problem during training
Complexity of problem requires a step-wise solving using gradient descent:1.Initialize 𝑤,𝑏randomly2.Loop until convergence of loss1.Compute gradients: ,ℒ,-, ,ℒ,.(through back-propagation algorithm)2.Update weights and biases: 𝑤=𝑤−𝜂⋅,ℒ,-, 𝑏=𝑏−𝜂⋅,ℒ,.3.Compute new loss: ℒA𝑦𝑤,𝑏,𝑦3.Return weights
argminY,Z	ℒ/𝑦𝑥|𝑤,𝑏,𝑦 10. Juli 2025
Gradient descent: the intuition
22
To decrease error, change 𝑤in the direction of negative slopeScale the size of change by learning rate 𝜂(here: 0.01)
𝑤1=𝑤0−𝜂⋅𝜕ℒ𝑤0𝜕𝑤0=1−0.01⋅82=0.18
ℒ𝑤
𝑆𝑙𝑜𝑝𝑒	𝑎𝑡	𝑤0=𝜕ℒ(𝑤0)𝜕𝑤0
Error(Loss) 3. Juli 2025
Adaptive Boosting (AdaBoost)
12
Training Procedure:
1. All observations in the dataset are initially given equal weights
2. Fit weak to the data and calculate errors for each observation in training set
3. Train next model on (re)weighted training set where previously wrongly predicted observations 
have a higher weight
4. Repeat procedure until convergence is achieved of restrictions are met
Prediction of new instances based on weighted majority vote of ensemble (weights 
according to individual model accuracy) 
Source",9.529709339,1,The generated answer and the ground truth are semantically identical. Both describe the process of changing weights in the direction of the negative slope and scaling this change by the learning rate.,1,The generated answer is faithful. The context explicitly outlines the two steps of gradient descent: '1. Compute gradients' and '2. [cite_start]Update weights and biases' using the provided formulas[cite: 813]. [cite_start]Another slide explains the intuition as changing the weight 'in the direction of negative slope' and scaling the change by the 'learning rate Î·'[cite: 813].,1,"The context directly explains the two steps. A slide with a diagram illustrating gradient descent states, ""To decrease error, change w in the direction of negative slope"" and ""Scale the size of change by learning rate Î·."" It also provides the formula for updating the weights."
"According to the text, what are hyperparameters and what is their primary function in machine learning?","Hyperparameters are variables that determine the structure of a model and its learning process. Their primary function is to regularize the model, which helps to control for overfitting and underfitting. For example, in a neural network, hyperparameters include the number of layers and neurons.","According to the text, the primary function of hyperparameters in machine learning is to “determine the structure of the model and the learning process” and “allow us to regularize our model and shape over- and underfitting outcomes.”","3. Juli 2025
Hyperparameters
The number of trees (parameters) and other choices on the model 
architecture (e.g., tree depth) are hyperparameters 
Hyperparameters are variables that 
• determine the structure of the model and the learning process
• allow us to regularize our model and shape over- and underfitting outcomes 
• different models and learning algorithms have different hyperparameters
• For example: neural networks’ hyperparameters include
• # layers
• # neurons per layer
• activation functions
By tuning hyperparameters, we can find a “good model complexity”, but 
of course the question is how we do that given the first rule of testing?
24 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
19
Representation +Evaluation +Optimization
Adjustmodel according to an algorithmic procedure until stop criterionis met.In linear regression: minimize the sum of squares (closed form) or gradient descent (iterative optimization).
OLS: min%⁄""&∑'𝑦−𝑦'
𝐿𝛽
𝛽
𝐿
𝛽∗ Journal of the Association for Information Systems 
 
814 
known to the investor. Again, the underlying model was 
a gradient boosted forest for which we automatically 
optimized the same seven hyperparameters on the current 
training dataset using a threefold cross-validation strategy 
with a directed Bayesian search.11 We used the remaining 
30% of the data from Study A to populate the pool of 
borrowers in our simulation, from which we randomly 
drew and matched 50 borrowers with randomly drawn 
investors from the corresponding investor pool. On 
average, the unbiased ML model’s accuracy for the share 
of observations not included in the training before the first 
updating iteration was 68.4%. 12  As we previously 
measured borrowers’ repayment decisions using  the 
strategy method in Study A, we knew whether they would 
make a repayment if there was an investment. Hence, we 
were able to determine whether the investment game 
between a given investor and borrower would result in (1) 
no investment, (2) investment but no repayment, or (3) 
investment and repayment by matching the unconditional 
investor with the conditional borrower decisions. 
Treatments: We introduce d treatment variations",7.017192125,1,"The generated answer and the ground truth are semantically identical. Both define hyperparameters as variables that determine model structure and its learning process, used to control for overfitting and underfitting.",1,The generated answer is faithful. [cite_start]The context defines hyperparameters as variables that 'determine the structure of the model and the learning process' and 'allow us to regularize our model and shape over- and underfitting outcomes'[cite: 817]. [cite_start]It also provides the example of layers and neurons for neural networks[cite: 817].,1,"The context directly answers the question. A slide titled ""Hyperparameters"" defines them as variables that ""determine the structure of the model and the learning process"" and ""allow us to regularize our model and shape over- and underfitting outcomes."""
What are the cores advantages of ensemble methods?,"Improved accuracy, Reduction of overfitting and Handling High Dimensionality.","According to the provided text, the core advantages of ensemble methods are:

*   **Improved Accuracy:** Ensemble methods result in a prediction accuracy that is better than any individual model could achieve.
*   **Reduction of Overfitting:** They tend to be more robust and less prone to overfitting.
*   **Handling High Dimensionality:** They can effectively handle large amounts of data with high-dimensional spaces.","3. Juli 2025
(Dis)advantages of ensembles
Core advantages of ensemble methods include:
1. Improved Accuracy: Ensemble methods result in a prediction accuracy that is better than
any individual model could achieve
2. Reduction of Overfitting: Because they use multiple learners, ensemble methods tend to
be more robust and less prone to overfitting, i.e., typically generalize rather well
3. Handling High Dimensionality: Ensemble methods can effectively handle large amounts
of data with high-dimensional spaces.
Core disadvantages of ensemble methods include:
1. Increased Complexity: Can be computationally expensive and require significant 
resources to train because they involve training multiple models instead of a single one.
2. Interpretability: Ensemble models are difficult to interpret compared to individual models. 
3. Risk of Overfitting with Noise: Although ensemble methods are generally more robust to 
overfitting, they can still overfit if the data is noisy. 
14 3. Juli 2025
Ensemble methods
Ensemble methods rely on different models that are simultaneously applied to the same problem
Predictions are aggregated producing the final prediction
• Hard vote: majority of individual votes
• Soft vote: average individual predicted probabilities and one final vote
• Meta model: train machine learning model to aggregate prediction (stacking)
Models‘ errors should be as uncorrelated as possible so that they cancel out through aggregation
6
Model 1
Model 2
Model 3
Model N-1
Model N
…
Aggregator Overall 
prediction 10. Juli 2025
Last time: ensemble methods
•Different models that are simultaneously applied to the same problem•However: only ”one layer” of models, i.e., no (comprehensive) stacking•Today: layers if modelsfeedingintoeachother•Idea(smallexample):
2
Model 1Model 2Model 3
Model N-1Model N… AggregatorOverall prediction
Model 1, Layer 1Model 2, Layer 1Model 1, Layer 2Model 2, Layer 2Final model(Aggregator)Overall prediction",8.732241392,1,"The generated answer is semantically identical to the ground truth, listing the same three core advantages.",1,"The generated answer is faithful. [cite_start]The context explicitly lists the 'Core advantages of ensemble methods' as 'Improved Accuracy', 'Reduction of Overfitting', and 'Handling High Dimensionality'[cite: 827].",1,"The context directly answers the question. A slide titled ""(Dis)advantages of ensembles"" lists the ""Core advantages,"" including ""Improved Accuracy,"" ""Reduction of Overfitting,"" and ""Handling High Dimensionality."""
What are some core disadvantages of decision trees?,"Overfitting, instability, and optimization difficulty.","According to the provided text, some core disadvantages of decision trees include:

1.  **Overfitting:** Decision trees can quickly lead to overfitting when they become too complex (too many nodes, levels, or leaves).
2.  **Instability:** Decision trees can be remarkably unstable, meaning they can produce different trees based on slight variations in the training data.
3.  **Optimization difficulty:** Algorithms used to build decision trees are typically heuristics and cannot guarantee finding the globally optimal tree.","3. Juli 2025
Last time: decision trees
Core disadvantages of decision trees include:
1. Overfitting: Can quickly lead to overfitting when tree is too complex, i.e., when it has too 
many nodes, levels, or leaves
2. Instability: Decision trees may be remarkably unstable conditional on variation in training 
data, leading to different trees
3. Optimization difficulty: Algorithms to build tree are typically heuristics that cannot 
guarantee global optimality
2
#  No de fau lt: 1 80  (9 0%)
#  Def au lt: 2 0 ( 10 %)
Ov era ll s ample
#  No de fau lt: 5 3 ( 91 .4 % )
#  Def au lt: 5  (8 .6%)
#  No de fau lt: 1 27  (8 9. 4%)
#  Def au lt: 1 5 ( 10 .6%)
Age = Young Age = O ld
#  No de fau lt: 3 2 ( 10 0%)
#  Def au lt: 0  (0 %)
Income 
>  50 .00 0
Income 
< = 5 0.0 00
#  No de fau lt: 2 1 ( 80 .8 % )
#  Def au lt: 5  (1 9.2 %)
#  No de fau lt: 1 26  (9 7. 7%)
#  Def au lt: 3  (2 .3%)
Uni degree =
y es
Uni degree =
no
#  No de fau lt: 1  (7 .7 % )
#  Def au lt: 1 2 ( 92 .3%)
Representation   + Evaluation + Optimization
How good is our current model?
Uncertainty according to target 
variable
How to improve our current model?
Recursively split data
until stop criterion is met (e.g., no 
more reduction in uncertainty) 26. Juni 2025
(Dis)advantages of decision / regression treesCore advantages of decision trees include:1.Handling numerical and categorical variables: Easy to implement and do not require a lot of preprocessing2.Non-parametric: No assumptions about distribution of variables and the classifier structure, allowing to non-linearities3.Interpretability: Easy to understand and to interpret so that one can detect possible inconsistenciesCore disadvantages of decision trees include:1.Overfitting:Can quickly lead to overfitting when tree is too complex, i.e., when it has too many nodes, levels, or leaves2.Instability:Decision trees may be remarkably unstable conditional on variation in training data, leading to different trees3.Optimization difficulty:Algorithms to build tree are typically heuristics that cannot guarantee global optimality
30 26. Juni 2025
Decision tree classifier: terminology
6
Decision trees belong to the hierarchical methods of classification, classifying data through recursive partitioningA decision tree consists of nodes (data) and edges (splits)•Topmost node: root•Connected posterior nodes: children•Connected prior nodes: parents•Node with parent and child: interior node•Node without child: leafAn interior node has at least two child nodesA tree where every non-leaf node has 2 children: binaryEach internal node tests one attribute 𝑋and splits data•Branch from internal node selects value of 𝑋•Prediction in leaf: 𝑝(𝑌!|𝑋!∈𝑙𝑒𝑎𝑓)
Leaf
Root
Interior
LeafInterior
LeafLeaf",9.869861126,1,"The generated answer is semantically identical to the ground truth, listing the same three core disadvantages.",1,"The generated answer is faithful. [cite_start]The context explicitly lists the 'Core disadvantages of decision trees' as 'Overfitting', 'Instability', and 'Optimization difficulty'[cite: 835].",1,"The context directly answers the question. A slide titled ""Last time: decision trees"" explicitly lists the ""Core disadvantages,"" which include ""Overfitting,"" ""Instability,"" and ""Optimization difficulty."""
"According to the document, what is the formula for Balanced Accuracy (Bal. Acc.)?",Bal. Acc. = (Sensitivity + Specificity) / 2,"According to the document, the formula for Balanced Accuracy (Bal. Acc.) is:

**Bal. Acc. = (Accuracy + Specificity) / 2** 

And also:

**Bal. Acc. = 2 * Sensitivity * Precision**","26. Juni 2025
Technical performance on unseen data (2/2)
Accuracy can be misleading, esp. in case of imbalanced samples, hence, we may also want to consider
Combining these measures results in other important measures:
𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚=𝑻𝑷𝑻𝑷+𝑭𝑵𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏=𝑻𝑷𝑻𝑷+𝑭𝑷
𝑩𝒂𝒍.	𝑨𝒄𝒄.=𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚𝟐 𝑭𝟏=𝟐∗𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚∗𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏
Tree A prediction$𝑦!	 True outcome(𝑦) Tree Bprediction$𝑦""	1 1 11 1 11 0 01 1 01 0 00 1 00 1 00 0 00 0 00 0 0
Confusion matrix
𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚=𝑻𝑵𝑻𝑵+𝑭𝑷
What is the Bal. Acc. And F1for the two models?
𝑦=1𝑦=0$𝑦=13 2$𝑦=02 3
𝑦=1𝑦=0$𝑦=12 0$𝑦=03 5
Tree A Tree B accuracy and establishing itself as a reliable tool (Chen,2022). It can collect and evaluate large
amounts of data that may exceed human analytical capacities, enabling AI to provide decision
recommendations (Shaw, 2019).
Modern technologies, including artiﬁcial intelligence solutions, have revolutionized work and
contributed to developing human resources management (HRM) for improved outcomes
(Hmoud and Laszlo,2019). One signiﬁcant area where their impact is felt is in the recruitment
process, where AI implementation can potentially provide a competitive advantage by enabling a
better understanding of talent compared to competitors, thereby enhancing the company’s
competitiveness (Johansson and Herranen,2019).
AI receives commands and data input through algorithms. While AI developers believe their
algorithmic procedures simplify hiring and mitigate bias, Miasato and Silva (2019) argue that
https://doi.org/10.1057/s41599-023-02079-x OPEN
1 College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China.✉email: njuczs@nuaa.edu.cn
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x 1
1234567890():,; 15. Mai 2025
Different types of solutions
10
Balanced Scorecard§Überarbeitung der Inhalte§Wiedereinführung der BSC zum kontinuierlichem Controlling
ISO-Zertifizierung§Interne Audits im HWS und FSS§ISO INTEGRA 1x1§Verpflichtende Besprechung von Prozessen auf Ressort-/ Stabstreffen
Point solutions•Definition:A solution that addresses a single, specific problem or need.•Scope:Narrow and focused.•Integration:Typically standalone; not integrated with other systems.•Use case: Quick wins, easy to implement, often first step in solving a broader challenge. •Example:Fraud detection model (Anomaly detection)Application solutions•Definition:A complete software or tool that solves a broader problem•Scope: Medium; focused on a domain or function•Integration: May integrate with other applications or systems.•Use case: Useful when a domain-specific tool is needed •Example: Customer support app (Classification + UI + reporting)",7.391802073,0,"The generated answer provides incorrect formulas for Balanced Accuracy, making it factually inconsistent with the ground truth.",0,"The generated answer is not faithful. The context provides two different formulas for Balanced Accuracy: `Bal. Acc. [cite_start]= (Sensitivity + Specificity) / 2` [cite: 845] [cite_start]and `F1 = 2 * Sensitivity * Precision / (Sensitivity + Precision)`[cite: 845]. The generated answer incorrectly combines these, stating `Bal. Acc. = (Accuracy + Specificity) / 2` and `Bal. Acc. = 2 * Sensitivity * Precision`. Neither of these is correct according to the context.",0,"The context provides an incorrect formula. It states Bal. Acc. = (Accuracy + Specificity) / 2, whereas the ground truth is (Sensitivity + Specificity) / 2. While a formula for Balanced Accuracy is given, it is not the correct one."
"According to the text, what is information gain?",Information gain is the improvement in an evaluation measure (such as the reduction in entropy) that is achieved by splitting the data at a parent node (level t) into its child nodes (level t+1).,"The text doesn't explicitly define ""information gain."" It mentions ""information gain: example"" in the context of choosing an attribute during a data analysis process (likely in a decision tree). It also uses the formula ""Gain+,-./*=0.92−56⋅0.72+16⋅0=0.32"" which relates to calculating information gain. 

Therefore, based on the context, information gain appears to be a metric used to evaluate the usefulness of an attribute in a data analysis process, likely within the context of building a decision tree.","26. Juni 2025
Information gain: example
12
We choose the attribute with the largest information gain, i.e., Income
ID AgeIncomeRepayment1 Old HighYes2 YoungHighYes3 Old HighYes4 Old HighNo5 YoungHighYes6 Old LowNo
Data at node t𝐺𝑎𝑖𝑛()*=0.92−46⋅1+26⋅0=0.25
𝐺𝑎𝑖𝑛+,-./*=0.92−56⋅0.72+16⋅0=0.32 integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",10.49705529,1,The generated answer and the ground truth are semantically identical. Both define information gain as the improvement in an evaluation measure achieved by splitting data at a node.,1,"The generated answer is faithful. [cite_start]While the context doesn't provide a textbook definition, it gives a clear example calculation (`Gain = 0.92 - 5/6 * 0.72 + 1/6 * 0 = 0.32` [cite: 857][cite_start]) and states that the attribute with the largest information gain is chosen[cite: 857]. This context is sufficient to verify that information gain is an evaluation measure used for splitting nodes.",0.8,"The context does not use the term 'information gain' directly in a definition. However, it provides a clear example of its calculation and use: 'We choose the attribute with the largest information gain, i.e., Income,' and shows the formula `Gain = ...`. This allows a user to infer the meaning and purpose of information gain as a metric for selecting the best attribute to split on, which is the core of the concept."
What is KNN used for?,KNN is a supervised learning algorithm used for classification and regression.,The provided text doesn't contain information about what KNN (K-Nearest Neighbors) is used for. It discusses general concepts of digital nudging and large language models (GPT).,"Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 2017 ). They are minimally invasive interventions that 
maintain freedom of choice (Thaler and Sunstein 2003 ) 
and can enhance economic efficiency by facilitating 
superior choices with minimal expenditure.
In digital environments, “digital nudges” (Wein -
mann et al. 2016 ) commonly manifest as design features 
in the user interface. Online marketplaces deploy digi -
tal nudges to stimulate product purchases, with retai -
lers using a variety of persuasive strategies to bolster 
profits (Ghose et al. 2023 ). However, such nudges can 
undermine consumer benefits by exploiting cognitive 
weaknesses (“dark patterns”) (Narayanan et al. 2020 , 
Posner et al. 2023 ) (e.g., by inducing fear of missing out 
through signals of scarce product availability (Hu et al. 
2019 )). Our work relates to prior research on digital 
nudging in the customer journey (Sch ¨ar and 
Stanoevska-Slabeva 2019 ) to encourage “greener shop -
ping behaviors” (i.e., more environmentally conscious 
shopping, fostering greater awareness and consider -
ation for the ecological impacts of product returns).
Recently, there has been growing interest in “green 
nudging,” which aims to promote environmentally 
friendly behaviors (White et al. 2019 ). Green nudging in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben.",5.557833672,0,"The generated answer is incorrect. It claims the text does not contain information about KNN, while the ground truth provides a correct definition of KNN's function.",1,The generated answer is faithful because it correctly states that the provided text does not contain information about KNN. The context discusses digital nudging and General Purpose Technologies but does not mention the KNN algorithm.,0,The context is irrelevant. It discusses general concepts of digital nudging and the application of machine learning in business but does not mention the K-Nearest Neighbors (KNN) algorithm at all.
"According to the document, what are the three components of the training process of supervised machine learning?",Representation +Evaluation +Optimization,"According to the document, the three components of the training process of supervised machine learning are:

*   Representation
*   Evaluation
*   Optimization","12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion•In very general terms, we can distinguish three components of three training process•We will first talk this through using simple linear regression•Same logic will be applied to more complex models
16
Representation +Evaluation +Optimization 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
17
Representation+Evaluation +Optimization
The model architecture that stores the learned knowledge.Now: linear regressionthat comprises learnable the parameters called coefficients.
'𝑦=)𝑓𝒙=+𝛽!++𝛽""𝑥""+⋯++𝛽#𝑥$ Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung.",5.115668774,1,"The generated answer and the ground truth are semantically identical, both identifying the three components as Representation, Evaluation, and Optimization.",1,The generated answer is faithful. [cite_start]The context includes a slide that explicitly breaks down the supervised machine learning process into three components: 'Representation + Evaluation + Optimization'[cite: 887].,1,"The context directly answers the question by listing the three components. A slide titled ""Components of supervised ML"" shows a diagram with ""Representation + Evaluation + Optimization"" as the core components of the training process."
"In the case study with the Sparkasse, is the combination of a human and a machine advisor better or worse than the machine alone?",Worse.,"According to the document, the combination of a human and a machine advisor is perceived as “viel besser und viel vertrauenswürdiger” (much better and much more trustworthy). The human helps to avoid excessive risk aversion.","Ende.
So suggerieren unsere 
Ergebnisse,
wenn Mensch plus Maschine da 
sind.
Nicht weil es denkt es besser, 
sondern einfach,
weil da jemand da ist, der ihnen 
die Hand hält.
Ähm, ja.
Takeaways, das habe ich glaube 
ich,
schon so zusammengefasst.
Es geht vor allem um dieses 
Handholding.
Maschine plus Mensch ist
zumindest nicht schädlich in der 
Beratung,
was die Qualität der Beratung 
angeht.
Aber der Mensch, der Kunde von 
Ihnen,
wird es Ihnen danken
und wird Ihre Empfehlung mehr 
annehmen.
Das ist auch das Wort zum 
Sonntag.
Ich hoffe, Sie konnten sich ein 
bisschen was mitnehmen.
Wieder. Und wir sehen uns 
nächste
Woche mit einem kurzen Quiz
und dann dem Einstieg in
die technische Seite. Vielen 
Dank. technische Performance der 
Maschine,
die für diese beiden Subgruppen 
vergleichbar sind.
Und grundsätzlich ist dieses 
Fehlverhalten von Maschinen,
wenn systematisch basierend auf
bestimmten soziodemographischen 
Faktoren,
die wir rechtlich oder
als Gesellschaft für wichtig 
erachten.
Wenn dort Unterschiede 
vorliegen,
die in der Realität aber 
eigentlich keine Rolle spielen,
also ohne Rechtfertigung,
unterscheidet die Maschine in
ihrer Vorhersagegenauigkeit,
in welchem Maß auch immer wir 
das jetzt messen,
zwischen diesen beiden Gruppen,
dann sprechen wir von 
algorithmischer Diskriminierung.
Algorithmische Diskriminierungen 
gibt es,
ein ganz tolles Papier hier von 
zwei
Kollegen werde ich Ihnen auch 
noch hochladen.
Und diese algorithmische 
Diskriminierung
hat ein ganz großes Problem.
Nämlich wir haben nicht, das es
vorher nicht auch schon hätte 
sein können,
dass Ihr Bankberater in dieser
einen Sparkasse
gegen bestimmte Minoritäten 
diskriminiert hat.
Aber diese Maschinen erzeugen 
oder erzeugen dieses
Muster auf eine Art und Weise,
auf eine skalierbare Art und 
Weise,
wie sie eine einzelne Person nie 
hätte darstellen können.
Und das hat auf der einen Seite 
eben technische Aspekte,
aber auch soziale Aspekte. Kunde ist.
Sie haben die Beratung,
die gleiche identische Beratung 
von dem Mensch
plus KI erhalten und
Ihr alternatives Ich von der 
reinen KI,
dem Robo Advisor das erhalten.
Dann haben Sie in dem Mensch 
plus Maschine Fall.
Haben Sie der Maschine 
signifikant,
haben Sie dem der Beratung 
signifikant häufiger.
Das sind zwei verschiedene 
Sachen
der Beratung des 
Risikoeinschätzung
und die eigentliche 
Investitionsempfehlung
haben Sie dem signifikant 
häufiger gefolgt.
Jetzt können Sie fragen Ist es 
gut oder schlecht,
dass Sie sagen Na ja,
wenn mensch dabei ist, finde 
ich, dass alles
viel besser und viel 
vertrauenswürdiger.
Nachricht ist gut, denn der 
Mensch hilft Ihnen,
exzessive Risikoaversion zu 
umgehen.
Und es scheint so ein bisschen,
wie der Kommilitone vorhin 
gesagt hat zu sein,
dass wenn mensch dabei ist und 
sie 100 € verlieren,
dann haben Sie wenigstens 
jemanden,
auf den sie sauer sein können.
Wenn Sie der Maschine gefolgt 
sind,
hilft Ihnen das nicht ganz so 
viel.
Das ist tatsächlich so ein 
bisschen intuitiv.
Auch die der Mechanismus,
auf den alles hindeutet.
Wir haben uns angeschaut. Glaubt 
der Mensch einfach.
Das glaubt der Kunde, dass 
Maschine
plus Mensch besser berät.
Also dass es genauer ist,",7.941281796,0,"The generated answer ('Worse') directly contradicts the ground truth, which states that the combination of human and machine is perceived as much better and more trustworthy.",0,The generated answer is not faithful. It claims the combination of human and machine is 'worse'. [cite_start]The context states the exact opposite: customers find the combination 'viel besser und viel vertrauenswÃ¼rdiger' (much better and much more trustworthy) [cite: 911] [cite_start]and that the human helps to 'umgehen' (avoid) excessive risk aversion[cite: 912].,0.7,"The context strongly implies that the combination is better. It states that with a human plus the machine, the advice is followed ""signifikant hÃ¤ufiger"" (significantly more often) and that customers find it ""viel besser und viel vertrauenswÃ¼rdiger"" (much better and much more trustworthy). It doesn't use the word 'worse' at all, but instead describes positive outcomes from the human-machine team."
"What is the concept of 'Mixed Initiative' in human-AI interaction, as exemplified by a smartwatch asking if it should record a workout?","'Mixed Initiative' is a middle ground between full automation and full manual control, where the system proactively suggests an action, but the human still makes the final choice to accept or reject it.","Based on the provided text, the concept of “Mixed Initiative” in human-AI interaction is being explored through the Sparkasse case study. Here’s a breakdown of what it means in this context:

**It’s about shifting control and decision-making between the human (the customer) and the AI system.**

Here's how it’s illustrated by the smartwatch example:

*   **Initial AI Initiative:** The smartwatch *initially* proposes an action – “Should I record your workout?” This is the AI taking the first step.
*   **Human Response & Potential for Human Initiative:** The human has the *option* to respond, either accepting or rejecting the suggestion. If the human says “No, don’t record,” then the AI’s initiative is paused. The human has now taken control.

**The Sparkasse study is investigating whether customers prefer a fully automated (AI-only) approach or a collaborative “Human AI Team” approach**, where the human and AI work together. The researchers are observing how customers react to the AI’s initial suggestions and whether they actively engage to shape the interaction.

**In essence, ""Mixed Initiative"" isn't about one side always dominating, but about a dynamic back-and-forth between human and AI, with the human retaining the ultimate control.**

Do you want me to elaborate on any specific aspect of this concept, like the motivations behind exploring this approach, or the potential outcomes of different interaction styles?","So legen wir los. Herzlich 
willkommen.
Vorlesung fünf sozusagen.
Fast schon Bergfest, oder?
Heute ist, glaube ich, Bergfest,
wenn man die restlichen 
Vorlesungen zusammenzählt.
Auch weil ich jetzt schon 
mehrfach gefragt wurde.
Dazu mit der Klausur.
Das hier ist jetzt die letzte
Vorlesung aus diesem ersten 
Teil,
wo es darum geht, ein 
grundsätzliches
Verständnis dafür zu schaffen,
was KI bewirken kann,
wie Menschen mit KI 
zusammenarbeiten können,
was potenzielle Schwachpunkte 
und Probleme sind,
wenn wir KI einsetzen.
Heute sprechen wir über Mensch 
Maschine Interaktion,
bauen sozusagen auf der 
Erklärbarkeit.
Vorlesungen von vor zwei Wochen
auf und dann ist dieser erste 
Block abgeschlossen.
Dann fängt es nächste Woche
an mit dem eher technischen 
Blog,
wo wir jetzt wirklich uns dann 
damit befassen werden,
wie wir diese Systeme bauen,
was die Mathematik dahinter ist,
wo die Stärken und die
Schwächen von bestimmten 
Modellen liegen.
Und zu Beginn der nächsten
Vorlesung werden wir dafür dann 
aber zum Abschluss dieses
ersten Blockes so eine Art 
kleinen Recap machen,
wo ich Ihnen auch
ein paar Fragen interaktiv
mit der Gruppe durchspielen 
werde.
Von der Struktur der Frage 
können Sie dann auf die
Struktur der Fragen in was man denken sollte.
Hier zum Beispiel von Microsoft 
Guidelines
für Human Centered AI,
also Guidelines an, was sie in 
welchen
Process Steps immer wieder 
denken sollten.
Ich werde das jetzt nicht hier 
alles vorlesen.
Die Idee ist wirklich, dass sie 
schon in
der bevor überhaupt mit dem 
System interagiert wird.
Bestimmte Fragen immer stellen,
immer wieder sich klar machen 
was kann das System
und nicht das Ganze als einmal 
sehen,
sondern über die Zeit dynamisch 
immer wieder schauen.
Interagieren Menschen 
ausreichend damit?
Muss das System irgendwo 
abgewandelt werden.
Immer den Menschen in den 
Mittelpunkt zu stellen.
Irgendwelche Fragen bis hierher?
Weil die letzten Minuten und 
dadurch,
dass ich mich länger bei dem 
Output aufgehalten habe,
als ich gedacht habe,
wird das jetzt in fünf Minuten 
abgehandelt,
habe ich ihn noch mal wieder 
eine Case Study
mitgebracht von einem Papier von 
Olli Hinz,
mir und zwei Kollegen aus Paris 
von der Chaussee,
wo wir uns hier mit der 
Sparkasse tatsächlich
im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob
das Ganze vielleicht 
automatisiert ablaufen sollte.
Das Papier ist jetzt
also kommt bald im Management 
Science Journal raus.
Wie gesagt, die letzten fünf 
Minuten.
Aber vorher noch mal 
irgendwelche
Unklarheiten zu dem Thema,
was wir oder Themen,
die wir bisher besprochen haben.
Cool. Wie gesagt, wir haben
uns in diesem Projekt
Sparkasse kam auf uns zu und hat 
gesagt naja,
wir haben hier eine Idee
oder wir möchten gerne mal 
rausfinden,
ob unsere Kunden eigentlich eher 
wollen,
dass sie von einem Robo Advisor 
beraten werden,
also einem automatisierten 
System,
wo kein Mensch mehr dabei ist 
oder von Mensch Berater,
der mit der KI zusammenarbeitet
und gemeinsam sozusagen als
Human AI Team diese Beratung an 
den Kunden gibt.
Der Kunde ist eben darüber 
informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,",24.41495109,1,"The generated answer captures the core meaning of the ground truth. Both describe 'Mixed Initiative' as a middle ground where the AI system suggests an action, but the human retains the final choice.",0.5,"The generated answer is partially faithful. It correctly identifies the core concept of Mixed Initiative as a dynamic interplay between human and AI, with the human retaining ultimate control. However, it incorrectly attributes this understanding to a 'Sparkasse case study' that is merely used as an example in the broader lecture on the topic. The explanation itself is a synthesis and not a direct summary of a specific study's findings as presented in the context.",0.8,"The context provides a perfect example of Mixed Initiative with the smartwatch scenario: the system proactively suggests an action ('Soll ich das eigentlich aufzeichnen?'), but the human retains the final decision-making power. It explains this as a middle ground where the user has not delegated the decision, but also did not have to actively initiate the action. While the context doesn't use the exact phrasing of the ground truth answer, it describes the concept perfectly."
What did the speaker say about the possibility of the smartwatch automatically starting to record a workout?,The speaker said the smartwatch could estimate the probability that the user wants their sport recorded.,"The speaker discussed four extreme cases where a smartwatch might automatically start recording a workout. In one case, the smartwatch might ask, “Should I record this?” when you’re exercising. However, the core idea is that the smartwatch *doesn’t* automatically initiate recording without your consent. The speaker emphasizes the concept of ""Not Action,"" where the smartwatch doesn't take the initiative to record without you explicitly saying yes.","Wenn Sie entscheiden würden, 
nicht zu delegieren,
dass Sie auf jeden Fall das 
selber machen, hilft es ja.
Ja, das ging jetzt gerade,
was sie gesagt hatten.
Naja, muss es denn immer das 
eine oder das andere sein?
Nee, muss es natürlich nicht.
Denken Sie an diejenigen im 
Raum,
die irgendeine Smartwatch haben.
Wenn Sie Sport machen und 
plötzlich
das System Sie fragt Soll ich 
das eigentlich aufzeichnen?
Sind Sie gerade am Sport machen?
Das ist irgendwie so eine Art 
Mischung davon.
Also weder das System selber 
entschieden,
jetzt irgendwas aufzuzeichnen 
ohne Ihre Einwilligung.
Sie haben also diese 
Entscheidung
des automatisierten 
Aufzeichnungs nicht
an die Maschine delegiert.
Aber Sie haben auch nicht aktiv 
gesagt Na ja,
ich gehe jetzt hin und such das 
raus.
Sondern die Maschine von sich 
entscheidet,
ihnen etwas vorzuschlagen in the 
first place.
Und das nennen wir grundsätzlich
The Idea of Mixed Initiative.
Und es geht auch schon auf ein 
sehr frühes
wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.
Und die Idee, die hier in diesem 
Paper auf einer
Informatik Konferenz postuliert 
wird, ist.
Naja, lassen Sie uns mal 
annehmen,
das System könnte die 
Wahrscheinlichkeit einschätzen,
dass Sie ein bestimmtes Goal 
haben.
Die Maschine hat irgendeine 
Möglichkeit vorherzusagen,
was die Wahrscheinlichkeit,
dass Sie gerne Ihren Sport 
aufgezeichnet hätten.
Und jetzt haben wir hier noch 
gegebene Evidence.
N Lassen wir mal weg.
Intuition dahinter ist wenn die 
uhr Ihren Puls misst,
kann die uhr eine Vorhersage 
darüber machen.
Was ist denn eigentlich egal?
Möchten Sie es aufgezeichnet 
haben?
Gegeben Ihren Puls 
beispielsweise
so und das haben die in ein 
Modell gepackt,
was eigentlich sehr simpel ist 
dieses Modell,
wo gesagt wird Naja,
wir haben hier unten unsere 
Wahrscheinlichkeit,
dass ein Nutzer ein bestimmtes 
Ziel verfolgt.
Und entweder haben wir die 
Wahrscheinlichkeit,
dass dieses eine Goal eben nicht 
verfolgt wird.
Was heißt die Wahrscheinlichkeit 
ist
null Wahrscheinlichkeit Null oder die uhr macht einfach 
nichts in Aktion.
In diesen Extremfällen Jetzt 
gehen wir gleich
wieder in die Mitte davon.
Aber Sie laufen und Sie möchten,
dass Laufen aufgezeichnet haben 
Sie laufen, möchten,
dass nicht aufgezeichnet haben 
und
die Maschine kann selber die 
Aktion machen,
das aufzuzeichnen oder die uhr 
macht das eben nicht.
Und dann haben wir vier 
verschiedene Fälle und
das versuchen wir wieder über 
Nutzen darzustellen.
Ganz, ganz allgemein.
Der Nutzen, den ich davon habe,
wenn die uhr die Aktion 
durchführt
und die Messung beginnt, 
gegeben.
Ich möchte auch, dass diese 
Messung gemacht wird.
Wenn ich Sport mache versus die
uhr macht das einfach
und eigentlich will ich das gar 
nicht.
Umgekehrt hier und das ist wer 
kennt dieses
Symbol? Was bedeutet das?
Genau. Das ist aus der 
Mathematik der Logik.
Wenn Sie dieses seitlich 
gedrehte Liter sehen,
heißt das immer Nicht immer.
Nicht a, nicht Action.
Sie können eigentlich hier,
um das Beispiel zu 
vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.",10.02209997,1,The generated answer and the ground truth are semantically consistent. The generation's statement that the smartwatch could 'estimate the probability' aligns with the core idea that it doesn't automatically record but intelligently prompts the user.,1,"The generated answer is faithful. It accurately reflects the claims made in the context. [cite_start]The context mentions that the smartwatch asks, 'Should I record this?'[cite: 948], implying it doesn't start automatically. [cite_start]It also explicitly mentions the concept of 'Not Action,' where the watch doesn't record even if the user wants it to[cite: 955, 973].",1,The context provides a clear example of what the speaker is talking about. The speaker discusses a smartwatch that can estimate the probability that a user wants their sport recorded based on their pulse. This directly supports the idea that the smartwatch could make an informed decision about whether to start recording a workout.
The lecture describes two extremes in human-machine collaboration. What is 'Algorithm Aversion'?,"'Algorithm Aversion' is the observation that people often consciously choose not to use a helpful algorithm, even when they know it is objectively more accurate than their own opinion or that of other humans.","The provided text does not contain information about “Algorithm Aversion.” It focuses on topics like mitigating bias in AI, fair representations, and measuring discrimination in algorithmic decision-making.","Mitigating unwanted biases with adversarial 
learning. Proceedings of the 2018 AAAI/ACM 
Conference on AI, Ethics, and Society (pp. 335-
340).  
Zhao, H., & Gordon, G. (2019). Inherent tradeoffs in 
learning fair representations. Proceedings of the 
33rd Conference on Neural Information 
Processing Systems.  
Žliobaitė, I. (2017). Measuring discrimination in 
algorithmic decision making. Data Mining and 
Knowledge Discovery, 31(4), 1060-1089. acteristics, such as race and gender (Jackson, 2021). When
assessments consistently overestimate or underestimate a parti-
cular group’s scores, they produce “predictive bias” (Raghavan
et al.,2020
). Unfortunately, these discriminatory results are often
overlooked or disregarded due to the misconception that AI
processes are inherently“objective” and “neutral.”
REVIEW ARTICLE HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | https://doi.org/10.1057/s41599-023-02079-x
2 HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x Policy, 8(1), 78-115.  
Wong, K. F. E., & Kwong, J. Y. (2018). Resolving the 
judgment and decision -making paradox 
between adaptive learning and escalation of 
commitment. Management Science , 64(4), 
1911-1925.  
Wong, P. H. (2020). Democratizing algorithmic 
fairness. Philosophy & Technology, 33(2), 225-
244.  
Woodworth, B., Gunasekar, S., Ohannessian, M. I., & 
Srebro, N. (2017). Learning non-discriminatory 
predictors. Proceedings of the Conference on 
Learning Theory (pp. 1920-1953).  
Yao, S., & Huang, B. (2017). Beyond parity: Fairness 
objectives for collaborative filtering. 
Proceedings of the 31st Conference on Neural 
Information Processing Systems.  
Zafar, M. B., Valera, I., Rogriguez, M. G., & 
Gummadi, K. P. (2017). Fairness constraints: 
Mechanisms for fair classification. Proceedings 
of the  20th International Conference on 
Artificial Intelligence and Statistics  (pp. 962-
970).  
Zhang, B. H., Lemoine, B., & Mitchell, M. (2018). 
Mitigating unwanted biases with adversarial 
learning. Proceedings of the 2018 AAAI/ACM 
Conference on AI, Ethics, and Society (pp. 335-
340).  
Zhao, H., & Gordon, G. (2019). Inherent tradeoffs in 
learning fair representations. Proceedings of the",5.954960823,0,"The generated answer is incorrect. It claims the text does not contain information on 'Algorithm Aversion,' while the ground truth provides a clear and correct definition of the term.",1,"The generated answer is faithful. It correctly states that the provided text does not contain information about 'Algorithm Aversion.' The context consists of reference lists and text snippets about fairness and mitigating bias, with no mention of this specific term.",0,"The provided context is irrelevant to the question. It is a list of references and snippets of text discussing fairness, mitigating bias, and discrimination in algorithmic decision-making, but it does not mention or define 'Algorithm Aversion'."
What are the two extremes of observation regarding human-machine collaboration?,Algorithm aversion and Algorithm appreciation.,"Based on the text, the two extremes of observation regarding human-machine collaboration are:

*   **Algorithm Aversion:** Preference for human help/decisions over AI ones.
*   **Algorithm Appreciation:** Preference for AI help/decisions over human ones.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT Die System eins Muster.
Eine sehr schöne Interpretation
dieser grundlegenden Idee,
dass wir unterschiedliche
Lösungsstrategien gesucht und 
gefunden haben.
Durch unser Lernen, durch unsere 
Erfahrungen,
die wir im Leben individuell 
gemacht haben.
Beispiele von Mensch Maschine 
Interaktion im Alltag.
Das hier ist insbesondere
für Softwareentwicklung 
natürlich hochgradig 
interessant.
Ich fange an, Code zu schreiben.
Wenn Sie mit Charlotte,
die in der Übung die
Notebook Files schreiben und das 
mit
einem Pro Account auf Google 
Collab machen.
Dann fangen sie an, was zu 
schreiben und plötzlich
stehen die nächsten 20 Zeilen 
Code schon da,
weil die Maschine antizipiert,
was sie eigentlich schreiben 
wollen und
Sie klicken nur noch Enter und
der ganze Code ist plötzlich 
schon da.
Diese Suggestions Johns 
eigentlich.
Wenn wir uns hier wieder
auf diese zwei verschiedenen 
sehr
dichotomen Möglichkeiten der
Zusammenarbeit konzentrieren,
haben wir über diese
Code Recommendations
eigentlich 
Entscheidungsunterstützung,
wo das System uns sagt Nein,
das ist eine Möglichkeit,
wie man diesen Code weiterführt.
Auf der anderen Seite haben
wir unsere ganzen großen 
Sprachmodelle,
mit denen wir tagtäglich 
zusammenarbeiten,",6.830907583,1,The generated answer is semantically identical to the ground truth. Both identify algorithm aversion and algorithm appreciation as the two extremes.,1,The generated answer is faithful. [cite_start]The context explicitly defines 'Algorithm aversion' as the 'Preference for human help/decisions over AI ones' and 'Algorithm appreciation' as the 'Preference for AI help/decisions over human ones'[cite: 996].,1,The context directly answers the question. A slide presents a list of factors that shape human-AI collaboration and explicitly defines both 'Algorithm aversion: Preference for human help/decisions over AI ones' and 'Algorithm appreciation: Preference for AI help/decisions over human ones'.
What were the developers struggling with regarding Windows eight?,The developers of Windows 8 were struggling with the bad user interface.,"According to the text, the developers were struggling with the user interface of Windows 8. It was considered a ""totale Katastrophe"" (total disaster) and users found it overwhelming.","5. Juni 2025
Acceptance and appropriate use of technology
2012, Microsoft released Windows 8 that significantly changed the user interface•Many of its features are standard nowadays•Yet, considered one of Microsoft’s worst OS•In 2021 its market share was about 1% while its predecessor Windows 7 was still at 15%
4
Remember Windows 8?
What does that imply for AI? the loop dahingehend Sie 
entscheiden,
Ich transferiere 
Entscheidungsgewalt in diesen
automatisierten Fällen oder Sie 
behalten immer
die Entscheidungsgewalt,
müssen sich aber entscheiden Wie
viel Gewicht geben Sie 
eigentlich?
Der Information von der Maschine 
macht es Sinn.
So fangen wir aber
mal einen Schritt weiter vorne 
an Erinnern Sie sich noch?
Ich weiß nicht, ob Sie dafür 
noch zu jung sind, Sogar.
Aber erinnern Sie sich noch
an diese Version von Microsoft?
Microsoft acht kam raus
2012 und es war ein vollkommenes 
Desaster für Microsoft.
Warum? Naja, viele der User 
waren total überfordert.
Das war eigentlich grundsätzlich 
ein
tolles System mit ganz vielen 
Neuerungen,
mit ganz vielen Funktionen,
wo sich die Entwickler gedacht 
haben Naja,
das sollte meinen Nutzern 
gefallen.
Das macht alles viel einfacher.
Da soll der Entscheidungsfindung 
Prozessen
arbeiten helfen.
War alles natürlich well 
intended.
Problem war, wir alle fanden das 
nicht so cool.
Das war bei mir auch der 
Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen, Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen,
wohingegen Windows acht, was
viele neue gute Funktionen 
hatte,
immer noch nur um die 1 % war.
Und es ist wieder considered 
eines der
schlechtesten Operating Systeme, 
die es gibt.
weil es einfach niemand benutzt 
hat.
Jetzt natürlich die Frage Wenn 
ich Ihnen hier
so eine anekdotische Geschichte 
erzähle,
um Sie so ein bisschen emotional 
abzuholen,
was glauben Sie, hat das mit KI 
zu tun?
Wie könnte ich aus dem 
vorherigen Beispiel,
dass die KI mir ein Signal gibt?
Ich sehe das und es
ist eigentlich ein hilfreiches 
Signal.
Wie kann ich diese Idee hier auf 
diesen
anekdotischen Fall aus der 
Geschichte
mappen? Wo ist die Verbindung?
Genau hier.
Wir haben ein tolles System. Es 
kann uns helfen.
Viele der Features im
Übrigen sind heute in einer 
anderen Darstellung,
in einer anderen Art und Weise 
in
den Microsoftsystemen immer noch 
da.
Die wurden dort entwickelt für 
Windows acht,
aber die Oberfläche war totale 
Katastrophe.
User Interface Wir hatten 
Entwickler,
die sich ausschließlich damit",6.058125973,1,The generated answer and the ground truth are semantically identical. Both state that the developers of Windows 8 struggled with a bad user interface.,1,The generated answer is faithful. [cite_start]The context explicitly states that the user interface for Windows 8 was a 'totale Katastrophe' (total disaster) [cite: 1020] [cite_start]and that many users were 'total Ã¼berfordert' (totally overwhelmed)[cite: 1008].,1,"The context directly answers the question. The speaker describes Windows 8 as a ""vollkommenes Desaster fÃ¼r Microsoft"" because many users were ""total Ã¼berfordert"" and the ""OberflÃ¤che war totale Katastrophe"" (user interface was a total disaster)."
"In a case study about the role of the human, what was the investment amount for the P2P vehicle in the experiment?",100,"The text doesn't specify the exact investment amount for the P2P vehicle in the experiment. It only mentions that investors were considering whether to invest with a particular borrower with a ""festgeschriebenen Zins"" (fixed interest rate) in P2P loans, such as private loans.","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,
also Privatkredite, finanziert 
werden sollten,
also ob ein Kunde 100 € 
beispielsweise in die in
jemand anderen als Privatkredit
investieren möchte mit einem 
festgeschriebenen Zins.
Und dieses KI System war darauf 
ausgelegt vorherzusagen,
ob dieser Privatkredit dann
tatsächlich zurückgezahlt wird 
oder nicht,
ob ein Ausfall dieses Kredits 
auftreten wird oder nicht.
Das System war eigentlich auch 
ziemlich gut.
Es hat irgendwie so 74 % 
Accuracy gehabt.
Was für so soziales
Verhalten oder menschliches 
Verhalten vorherzusagen,
schon immer ganz gut ist.
Dann sind wir in die Sparkasse 
rein und haben mit
27 Bankern haben
wir hingesetzt und gesagt Hier 
sind
verschiedene von diesen 
Privatkrediten.
Gebt doch mal bitte deine eigene 
unabhängige Meinung an,
ob dieser Kredit und ich gebe 
dir ein
paar Informationen zu dem 
Privatkredit,
ob der zurückgezahlt wird oder 
nicht.
Würdest du das deinem Kunden?
Du als Mensch ohne KI Hilfe?
Würdest du dem Kunden empfehlen investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records 
from all previous iterations and newly collected 
instances of borrowers who received an investment and 
made a repayment decision. 8  The next iteration then 
started, where we again randomly sample d—with 
replacement—and paired 50 borrowers and investors. 
Investors and their decisions : At the outset of each 
simulation prior to the first iteration, we randomly split 
Study B data into two equally sized shares. We used the 
first share (comprising 304 unique individuals with 20 
decisions each, yielding 6,080 discrete observations) to 
train an ML model that simulates whether an investor 
would choose to invest with a particular borrower, using 
31 features (20 personal characteristics of the investor, 
10 characteristics of the borrower, and a repayment 
forecast for the borrower). The employed model is a 
gradient boosted forest, executed using the XGBoost 
library in Python (Chen and Guestrin, 2016). We 
employed an automated process to optimize seven",8.108078718,1,"The generated answer and the ground truth are semantically identical, both stating the investment amount was 100 â‚¬.",1,"The generated answer is faithful. [cite_start]The context states that in the experiment, they considered whether a customer would want to invest '100 â‚¬ beispielsweise' (100 â‚¬, for example) in a peer-to-peer loan[cite: 1030].",0.8,"The context is highly relevant and provides strong clues to the answer. It mentions a case study involving P2P loans and asks if a customer should invest ""100 â‚¬ beispielsweise"" (100 â‚¬, for example) in a private loan. While it uses the word ""beispielsweise"" (for example), it is the only monetary amount mentioned in the context of the investment, making it a direct and reasonable answer."
"According to the document, what did Eric Horvitz advocate in his 1999 paper?",Elegant coupling of automation with direct manipulation by user.,"The document doesn't mention a paper by Eric Horvitz, nor does it detail what he advocated in a 1999 paper. It does, however, describe a conversation about how to approach systems of automation and delegation, suggesting a framework where humans retain control over decisions rather than simply automating everything.","findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. 
Our results have important practical implications, especially considering the impending regulations 
mandating human involvement in ML-supported decision-making processes. 
Keywords: Continuous Updating, Algorithmic Discrimination, Human Discrimination, Feedback 
Loops, Investment Game 
John Qi Dong  was the accepting senior editor. This research article was submitted on  June 22, 2022 and underwent 
three revisions.  
1 Introduction 
With the aim of bolstering economic efficiency and 
societal welfare (Benbya et al., 2020; Teodurescu et al., 
2021), machine learning (ML) systems are augmenting 
human decision -making across diverse domains, 
including bail decisions (Kleinberg et al., 2018), hiring 
processes and student admissions (Horton, 2017; Zhao 
et al., 2020), banking (Fu et al., 2021), and medicine 
(Jussupow et al., 2021). In Europe, Big Data analytics 
software revenue reached US$14.6 billion in 2018 
alone. Wells and Spinoni’s projections (2019) indicate 
an expected annual growth rate of about 8% for this wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen
letztlich verantwortlich ist.
Da. Der hat nämlich damals genau 
das gesagt,
diese ganze Diskussion zwischen 
entweder
automatisieren oder wir bleiben
im Decision Support.
Das sollten wir gar nicht so
als entweder oder betrachten,
sondern wir sollten es aus einer 
Perspektive betrachten,
dass wir vielleicht Systeme 
haben,
die uns Automatisiert von selbst 
bestimmte
Dinge abnehmen und dann autonom 
agieren.
Und für andere Dinge sollten Sie 
uns vielleicht fragen.
Das heißt, wir geben eigentlich 
die Entscheidung,
ob wir im Automation oder 
Delegation,
im Automation oder Augmentation 
Setting sind,
auch an die Maschine ab.
Und er hat dann sehr intuitives,
sehr intuitiven, konzeptionellen 
Vorschlag gemacht,
wie wir darüber nachdenken 
können.
Das ist jetzt auch wieder das 
Modelldenken,
was ich glaube sehr wichtig ist 
an Universitäten.
Gelernt werden sollte,
dass sie ein grundlegendes 
Schubladendenken haben,
wie sie über solche Probleme 
nachdenken können,
ohne dass wir jetzt keine They were therefore assured of their comprehension and consent.
Additional information
Supplementary informations The online version contains supplementary material
available at https://doi.org/10.1057/s41599-023-02079-x.
Correspondence and requests for materials should be addressed to Zhisheng Chen.
Reprints and permission informationis available athttp://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory",8.144184113,0,"The generated answer is incorrect. It claims the document doesn't mention the paper, while the ground truth provides the specific advocacy from that paper ('Elegant coupling of automation with direct manipulation by user').",1,The generated answer is faithful because it correctly states that the document does not mention a 1999 paper by Eric Horvitz. [cite_start]The context refers to a conceptual proposal from him from the early 2000s [cite: 1051] but provides no details that would support the claims in the user's prompt.,0,"The context does not mention a 1999 paper by Eric Horvitz. It discusses a conceptual proposal from him from the early 2000s about moving beyond the simple dichotomy of automation vs. decision support, but it does not mention 'elegant coupling' or direct manipulation."
What is augmentation in the context of human-AI collaboration?,AI serves as a decision support tool; human always makes final decision.,"According to the text, augmentation is where “AI serves as a decision support tool; human always makes final decision.” It’s a mode of collaboration where the AI provides advice, but the human retains control and makes the ultimate decision.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Goal: Human-AI complementarities
9
The objective of human-AI collaboration is achieving complementary performance•Humans plus AI outperform either party acting alone for a set of tasks•Complementary strengths can originate from•Access to different data (e.g., soft signals)•Learning of distinct problem-solving strategies (imperfectly correlated errors) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT",5.471441031,1,The generated answer and the ground truth are semantically identical. Both define augmentation as a scenario where the AI is a decision support tool and the human makes the final decision.,1,The generated answer is faithful. [cite_start]The context includes a slide that defines 'Augmentation' as a collaboration mode where the 'AI serves as a decision support tool; human always makes final decision'[cite: 1066].,1,"The context directly answers the question. A slide titled ""Two modes of human-AI collaboration"" defines augmentation as a collaboration mode where ""AI serves as a decision support tool; human always makes final decision."""
What is the mean of the prediction change when considering coalitions of features?,Δ=−0.1,The mean of the prediction change across all coalitions is −0.1.,"22. Mai 2025
Marginal contributions across coalitionsIdea:•Remove feature across all possible coalitions and compute changes in predictions•Compute a feature’s importance by marginalizing over all its contributionsSimplified example for “savings”:
21
Coalition of 3 features
Δ=−0.07
Coalitions of 2 features
Δ=−0.3
Δ=−0.02
Coalition of 1 feature
Δ=−0.01
Mean: Δ=−0.1
What is the prediction of no feature is present? with modern machine learning methods. This integra -
tion enables the estimation of individual-level differ -
ences between actual outcomes and hypothetical 
outcomes that would have occurred without the inter -
vention (Athey and Imbens 2015 ).
Contrasting with traditional machine learning’s 
focus on predicting outcomes from correlational data 
relationships, CML aims to forecast cause-and-effect 
relationships. This approach enhances the understand -
ing of how interventions affect outcomes of interest 
(Wager and Athey 2018 ). For example, although tradi -
tional machine learning may predict a customer’s likeli -
hood to purchase when exposed to a marketing 
intervention, CML methods determine how the inter -
vention itself alters the customer’s purchase likelihood 
compared with no exposure (i.e., they predict the 
causal effect attributable to the intervention).
Research on the application of CML in practice is still 
in its infancy. To date, only a few studies have applied 
these methods to analyze heterogeneous treatment 
effects. Nevertheless, initial applications are promising. 
For instance, Zhang and Luo ( 2023 ) employ causal for -
ests to explore the varied impact of social media posts availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating.",4.083069563,1,"The generated answer and the ground truth are semantically identical, both stating the mean of the prediction change is -0.1.",1,The generated answer is faithful. [cite_start]The context includes a slide that explicitly shows a simplified calculation of marginal contributions and states the result as 'Mean: Î” = -0.1'[cite: 1069].,1,"The context directly answers the question. A slide shows a simplified example of calculating marginal contributions for the feature ""savings"" across different coalitions, and explicitly states at the bottom, ""Mean: Î” = -0.1."""
"According to the document, what is interpretability?",The ability to explain or provide the meaning in understandable terms to humans.,"According to the document, interpretability is “the ability to explain or provide the meaning in understandable terms to humans.”","22. Mai 2025
Some definitionsConfusing nomenclature: explainable / interpretable / transparent models•Interpretability:the ability to explain or provide the meaning in understandable terms to humans•Explainability:explanation as an interface between humans and a decision maker that is both an accurate proxy of the decision maker and comprehensible to humans•Transparency: a model is transparent if by itself it is understandable.Explaining different things:•Predictionsand model behavior (overarching logic the model learned)•Decisions(predictions plus rule that translates into decision)Explanation modes:•Analytic statement:natural language descriptions of elements and context that support decision•Visualizations:highlight parts of data that support the decisions•Cases:give typical/illustrative examples that support the decision•Rejections or alternative choice:counterfactuals that argue against the alternative output
15 des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses betriebswirtschaftlich
ökonomische Entscheidungen zu 
treffen.
Wer sieht das nicht?
Noch mal, was ich letztes Mal 
gesagt habe Wenn Sie
was nicht verstehen, ist es 
nicht Ihre Schuld.
Dann habe ich es einfach 
schlecht erklärt.
Deswegen immer raus damit.
Dann gehe ich davon aus, dass 
jeder verstanden hat.
Aber eine andere Frage haben
wir ja theoretisch auch
noch andere 
Entscheidungsmöglichkeiten,
Parametern, die dieselbe 
Qualität liefern.
Vollkommen richtig. Da wäre 
jetzt die Frage,
ob es eine 
Entscheidungsmöglichkeit,
die zwei Parameter verwendet,
besser als eine auf einem 
Papier.
Das ist sehr, sehr gute Frage.
Da gibt es verschiedene Theorien 
zu.
Grundsätzlich, wie wir sehen 
werden,
für den technischen Teil der 
Vorlesung.
Wir können für das gleiche
Problem verschiedene Regeln 
identifizieren.
Verschiedene Machine Learning 
Modelle trainieren.
Diese Machine Learning Modelle 
können bessere oder
schlechtere Performance 
aufweisen.
Wenn sie aber genau die gleiche 
Performance aufweisen,
dann haben wir natürlich in 
einer gewissen Art und
Weise die Wahl, immer darauf 
achtend,
dass die einzige Performance, 
die wir sehen,
aktuell immer Performance auf 
historischen Daten ist.
Das heißt, wir müssen auf der",4.724214315,1,"The generated answer and the ground truth are semantically identical, both defining interpretability as the ability to explain something in understandable terms to a human.",1,The generated answer is faithful. [cite_start]The context includes a slide that defines interpretability as 'the ability to explain or provide the meaning in understandable terms to humans'[cite: 1082].,1,"The context directly answers the question. A slide titled ""Some definitions"" provides a bullet point that defines interpretability as ""the ability to explain or provide the meaning in understandable terms to humans."""
What does the EU AI Act Art 86 demand?,Right to Explanation of Individual Decision-Making.,The provided text doesn't detail the specific demands of EU AI Act Article 86. It only mentions that the European Commission has drafted proposals regarding the influence of algorithmic systems on human rights and established a high-level expert group on AI.,"ethics, and responsibility in AI (Zuiderveen Borgesius,2020). The
Organization for Economic Cooperation and Development
(OECD) has provided recommendations on AI, while the
European Commission has drafted proposals regarding
the inﬂuence of algorithmic systems on human rights. In 2019,
the European Commission established a high-level expert group
on AI, which proposed ethical guidelines and self-regulatory
measures regarding AI and ethics.
Public organizations have played a role in establishing
mechanisms to safeguard algorithmic fairness. The Algorithm
Justice League (AJL) has outlined vital behaviors companies
should follow in a signable agreement. Holding accountable those
who design and deploy algorithms improves existing algorithms
in practice (36KE,2020). After evaluating IBM’s algorithm, AJL
provided feedback, and IBM responded promptly, stating that
they would address the identi ﬁed issue. As a result, IBM
signiﬁcantly improved the accuracy of its algorithm in minority
facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human 22. Mai 2025
EU AI act
10 Retailing 97(4):676–696.
European Union (2016) Regulation EU 2016/679 of the European 
Parliament and of the council of 27 April 2016, article 22. Offi -
cial J. Eur. Union 119:59.
European Union (2021) Proposal for a regulation EU of the Euro -
pean Parliament and of the council of April 21, 2021, laying 
down harmonised rules on artificial intelligence (Artificial Intel -
ligence Act) and amending certain union legislative acts. Official 
J. Eur. Union 119.
Frisch R, Waugh FV (1933) Partial time regressions as compared 
with individual trends. Econometrica 1(4):387–401.
Ghose A, Lee HA, Nam K, Oh W (2023) The effects of pressure and 
self-assurance nudges on product purchases and returns in 
online retailing: Evidence from a randomized field experiment. 
J. Marketing Res. 61(3):517–535.
Goldstein NJ, Cialdini RB, Griskevicius V (2008) A room with a 
viewpoint: Using social norms to motivate environmental con -
servation in hotels. J. Consumer Res. 35(3):472–482.
Goldstein NJ, Griskevicius V, Cialdini RB (2007) Invoking social 
norms: A social psychology perspective on improving hotels’ 
linen-reuse programs. Cornell Hotel Restaurant Admin. Quart. 
48(2):145–150.
Grahl J, Hinz O, Rothlauf F, Abdel-Karim BM, Mihale-Wilson C",6.186999798,0,"The generated answer is incorrect. It claims the text doesn't detail the demands of Article 86, while the ground truth correctly states that it demands a 'Right to Explanation of Individual Decision-Making'.",1,"The generated answer is faithful as it correctly states that the context does not specify the demands of EU AI Act Article 86. The context mentions the European Commission's work on AI ethics and proposals [cite: 1100, 1101] but does not detail specific articles of the AI Act.",0,The provided context is entirely irrelevant. It discusses the GDPR and the role of public organizations in algorithmic fairness but makes no mention of the EU AI Act or Article 86.
What is a 'selective labels problem'?,It occurs when a machine learning model's prediction influences decisions that prevent the collection of data needed to evaluate the model's accuracy.,"According to the text, the “selective labeling problem” (also referred to as the “endogenous composition of future training data”) can “maintain or even exacerbate inaccurate or discriminatory predictions.” It’s a situation where machine learning models, by making predictions, influence the data they’re then trained on, potentially reinforcing bias.","endogenous composition of future training data, a 
problem referred to as selective labeling (Lakkaraju et 
al., 2017; Little & Rubin, 2019), can maintain or even 
exacerbate inaccurate or discriminatory predictions. 
In this paper, we explore the dynamic interplay 
between the continuous updating of ML models and 
algorithmic discrimination when ML predictions 
endogenously shape the composition of additional data 
available for model updating. In particular, our work 
focuses on the role of ongoing discrimination by 
human decision  makers who have the final say in an 
ML-supported decision-making task. A steady stream 
of previous work has examined how algorithmic 
discrimination can reproduce and perpetuate 
discriminatory patterns ingrained in societal structures 
(Lambrecht & Tucker, 2019; Morse et al., 202 2; 
Teodurescu et al., 2021). Some studies have developed 
theoretical arguments concerning the emergence of 
feedback loops when ML models continually learn 
from the discriminatory outcomes they help ed to 
generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider
social issues in context. It is crucial to make them aware of
attribute selection’s impact on the algorithm (Yarger et al.,2019).
The algorithm engineer plays a crucial role in the entire system,
from setting goals for machine learning to selecting the
appropriate model and determining data characteristics such as
labels. If inappropriate goals are set, bias may be introduced from
the outset (36KE,2020).
An engineer is responsible for developing the algorithmic model.
If they hold certain beliefs and preconceptions, those personal
biases can be transmitted to the machine (Njoto,2020). Although
the device is responsible for selecting employee resumes, it operates
based on underlying programming. The programmer guides the AI
in making decisions about the best candidate, which can still result
in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to case the borrower is male. A comparison of panels for Iterations 1 and 100 for the two measures of algorithmic discrimination provides insights 
into their conditional development across iterations. Abbreviations: Outcome under perf. inf. = outcome under perfect information; Init. lab. imb. 
= initial label imbalance; Human discr. = human discrimination. 
Figure A9. Development of the Mean Share of Game Outcomes by Initial Label Imbalance  
and Degree of Human Discrimination",6.921473742,1,"The generated answer and the ground truth are semantically identical. Both describe the selective labels problem as a situation where a model's prediction influences data collection, thereby hindering its own evaluation.",1,The generated answer is faithful. [cite_start]The context states that 'selective labeling... can maintain or even exacerbate inaccurate or discriminatory predictions'[cite: 1116]. This directly supports the idea that the model's predictions influence data collection in a way that can prevent proper evaluation.,1,"The context directly defines the concept. It states that the ""endogenous composition of future training data, a problem referred to as selective labeling... can maintain or even exacerbate inaccurate or discriminatory predictions."" This explains that the model's predictions influence the data available for future training, which is the core of the problem."
"According to the text, what is one way to control for gender when analyzing a dataset?","By running the dataset through without including gender, but then rerunning it while including gender to compare performance.","According to the text, one way to control for gender when analyzing a dataset is to remove the gender column entirely and only use features that aren’t related to gender. This was exemplified by the Amazon example, where they excluded the gender column from the dataset to assess performance differences based on gender.","data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately 
for women. Second, it can be created by unrepresentative, 
or imbalanced data (Barocas & Selbst, 2016; Mehrabi et 
al., 2021). When the dataset underrepresents certain 
groups, such as women, the ML model can make more 
errors when predicting for this group. Imbalances occur 
when labels skew significantly within specific subgroups. 
Third, technical constraints or malpractices during model 
development and training may yield discriminatory 
outputs (Friedman & Nissenbaum, 1996; Williams et al., 
2018). Biases may emerge during data integration due to 
inconsistent data formats or when the integration of 
personal information like ethnicity is mishandled (Schelter 
& Stoyanovich, 2020; Williams et al., 2018). It is critical 
to appreciate the sociotechnological nature of algorithmic 
discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and Datensatz,
in dem man alles angibt,
außer das Geschlecht und lässt 
das durchlaufen.
Aber hierbei würden sie Gender 
mit rein nehmen.
Also in einem ersten Schritt,
um das zu kontrollieren.
Genau in diesem Beispiel von dem 
Amazon,
das haben Sie schon richtig 
erfasst,
war es so die haben Gender 
rausgelassen,
hatten aber eine Spalte im 
Datensatz,
wo Gender noch drin war,
haben die Maschine dann
basierend auf allen Features 
außer Gender die
Vorhersage machen lassen und 
konnten dann natürlich
innerhalb ihrer Daten trotzdem 
vergleichen.
Wie ist die Performance dort in 
den Einträgen,
wo wir Männer,
Frauen nonbinär hatten und haben 
das dann verglichen?
Praktisch ja.
Aber es gibt viele Argumente 
dafür,
dass wir diese Features, die 
protected sind,
dass wir eigentlich größere 
Probleme erzeugen,
wenn wir sie den Maschinen nicht 
zur Verfügung stellen,
weil diese Maschinen so mächtig 
sind,
Korrelationen herauszufinden,
die letztlich ablesen lassen,
ob es welches Geschlecht es ist 
und das implizit lernen.
Und das ist sehr, sehr, sehr 
versteckt ist,
wo eigentlich jetzt eine 
Diskriminierung herkommt.
Wenn wir es aber drin lassen,
gibt es bestimmte Methoden, das 
direkt abzulesen.
Aber die Frage was will man denn 
da, Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D.,",7.903859854,1,The generated answer and the ground truth describe the same core process: running the analysis with and without the gender variable to compare outcomes and thereby control for its effect.,1,"The generated answer is faithful. [cite_start]The context describes a method where 'die haben Gender rausgelassen' (they left gender out) and then compared the performance within the data for men and women to see the difference[cite: 1143, 1144].",1,"The context directly answers the question. In a discussion about the Amazon recruiting tool example, the speaker explains that to check for bias, they compared the model's performance on a dataset where gender was left out versus one where it was included. The text says, ""das haben Sie schon richtig erfasst, war es so die haben Gender rausgelassen... und konnten dann natÃ¼rlich innerhalb ihrer Daten trotzdem vergleichen."""
"According to the text, what is one way large language models can exhibit biased tendencies?",They have been optimized to be overly agreeable and to 'say yes to everything.'.,"According to the text, large language models can exhibit biased tendencies by replicating existing patterns of bias and discrimination because they are typically trained to replicate the outcomes achieved by human decision-makers. Additionally, continuous model updating can be impeded by human bias.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” 
initial biases over time given sufficiently diverse and 
unbiased data, human bias es can impede this process. 
This insight is especially relevant in the current era of 
growing political and social divisions in human 
societies (e.g., Allcott et al., 2020). When people 
increasingly adopt biased attitudes and behaviors, 
biased, continuously updated ML models are less inherent to these models, adding a new dimension to 
the algorithmic fairness literature by shifting from 
static, one-time biases to a more temporal perspective: 
a comprehensive understanding of algorithmic 
discrimination and fairness must incorporate not just 
the initial conditions under which a model is trained, 
but also the dynamic environments where these models 
operate and evolve. Our results suggest that continuous 
model updating could serve as a mechanism for 
mitigating bias in ML systems and might act as a force 
to supplement traditional data cleaning and debiasing 
efforts (Shrestha et al., 2019) to reduce discrimination 
risks. 
On the other hand, our simulations reveal that the 
positive impact of continuous updating on algorithmic 
discrimination (and economic efficiency) largely 
hinges on the sociotechnological environment, which 
includes the degree of the system ’s initial bias and, 
perhaps more importantly, the persistence of biased 
decision-making by humans in the loop. We found that 
the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that",6.358855963,0,The generated answer identifies a different source of bias (being trained to replicate human decisions) than the ground truth (being overly agreeable and saying 'yes to everything'). The two explanations are not semantically equivalent.,1,"The generated answer is faithful. [cite_start]The context states that AI systems 'are typically trained to replicate the outcomes achieved by human decision-makers' [cite: 1163][cite_start], which can 'perpetuate existing patterns of bias and discrimination'[cite: 1163]. [cite_start]It also mentions that 'human bias es can impede' the self-correction process of models[cite: 1168].",1,"The context directly supports the answer. It states that AI systems ""are typically trained to replicate the outcomes achieved by human decision-makers,"" which can lead to perpetuating ""existing patterns of bias and discrimination."" It also notes that human biases can impede the self-correction of continuously updated models."
What did Google do to address concerns about the representation of nurses in image search results?,Google filtered the results so that both men and women are shown when 'nurse' is entered.,"According to the text, Google implemented filters to ensure that both men and women were shown when searching for ""nurse."" This was done to address the initial issue where image search results overwhelmingly depicted nurses as female.","in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not reﬂect the opinions of those who create them. The persistent
underrepresentation of women, African-Americans, and Latino
professionals in the IT workforce leads to biased algorithms. For
instance, a study in 2019 found that only 2.5% of Google ’s
employees were black, while Microsoft and Facebook had only 4%
representation. Another study revealed that 80% of AI professors
in 2018 were male. Involving diverse individuals in data collection
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | https://doi.org/10.1057/s41599-023-02079-x REVIEW ARTICLE
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x 7 Das hat Google jetzt anscheinend 
durch Filter so gemacht,
dass wenn man Krankenpfleger 
eingibt,
dass sowohl Männer und Frauen 
gezeigt werden.
Aber bis vor zwei Jahren war
es immer noch ein sehr gutes 
Beispiel.
Wenn ich Sie frage, googeln Sie 
mal bitte nach Images.
Dann war Krankenpfleger zu 99 %
weiblich und Arzt zu 99 % 
männlich,
obwohl diese Bilder natürlich 
von beiden
eigentlich schon vorhanden sein 
sollen.
Das heißt, wir haben hier
irgendwelche gewachsenen 
technischen Grundlagen,
Datenbanken, die das einfach 
noch nicht abbilden können.
Having said that, die 
Unternehmen sind
sehr sensibilisiert durch diese 
ganzen PR Probleme,
die sie damit erlitten haben und 
haben angefangen,
diese Daten zu bereinigen.
Aber das erfordert eben massiv 
viel Investitionen
in Datenbereinigung,
wo wir wieder in dem 
Kriegsmodell,
bei dem Data Preparation sind.
Sie müssen sich klar sein Wie 
sind
Ihre Strukturen der Daten 
eigentlich aufgebaut?
Soweit ich weiß es auch manchmal
das Labeling von Menschen 
gemacht.
Richtig? Genau. Es könnte so 
sein,
dass wir haben eine Gruppe von 
Menschen,
die nicht politisch korrekt 
denken.
Und dann würden sie es bewusst 
falsch.
Richtig. Auch ein massives 
Problem.
Wir haben auf der einen Seite",6.34782815,1,The generated answer and the ground truth are semantically identical. Both state that Google filtered image search results for 'nurse' to show both men and women.,1,"The generated answer is faithful. [cite_start]The context explicitly states that Google implemented filters 'dass wenn man Krankenpfleger eingibt, dass sowohl MÃ¤nner und Frauen gezeigt werden' (so that when you enter 'nurse', both men and women are shown)[cite: 1183].",1,"The context directly answers the question. It states, ""Das hat Google jetzt anscheinend durch Filter so gemacht, dass wenn man Krankenpfleger eingibt, dass sowohl MÃ¤nner und Frauen gezeigt werden."" (Google has now apparently done this with filters, so that when you enter nurse, both men and women are shown)."
"According to the document, what was the reason Amazon's AI recruitment system discriminated against women?","It recognized correlations between hobbies, universities attended, and gender due to historical data showing fewer women attending certain universities and pursuing technical fields.","According to the document, Amazon’s AI recruitment system discriminated against women because it was trained on a dataset of CVs that were predominantly male. This led the algorithm to perceive this biased model as indicative of success, resulting in the downgrading of applicants with keywords like “female.”","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",7.429790735,0,The generated answer provides a different reason (correlations with hobbies and universities) for the discrimination than the ground truth (being trained on predominantly male CVs). The two are not semantically equivalent.,1,"The generated answer is faithful. [cite_start]The context states that the bias in Amazon's AI recruitment tool 'stemmed from training the AI system on predominantly male employeesâ€™ CVs' [cite: 1198][cite_start], which led the algorithm to perceive this as a successful pattern and consequently discriminate against female applicants[cite: 1199].",1,"The context directly answers the question. It states that the bias in Amazon's hiring tool ""stemmed from training the AI system on predominantly male employeesâ€™ CVs,"" which led the algorithm to perceive this as a successful pattern and discriminate against female applicants."
"According to the lecture, what is the CRISP-DM model and what is its first and most important step?","The CRISP-DM (Cross-Industry Standard Process for Data Mining) is a blueprint for developing machine learning applications. Its first and most important step is Business Understanding, which involves clearly defining the business problem you are trying to solve.","According to the lecture, the CRISP-DM model is a standard process for structuring AI projects. Its first and most important step is **Business Understanding**.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 15. Mai 2025
Overview CRISP-DM
9",5.223655939,1,The generated answer and the ground truth are semantically identical. Both identify CRISP-DM as a blueprint for ML applications and state that its first and most important step is Business Understanding.,1,The generated answer is faithful. The context includes a slide titled 'Structuring an AI project: CRISP-DM' which clearly lists '1. [cite_start]Business Understanding' as the first step[cite: 1219].,1,"The context directly answers the question. A slide titled ""Structuring an AI project: CRISP-DM"" clearly identifies the first step as ""1. Business Understanding."" The lecture content emphasizes the importance of this step for the success of any data science project."
The lecture describes three levels of integrating machine learning solutions into an organization. What is a 'System Solution' (the third level)?,"A 'System Solution' is the deepest level of integration, where machine learning fundamentally changes the entire organizational structure and business model, requiring a complete rethinking of processes. The example given is Amazon's 'Anticipatory Shipping' patent.","According to the lecture, a “System Solution” is a “Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems.” It’s enterprise-level and highly integrated with multiple components and data flows, aiming to transform or optimize an entire organization – like Amazon’s “Ship then Shop” system.","System solutions•Definition:Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems.•Scope:Broad; enterprise-level•Integration:Highly integrated with multiple components and data flows.•Use case:Innovation must transform or optimize an entire organization•Example:Amazon’s “Ship then Shop” (Prediction + Inventory + Logistics + …) Und diese Systeme haben ein
Zusammenspiel dann was bestimmt,
ob ich eigentlich das System 
nutze.
Das war eines der fundamentalen 
Ergebnisse,
die Sie hier gefunden haben in 
der Studie.
Und dann Sie haben gesagt okay, 
wenn ich weiß,
dass wir die Menschen in
ihrer Kognition überzeugen 
müssen,
dass sie das System nutzen 
sollten und akzeptieren
sollten entweder über System
eins oder über System zwei 
Aktivierungen,
dann lass uns doch mal drüber 
nachdenken,
welche Faktoren denn hier eine 
besondere Rolle spielen.
Und da haben Sie insgesamt in
diesem Modell vier verschiedene 
Faktoren identifiziert.
Einer der Faktoren nennen Sie 
die Performance Expectancy.
Das können Sie sich vorstellen.
Sie haben ein System vor sich 
und ob
Sie eine Bereitschaft 
grundsätzlich Formen
in ihrer Kognition des System zu 
benutzen,
hängt damit zusammen, ob sie 
glauben,
dass dieses System Ihnen hilft,
bestimmte Ziele zu erreichen.
Wenn Sie sich die verschiedenen 
GPTsysteme,
die aktuell verfügbar sind,
auch in den öffentlichen 
Versionen,
dann sind da ganz viele 
verschiedene.
Dann gibt es aktuell vier 
Punkte, wo ich glaube,
das ist generell ZugReifbar 
mittlerweile.
Dann haben wir drei,
wir haben 4.1 4:05 usw.
Wenn ich die Entscheidung 
treffe, hinzubekommen.
Und auf der einen Seite immer
diese technische Perspektive auf 
die Systeme,
die müssen ausreichend 
performant und gut sein.
Aber wir brauchen eben auch
diese Einbettung der Systeme der
technischen Systeme in das Sozio 
system.
Deswegen sprechen wir 
grundsätzlich dann
von diesen soziotechnischen 
Systemen,
wo diese Systeme wirklich 
genutzt
werden und letztlich 
Veränderungen hervorrufen.
Und viel davon passiert eben 
hier
Information Systems auf Deutsch 
aus
der Perspektive der 
Wirtschaftsinformatik.
Was immer so eine 
Schnittstellendisziplin ist,
wo wir aus ganz verschiedenen 
Blickwinkeln Psychologie,
Management, Ökonomie,
Verhaltensökonomie, Neurologie
auf diese Phänomene draufgucken,
wie tatsächlich Menschen, 
Organisationen,
soziale Systeme mit
diesen technischen Systemen 
interagieren.
Und in diesem Sinne, ich denke, 
diejenigen,
die Wirtschaftsinformatik ich 
weiß gar nicht,
wer es gerade gibt,
Peter oder Olli, einer meiner 
Kollegen der Abteilung.
Wenn Sie Wirtschaftsinformatik 
eins
oder zwei besucht haben,
werden Sie bestimmt schon mal 
über das Unified,
also Unified Theory of 
Acceptance
and Use of Technology gehört 
haben.
Wer hier hat das in der 
Vorlesung schon mal gesehen?",8.142154932,1,"The generated answer and the ground truth are semantically identical. Both describe a 'System Solution' as the deepest level of integration that fundamentally changes the organization, using Amazon's 'Anticipatory Shipping' as the example.",1,"The generated answer is faithful. [cite_start]The context includes a slide defining a 'System solution' as a 'Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems' [cite: 1227] [cite_start]and provides 'Amazonâ€™s â€œShip then Shopâ€' as the example[cite: 1227].",1,"The context directly defines a 'System solution' and provides the exact example mentioned in the ground truth. A slide defines it as a ""Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems"" and gives the example: ""Amazonâ€™s â€œShip then Shopâ€ (Prediction + Inventory + Logistics + â€¦)."""
"According to the text, what is one thing machine learning models currently cannot do?",Machine learning models currently cannot provide a valuation of the consequences of a particular state of the world.,"According to the text, one thing machine learning models currently cannot do is “with unstructured data like words simply deal.” They require a significant amount of “data preprocessing.”","Sie müssen das sauber machen.
Maschinen, wie wir sehen werden,
können nicht mit 
unstrukturierten
Daten wie Wörtern einfach so 
umgehen.
Wir haben ganz viel 
Datenvorarbeit zu betreiben
und es gibt bestimmte Machine 
Learning Modelle,
die sind sehr schön,
die viel dieser Arbeit durch
bestimmte algorithmische 
Lernverfahren abfangen.
Zum Beispiel der 
Entscheidungsbaum
oder auch der Random Forest.
Aber es gibt andere Methoden und 
insbesondere die
most sophisticated Methoden wie 
neuronale Netze,
die auf jede Kleinigkeit in
den Daten massiv sensitiv 
reagieren,
müssen Daten standardisieren.
Wir müssen auf eine bestimmte 
Art und
Weise sampeln müssen
bestimmte Observationen 
rausschmeißen.
All das, wie Sie hier sehen,
braucht ungefähr 60 bis
80 % der Zeit eines solchen Data 
Science Projekts.
Es ist teilweise über Wochen, 
Monate.
Wenn wir Daten von Unternehmen 
bekommen,
mit denen wir zusammenarbeiten,
um Forschungsprojekte zu machen,
dass wir erst mal eine Idee 
bekommen,
was diese Daten alles bedeuten.
Sie haben bei mir in der im 
Seminar
gesessen letztes Semester diese 
Daten,
die Sie da erhalten haben, von 
der Firma.
Totales Chaos.
Muss man sich erstmal reindenken 
und
das braucht massiv viel Zeit. sind dann in eine Case Study 
gemacht,
wo ich ihnen gezeigt habe,
wie man Machine Learning 
anwenden kann.
Nicht immer nur zu negativen 
Aspekten,
aber diese Predictions,
die Machine Learning uns liefern 
kann,
wie man die auch dazu nutzen 
kann,
zum Beispiel zur Vorhersage,
welcher Kunde wie auf welches 
Messaging reagiert,
um letztlich die
Rücksenderaten von Produkten zu 
verringern.
Letzte Woche sind wir dann mehr 
ins
Detail schon reingesprungen, 
Eingesprungen,
haben uns überlegt, aus einer 
ökonomischen Perspektive,
was Prediction bedeuten könnte,
wenn wir bei General Purpose 
Technologien gesprochen.
Wir haben darüber gesprochen,
dass wir uns, wie
es bei so vielen General Purpose 
Technologien ist,
aktuell auf einen bestimmten 
Teil der Kurve,
die noch befinden,
die Kurve als die Darstellung, 
die fundamentale Idee,
dass wir nicht in große 
Technologie Investitionen habe,
dass sich die nicht sofort 
auszahlt.
Das ist immer so ein bisschen 
Verzögerung gibt,
am Anfang sogar die marginale
Produktivitätsveränderungen 
negativ sein kann.
Das ist so dieser untere 
Halbkreis von dem J.
Und das kann verschiedene Gründe 
haben.
Unter anderem eben, dass diese
Technologien noch nicht 
ausreichend in den
Organisationen mit Stelle sein.
Sie müssen also identifizieren 
welche
Daten welche Variablen sind 
eigentlich da?
Müssen Sie vielleicht neue 
Informationen erheben?
Sind bestimmte Informationen gar 
nicht zugänglich?
Und das Letzte kann tatsächlich 
und findet häufig statt,
dass man sagt Ich habe eine 
Riesenidee,
wie ich eigentlich Machine 
Learning nutzen kann.
Dazu brauche ich aber in einer 
First World ABCDE.
Und dann kommt der Betriebsrat 
und sagt ihnen Nee,
das können wir nicht machen, das 
funktioniert so nicht.
Und dann stehen sie dort und 
sagen Ja gut,
dann macht meine Modellidee,
um dieses Businessproblem zu 
lösen.
So wie ich es mir vorgestellt 
habe,
vielleicht gar keinen Sinn.
Deswegen befindet sich dann 
dieser Pfeil hier zurück.
Und Sie müssen natürlich sagen,
in der First Best World würde 
ich es gerne so haben.
Das sollte immer Ihr 
Ausgangspunkt sein.
Und dann werden Sie mit
der Realität konfrontiert und 
merken,
dass einige Sachen technisch 
einfach nicht da sind,
vielleicht noch nicht da sind.
Vielleicht bedarf es eben 
Änderungen
in der Art und Weise,
welche Informationen erheben 
werden müssen.
Es kann sein, dass Sie zu der 
Entscheidung kommen,
hier in diesem Data 
Understanding, dass Sie sagen 
Hey,",6.101855755,0,"The generated answer is incorrect. It claims models cannot deal with unstructured data, while the ground truth states that models cannot provide a valuation of the consequences of a particular state of the world.",1,The generated answer is faithful. [cite_start]The context states that machines 'kÃ¶nnen nicht mit unstrukturierten Daten wie WÃ¶rtern einfach so umgehen' (cannot simply deal with unstructured data like words) [cite: 1245] [cite_start]and that a significant amount of data preprocessing is required[cite: 1246].,0,"The context is not relevant. It discusses various aspects of data science projects, including data preprocessing and economic perspectives on predictions, but it does not contain information about what machine learning models cannot currently do regarding the valuation of consequences."
What are two types of bias metrics?,Individual-level metrics and group-level metrics.,"According to the text, two types of bias metrics are:

*   **Individual-level metrics:** These ensure that people with similar qualifications receive similar outcomes.
*   **Group-level metrics:** These ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups.","15. Mai 2025
Measurement of algorithmic fairness
•There is no single agreed-upon measure for discrimination/fairness•There are different types of bias metrics that we can distinguish•Individual-level metrics:ensure that people who have similar qualifications with respect to a task receive similar outcomes•Group-level metrics:ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups
15
Verma & Rubin, 2018 data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",6.256340981,1,"The generated answer and the ground truth are semantically identical, both identifying individual-level and group-level metrics as the two types.",1,The generated answer is faithful. [cite_start]The context includes a slide that clearly distinguishes between 'Individual-level metrics' and 'Group-level metrics' and provides a brief description of each[cite: 1270].,1,"The context directly answers the question. A slide titled ""Measurement of algorithmic fairness"" clearly distinguishes between two types of metrics: ""Individual-level metrics"" and ""Group-level metrics,"" and provides a brief definition for each."
"According to the CRISP-DM framework, what is the approximate percentage of time spent on data preparation within the CRISP-DM process?",Approximately 60-80% of time.,"The provided text doesn’t give an exact percentage of time spent on data preparation. It simply states that the “Data Preparation phase covers all activities to construct the final dataset” and that tasks are “likely to be performed multiple times, and not in any prescribed order.”","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and 15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9",6.784554243,0,"The generated answer is incorrect. It claims the text doesn't give a percentage, while the ground truth provides a specific range (60-80%).",1,"The generated answer is faithful. It correctly states that the context doesn't give a specific percentage. [cite_start]The source text describes the Data Preparation phase but never assigns a time percentage to it[cite: 1286, 1287].",0,"The context discusses the phases of the CRISP-DM model, including Data Preparation, but it does not provide any information on the percentage of time spent on this specific phase."
What does the document state about the impact of Generative AI on employee happiness?,"The document states that employees became happier, with less escalation and longer tenure.","You are absolutely correct. The document focuses on algorithmic discrimination in recruitment and selection, and does not contain any information about the impact of Generative AI on employee happiness.","employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender a study on the impact of artiﬁcial intelligence on eliminating hiring bias from
recruitment and selection process. J Comput Theor Nanosci 17(9):4404–4407
Ruwanpura KN (2008) Multiple identities, multiple-discrimination: a critical
review. Fem Econ 14(3):77–105
Samuelson PA (1952) Spatial price equilibrium and linear programming. Am Econ
Rev 42(3):283–303
Shaw J (2019) Artiﬁcial intelligence and ethics. Perspect: Policy Pract High Educ
30, 1–11
Shin D, Park YJJCIHB (2019) Role of fairness, accountability, and transparency in
algorithmic affordance. Perspect: Policy Pract High Educ 98, 277–284
Smith B, Shum H (2018). The future computed. Microsoft
Tilcsik A (2021) Statistical discrimination and the rationalization of stereotypes.
Am Sociol Rev 86(1):93–122
Timmermans S, Tavory I (2012) Theory construction in qualitative research: from
grounded theory to abductive analysis. Sociol Theory 30(3):167–186. https://
doi.org/10.1177/0735275112457914
Upadhyay AK, Khandelwal K (2018) Applying artiﬁcial intelligence: implications
for recruitment. Strateg HR Rev 17(5):255–258
van Esch P, Black JS, Ferolie J (2019) Marketing AI recruitment: the next phase in
job application and selection. Comput Hum Behav 90:215–222. https://doi.",5.888198853,0,"The generated answer is incorrect. It claims the document contains no information on the topic, while the ground truth provides a specific finding (employees became happier).",1,The generated answer is faithful. It correctly states that the document does not contain information about the impact of Generative AI on employee happiness. The provided context is about algorithmic discrimination in recruitment and does not mention this topic.,0,The context is irrelevant. It discusses theories of discrimination in the labor market and the benefits of AI in recruitment but contains no information about the impact of Generative AI on employee happiness.
What did Amazon patent in 2014?,A 'Chip Shop' (patent).,This text doesn't contain information about any patents Amazon held in 2014. It provides background information about a marketing science article and INFORMS.,"work as “Marketing Science. Copyright © 2024 The Author(s). https:/ /doi.org/10.1287/mksc.2022. 
0393, used under a Creative Commons Attribution License: https:/ /creativecommons.org/licenses/ 
by-nc-nd/4.0/.” 
Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE 
and the Deutsche Forschungsgemeinschaft [Grant 449023539]. 
Supplemental Material: The online appendix and data files are available at https:/ /doi.org/10.1287/mksc. 
2022.0393. 
Keywords: electronic commerce • nudging • causal forest • digital footprint • consumer returns • artificial intelligence
1. Introduction
In recent years, global e-commerce has experienced sig -
nificant and sustained growth. E-commerce presents 
numerous advantages for both customers and retailers. 
However, the lenient, and frequently free, return poli -
cies introduce drawbacks that complicate this expan -
sion. In the fashion industry, the share of purchases 
returned can reach a staggering 50% (Dzyabura et al. 
2023 ).
Product returns pose a substantial financial burden 
on retailers (Nageswaran et al. 2020 ). The costs of estab -
lishing return infrastructures (Ofek et al. 2011 ), incur -
ring transportation expenses (Zhou and Hinz 2016 ), profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for 
components of this work owned by others than the Association for Information Systems must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior 
specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, 
GA, 30301-2712 Attn: Reprints, or via email from publications@aisnet.org. License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The
Author(s). https://doi.org/10.1287/mksc.2022.0393, used under a Creative Commons Attribution License: https://
creativecommons.org/licenses/by-nc-nd/4.0/.”
Copyright © 2024 The Author(s)
Please scroll down for article—it is on subsequent pages
With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations
research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning
opportunities for individual professionals, and organizations of all types and sizes, to better understand and use
O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",5.275604725,0,"The generated answer is incorrect. It claims the text doesn't contain information about Amazon's patents, while the ground truth specifies a 'Chip Shop' patent from 2014.",1,The generated answer is faithful as it correctly states that the provided context does not contain information about an Amazon patent from 2014. The context is about a marketing science article and the organization INFORMS.,0,"The context is irrelevant. It provides information about a marketing science article and the organization INFORMS, but it does not mention any patents held by Amazon in 2014."
The lecture identifies two fundamental aspects that make AI different from previous General Purpose Technologies like electricity. What are they?,"First, it is a technology that can improve itself through its own use (self-learning). Second, this self-learning creates massive cross-industry spillover effects, where use in one industry can improve the system for another.","According to the text, the two fundamental aspects that make AI different from previous General Purpose Technologies like electricity are:

1.  **The technology itself can learn and improve continuously** through use – it’s the first technology to have this capability.
2.  **It sparks waves of follow-on innovations, new business models, and organizational redesigns** (it’s a “catalyst for complements”).","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. some plants. After the mid-1990s, artiﬁcial intelligence software
saw signiﬁcant advances. Until today’s digital economy, AI has
been commonly used in various industries (Hmoud and Laszlo,
2019).
Artiﬁcial intelligence is deﬁned as the ability of something like
a machine to understand, learn, and interpret on its own in a
human-like manner (Johansson and Herranen,2019). Artiﬁcial
intelligence aims “to understand and simulate human thought
processes and to design machines that mimic this behavior.”It is
designed to be a thinking machine with a level of human
Table 1 Research theme, papers, and main content.
Theme Research papers Main content
1. AI-enabled recruitment
application and its beneﬁts.
Bogen and Rieke (2018), Ahmed (2018), Hmoud and Laszlo
(2019), Albert (2019), van Esch et al. (2019), Köchling et al.
(2022), and Chen (2023)
Summarize the recruitment process into four
main steps: sourcing, screening, interviewing, and
selection
Beattie et al. (2012), Newell (2015), Raub (2018), Miasato and
Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal- drauf gucken,
ob es das Richtige ist,
die Kohlekraftwerke wieder 
hochzufahren.
Grundsätzlich kann das aber als
Kerninnovation KI KI braucht.
Das wollen wir alle machen,
braucht aber weitere 
Innovationen,
dass wir es wirklich gut nutzen 
können.
Was glauben Sie denn,
wenn Sie KI mit den vorherigen 
Innovationen vergleichen?
Was macht KI besonders?
Was ist ganz fundamental
unterschiedlich
zu allen vorherigen General 
Purpose Technologies.
Und ich habe es gerade schon 
gezeigt Ja,
man braucht keine neue Hardware 
als Nutzer.
Am Ende doch die.
Also man braucht immer wieder 
andere Hardware.
Irgendwann reiben sich auch 
GPU's zum Beispiel auf,
geht aber in die richtige 
Richtung,
in das Technische zu denken.
Yes, das ist ein Teil davon.
Wir haben zum Ersten Mal Systeme 
oder eine Technologie,
die sich selbst verbessern kann.
Wenn Sie sich überlegen Sie 
haben den Hammer,
was auch eine General Purpose 
Technologie ist,
nur weil Sie dreimal den Hammer 
benutzen,
wird der Hammer per se nicht 
besser.
If anything wird der Hammer.
Irgendwie über die Zeit 
schlechter.
Wir haben aber jetzt eine 
Technologie.
Dadurch, dass wir sie nutzen,
wir neue Daten kreieren und
diese neuen Daten wieder in
die Maschine reingefüttert 
werden können,",9.495078564,1,The generated answer and the ground truth are semantically identical. Both identify self-learning and cross-industry spillover effects as the two fundamental aspects that differentiate AI.,1,The generated answer is faithful. [cite_start]The context provides a slide that lists two unique aspects of AI as a General Purpose Technology: 'For the first time: continuous learning of the technology itself with cross industry spillovers' and its role as a 'Catalyst for complements' that 'sparks waves of follow-on innovations'[cite: 1333].,1,"The context directly answers the question. A slide explaining why AI is a General Purpose Technology lists two key differentiators: ""For the first time: continuous learning of the technology itself with cross industry spillovers"" and its role as a ""Catalyst for complements,"" which sparks follow-on innovations."
"From a purely economic perspective, what is the single most important function that machine learning technology provides?","It is a technology that massively lowers the cost of making high-quality, accurate predictions.","Based on the provided text, from a purely economic perspective, the single most important function that machine learning technology provides is **enabling data-driven decision-making.** 

The text highlights how ML helps translate data into insights, which then leads to more informed management decisions across areas like marketing, customer analysis, and market trend prediction.","Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 24. April 2025
However, beyond the hype what does Machine Learning actually deliver?",7.400822878,0,"The generated answer ('enabling data-driven decision-making') is a broader, more generic function than the very specific economic function identified in the ground truth ('massively lowers the cost of making high-quality, accurate predictions'). They are not semantically equivalent.",1,The generated answer is faithful. [cite_start]The context states that Machine Learning has become an indispensable tool for making 'datenbasierte Entscheidungen' (data-based decisions) to increase efficiency and drive innovation[cite: 1353].,1,"The context directly answers the question by explaining that machine learning is a tool for making ""datenbasierte Entscheidungen"" (data-based decisions) to increase efficiency and drive innovation, which is the core economic function."
"According to the text, what does the 'J-curve' represent?","The 'J-curve' represents a phenomenon where, with many technologies, productivity initially decreases before eventually increasing.","The provided text does not contain any information about a “J-curve”. It discusses data mining, machine learning models, and data representation, but it doesn't mention a J-curve.","The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase
w hich phase, or w hich particular task of a phase, has to be perform ed next.
The outer circle in Figure 2 sym bolizes the cyclic nature of data m ining itself. D ata m ining is not
finished once a solution is deployed. The lessons learned during the process and from  the
deployed solution can trigger new , often m ore focused business questions. Subsequent data
m ining processes w ill benefit from  the experiences of previous ones (cf. the virtuous cycle of
(B erry and Linoff, 1997)). dieser Gleichung hier 
ausdrücken.
Wir haben drei verschiedene 
Komponenten.
Wir haben hier dieses Hütchen,
wir haben das fix und wir haben 
hier dieses Y Hütchen.
Meine erste Frage an Sie Was war 
das? F Hütchen.
Yes, genau richtig.
Wir haben hier das erlernte 
Machine Learning Modell,
was die Muster, die wir in 
historischen
Daten durch Algorithmen erkannt 
haben, abspeichert.
Hier in diesem F. Steckt unser 
gesamtes Wissen drin,
was wir irgendwie extrahieren
und wie genau wir das 
extrahieren.
Das wird der zweite Teil unserer 
Vorlesung sein.
Wichtig hierbei Hütchen ist 
immer dafür ein Indikator,
dass wir von einer Approximation 
sprechen.
Das ist ein Modell.
Das ist nicht der wahre 
unterliegende Prozess,
der die Daten generiert.
Das ist nicht die Landschaft von 
Frankfurt sozusagen,
sondern es ist die Landkarte.
Es ist ein explizit abstrakteres 
Modell,
eine Darstellung der Realität.
Und da können eben Fehler drin 
sein.
Es ist ganz wichtig, sich
immer wieder vor Augen zu 
halten.
Dann haben wir die zweite 
Komponente.
Das war das XML. Was ist das?.X. 
Jemand eine Idee?
Jetzt dahinten? Genau das sind 
die Daten,
die wir in diese Maschine 
reintun.
Wir haben zum Beispiel, wenn Sie 
überlegen,
Sie interagieren mit Chat GPT 10. Juli 2025
Gradient descent: the intuition
18
Imagine a data set with three observations:
Can you spot the relationship between 𝑥and 𝑦?We train a perceptron with one weight 𝑤and no bias
Assume 𝑓𝑤⋅𝑥=𝑤⋅𝑥(linear activation function). What is the optimal 𝑤?
𝑥 𝑦3 -0.21-5 0.359 -0.63
𝑓(𝑤⋅𝑥)𝑥 A𝑦",5.822864056,0,"The generated answer is incorrect. It claims the text doesn't contain information about the 'J-curve,' while the ground truth provides a correct definition of the phenomenon.",1,The generated answer is faithful as it correctly reports that the context does not mention a 'J-curve'. The provided text discusses data mining and machine learning models but does not contain this term.,0,The context is irrelevant. It discusses the phases of the CRISP-DM data mining process and the components of a machine learning equation but makes no mention of a 'J-curve'.
What is a potential new attack vector related to large language models?,"Systems can be hacked, leading to false or harmful code being written into company codebases.","The provided text doesn’t offer information about potential attack vectors related to large language models. It discusses:

*   The interplay between continuous model updating and algorithmic discrimination.
*   The origins and impacts of algorithmic feedback loops.
*   Details about Word2Vec and transformer architectures.

It does not contain any information about new attack vectors.","affecting ML systems’ functionality by explaining the 
interplay b etween continuous model updating and 
algorithmic discrimination. Despite its critical importance 
in developing countermeasures and refining policy 
recommendations, the complex interplay between 
algorithmic discrimination and continuous ML model 
updating remains an underexplored area in the literature.  
Second, our paper complements the nascent body of 
studies exploring the origins and impacts of 
algorithmic feedback loops (Burghardt & Lerman, 
2022; Cowgill, 2018). For instance, Lum and Isaac 
(2016) revealed that the iterative updates of a 
predictive poli cing system led to increased 
discrimination against minority groups. Similarly, 
Ensign et al. (2017) showed how feedback loops could 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
A game changer
27
The transformer architecture1.Transform words into numbers –Word embeddings2.Control word positioning of words –Positional encoding3.Represent words in the context of other words –Self-attention4.Predict the next word using a feed-forward neural net",7.077908278,0,"The generated answer is incorrect. It claims the text doesn't offer information on this topic, while the ground truth provides a specific example of a new attack vector.",1,The generated answer is faithful. It correctly states that the provided text does not contain information about attack vectors related to large language models. The context discusses other topics like feedback loops and Word2Vec.,0,"The provided context is irrelevant. It discusses feedback loops in machine learning, the Word2Vec model, and the transformer architecture, but it contains no information about new attack vectors related to large language models."
"According to the text, what is a problem with relying on a perfect machine learning model to predict future outcomes?","It would shift us into a situation where there appears to be no risk, but this is conceptually problematic.","According to the text, a problem with relying on a machine learning model is that “it does not necessarily have to be right.” The model’s output – a prediction – “is not necessarily a real outcome.” It’s aiming for accuracy, but it’s not guaranteed to be correct.","1933 ), uses machine learning functions to flexibly esti -
mate residuals in treatment and outcome prediction 
equations, enhancing the robustness of our treatment 
effect estimates and aligning with accepted practices 
(detailed further in Online Appendix A.6). We further 
conform to data science standards by dividing our data 
into training (80%) and testing (20%) sets (Hastie et al. 
2009 ). We fine-tune the causal forest’s hyperparameters 
through a grid search with fivefold crossvalidation on 
the training data, focusing on out-of-sample R-score per -
formance (Schuler et al. 2018 ). We provide hyperpara -
meter tuning details in Online Appendix A.7.
4.1.3. Evaluation of the CML Model. Our evaluation of 
the CML model is based solely on test data and follows 
two steps. First, we assess the CML model’s predictions 
through group average treatment effects following 
Chernozhukov et al. ( 2018 ), where we separately esti -
mate the treatment effects for individuals who the 
CML model would treat and those who it would not 
treat. Second, we evaluate the CML-based treatment 
policy through off-policy evaluation following Hitsch 
et al. ( 2024 ). Off-policy evaluation estimates the effect 12. Juni 2025
Remember the following case study from lecture 1.(a) Explain in your own words what the central machine learning prediction helps to do. (b)Provide a brief discussion about the potential dangers associated using ML models for such purposes in the context of misinformation.
5 Das ist unsere Prädiktion,
auf Englisch auch wieder 
indiziert durch das Hütchen.
Das Hütchen zeigt uns an dieser 
Stelle,
dass wir nicht ganz explizit 
wissen,
ob das eigentlich das reale 
Outcome ist.
Wenn Sie eine Machine Learning 
System haben,
was Ihnen basierend auf den
Soziodemografischen Faktoren 
einer Person
eine Vorhersage gibt darüber,
ob diese Person tatsächlich 
kreditwürdig ist oder nicht.
Ist dieser Output der
Maschine das Y immer eine 
Prediction,
eine Vorhersage, die nicht
notwendigerweise richtig sein 
muss.
Wir hoffen, dass sie so akkurat 
wie möglich ist.
Das ist aber nicht zwangsläufig 
gegeben.
Und wer erinnert sich an das 
Ziel von
Machine Learning aus einer 
konzeptionellen Perspektive?
Wenn wir über die overarching 
Objectives reden,
was wollen wir damit erreichen?
Wofür ist dieses ganze Modell 
gebastelt etc.
da? Die bestmöglichen 
Vorhersagen?
Genau, Und für welche Daten?
Ja, und eine Sache will ich noch 
rauskitzeln.
Was sind die Inputdaten am 
besten?
Daten, die das Modell noch nicht 
kennt? Genau das.
Es geht uns nicht darum,
dass wir ein Modell haben,
was uns optimal erklärt,
was in der Vergangenheit alles 
passiert ist.
Aber wir wollen keinen 
Vorhersagemodell
über den Stockmarket Price",7.740653992,0,"The generated answer is incorrect. It focuses on the idea that a model's prediction is not always right, which is different from the ground truth's more nuanced point about the conceptual problem of shifting into a situation that appears to have no risk.",1,"The generated answer is faithful. [cite_start]It correctly identifies that the model's output is 'immer eine Prediction, eine Vorhersage, die nicht notwendigerweise richtig sein muss' (always a prediction, a forecast, that doesn't necessarily have to be right)[cite: 1404]. This directly supports the generated answer's claim.",1,"The context directly answers the question. It states that the machine's output (Y-hat) is a ""Prediction, eine Vorhersage, die nicht notwendigerweise richtig sein muss"" (a prediction that does not necessarily have to be right). This aligns with the ground truth that relying on a perfect model is conceptually problematic because predictions are not certain outcomes."
How much have the costs for inference (using GPT or other Large Language Models) decreased in the last two years?,The costs have decreased by a factor of 34.,"According to the text, the costs for inference (using GPT or other Large Language Models) have decreased by a factor of 34 in the last two years.","Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen.
Wenn wir über GPT Systeme 
sprechen,
weil die da sehen wir natürlich 
merklich unterschiedlich,
ob ich 60 Gigabyte Text
reinknalle oder das halbe 
Internet.
Dann habe ich natürlich schon 
unterschiedliche
Kommunikationskosten.
Wobei jetzt in dem neuen Report
war zum Beispiel angezeigt,
dass die Kosten zumindest für 
die Inferenz,
also wenn Sie GPT benutzen oder
andere Large Language Models,
die Kosten sind seit zwei zwei 
Jahren Haben
die sich bei einem Faktor 34 
verringert?
Ja, das wird massiv effizienter.
Wir sehen jetzt so ein bisschen 
die
Laws of Scaling einsetzen.
Ganz kurz noch hat das Ihre 
Frage.
Das ist die beantwortet mit 
Ockhams Razor.
Wenn wir jetzt nur nach 
Temperatur entscheiden,
wäre nach diesem Gesetz kein 
Gesetz,
das vorsichtig sein. Das ist so 
eine.
Ich würde es als Daumenregel 
bezeichnen.
Deswegen hat man auch davon 
abgesehen,
dass irgendwie Rule zu nennen.
Aber grundsätzlich so ein 
bisschen aus der
Physik motiviert zu sagen naja,
ein einfacheres Modell, was mir 22. Mai 2025
A frequent trade-off
Model complexity, prediction accuracy, and model interpretability•Linear models simple to interpret as input-output relationships can be quantified in magnitude and direction, see, e.g., OLS coefficients•More complex models typically more accurate in predictions•Often a tradeoff; but not always (Sometimes “Occam’s Razor” seems to apply)
12
Model complexity / ability to capture pattern
Interpretability
BayesLinear regressionDecision tree
Deep NNEnsemble methods (e.g., RF)SVM
NOTE: Graph just an illustration without precise scales; exact relationship highly dependent on actual problem 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18",5.978545666,1,"The generated answer and the ground truth are semantically identical, both stating that costs have decreased by a factor of 34.",1,The generated answer is faithful. [cite_start]The context explicitly states that the costs for inference 'sind seit zwei zwei Jahren... bei einem Faktor 34 verringert' (have decreased by a factor of 34 in the last two years)[cite: 1414].,1,"The context directly answers the question. It states that ""die Kosten zumindest fÃ¼r die Inferenz...seit zwei zwei Jahren Haben die sich bei einem Faktor 34 verringert?"" (the costs at least for inference...for two years have they decreased by a factor of 34?)."
"What is the idea behind using simpler models, according to Ockhams Razor?",To prefer a model that needs less information to make a good prediction.,"According to the text, the idea behind using simpler models, as guided by Ockham’s Razor, is to avoid recognizing random fluctuations in the data as important patterns due to increased model complexity. Essentially, the goal is to minimize the risk of the model overcomplicating things and attributing noise to meaningful relationships.","Ockhams Razor aber dafür,
dass man versuchen sollte,
ein Modell so einfach wie 
möglich zu belassen.
Bedeutet in unserem Beispiel ich 
sollte ein Modell
laut diesem Ockhams Razor 
bevorzugen,
was weniger Informationen 
benötigt,
um eine gute Vorhersage zu 
machen.
Wäre das nicht auch teilweise 
ökonomischer?
In dem Modell, eine Menge 
Energie zu verbrauchen,
würden vielleicht weniger Daten
das gleiche Ergebnis bringen.
Das ist auch ein wichtiger Punkt 
grundsätzlich,
den wir aber in den Datenmengen,
über die wir alle sprechen,
wenn wir über tabulare Daten
insbesondere sprechen, 
ignorieren können,
weil die Kosten dafür so gering 
sind,
dass das keine Rolle spielt,
ob ich einen oder zwei Features 
mehr nehme.
Es gibt Unternehmen, die viel 
mehr Daten.
Das stimmt. Das ist 
grundsätzlich richtig.
Es kommt auf den 
Trainingsprozess an,
es kommt auf die Art des Modells
an für unsere Purpose is.
Aber das meine ich damit Für 
unsere Datensätze,
die wir nutzen, ist das eine 
Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen. die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell eher das Modell nehmen, was 
einfacher ist.
Der Gedanke, der dahinter 
steckt,
ist, dass wir vermeiden wollen,
dass die zufälligen Schwankungen 
in den
Daten mit höherer 
Modellkomplexität
in dem Modell erkannt werden als 
wichtige Muster.
Time ist jetzt auch ein bisschen 
drauf gucken,
dass ich meinen Time Management 
richtig betreibe,
weil wie Sie vielleicht gemerkt 
haben,
habe ich die Tendenz, viel zu 
reden.
Kommen Sie gerne nach der 
Vorlesung,
dann können wir darüber 
sprechen.
Wenn wir das verallgemeinern und 
jetzt nicht nur
über unser EiscremeBeispiel 
nachdenken,
sondern ganz, ganz, ganz High 
Level aus einer fast schon.
Und Uwe Walz würde sich freuen,
wenn diese Formel hier angezeigt 
wird.
Wahrscheinlich aus einer 
mikroökonomischen
Perspektive kann man
natürlich fragen Warum brauchen 
wir Vorhersagen?
Und aus einer ökonomischen
Sicht Wenn ich bestimmte 
Entscheidungsprobleme,
Nutzenmaximierungsprobleme,
Gewinnmaximierungsprobleme 
aufschreibe, dann ist immer so
ein bisschen in
fast allen Optimierungsproblemen 
uncertainty mit drin.
Bestimmte Sachen weiß ich 
einfach nicht.
Es kommt so ein bisschen darauf 
an,
was in der Zukunft passieren 
wird.
Wenn ich heute investiere,
wenn ich eigentlich mein",8.225976467,1,The generated answer and the ground truth are semantically identical. Both state that the idea is to prefer simpler models that require less information for a good prediction.,1,"The generated answer is faithful. [cite_start]The context explains that the idea behind simpler models is to 'vermeiden wollen, dass die zufÃ¤lligen Schwankungen in den Daten mit hÃ¶herer ModellkomplexitÃ¤t in dem Modell erkannt werden als wichtige Muster' (to want to avoid that random fluctuations in the data are recognized by a model with higher complexity as important patterns)[cite: 1437].",1,"The context directly answers the question by explaining the rationale behind Ockham's Razor. It states a preference for a simpler model to ""vermeiden wollen, dass die zufÃ¤lligen Schwankungen in den Daten mit hÃ¶herer ModellkomplexitÃ¤t in dem Modell erkannt werden als wichtige Muster"" (to avoid having random fluctuations in the data being recognized as important patterns by a more complex model)."
"In the paper 'The Productivity J-Curve' by Brynjolfsson et al., what does the Fortune 500 enterprise software company specialize in?",The company specializes in providing business process software for small and medium-sized U.S. businesses.,The provided text does not contain information about “The Productivity J-Curve” by Brynjolfsson et al. or any details about a Fortune 500 enterprise software company. It only describes a lecture course on Machine Learning for Management Decisions.,"Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 8. Mai 2025
MachineLearning für Management Entscheidungen (MLME)Lecture2Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz 23. Juli 2025
MachineLearning für Management Entscheidungen (MLME)Lecture11Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz",7.020652771,0,"The generated answer is incorrect. It claims the text does not contain the requested information, while the ground truth provides the specific specialization of the company.",1,The generated answer is faithful. It correctly states that the provided context does not contain the requested information. The text is about a general course on Machine Learning for Management Decisions and does not mention 'The Productivity J-Curve' paper or a specific Fortune 500 company.,0,The context is entirely irrelevant. It provides general information about a machine learning course for management students but does not mention the paper 'The Productivity J-Curve' or the specific company in question.
"According to the document, what is the economic decision rule for an investor?","'Invest'  if  p ⋅U (15,15)+ (1−p) ⋅ U(0,20) ≥U (10,0). 'Not invest' otherwise.","According to the document, the economic decision rule for an investor is:

“Invest” iff “𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)” “Not invest” otherwise.

Where:

*   𝑝 = the probability that the borrower would make a re-transfer
*   𝑈15,15+1 = utility of investing 15 mil €
*   𝑈0,20 = utility of not investing 0 €
*   𝑈(10,0) = utility of investing 10 mil €","8. Mai 2025
Data driven decision-makingWhat you implicitly did (probably) are two parts:•Compared how they feel about certain outcomes•Assigned a probability 𝑝 that the borrower would make a re-transfer Choose the strategy that maximizes your „utility“ (whatever this is)Economic decision rule for investor: „Invest“  iff  𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)  „Not invest“ otherwise
12
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup : 20 mil €
Investor: 10 mil €Startup : 0 mil € human decisions from a nother prior empirical study 
(Study A, see below for more details) where we used 
the strategy method to measure participants ’ choices 
assuming the investor initially made the investment. 
That is, we kn ew whether each individual borrower 
would or would not make a repayment if an investor 
were to initially invest 10 MU with them. Importantly, 
due to the strategy method , we kn ew the borrower ’s 
conditional repayment decision regardless of the actual 
investor decision, i.e., we kn ew counterfactual 
outcomes. Game outcomes were determined by pairing 
a simulated investor decision with the borrower ’s 
actual conditional decision. 
Where do the ML system, selective labels problem, 
and continuous updating come into play? There are 
two pertinent aspects of the investment game. First, 
investors make initial decisions under uncertainty and 
form beliefs about the likelihood of borrowers making 
a repayment. Here, an ML  model prediction can 
decrease information asymmetries (Agrawal et al., 
2019) by providing individual-level predictions on the 
borrower’s repayment likelihood, increasing the 
potential for economically efficient outcomes. Second, 
investors only observe borrower behavior if they 8. Mai 2025
Data driven decision-making: an exampleConsider the following economic transaction
11
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup: 20 mil €
Investor: 10 mil €Startup: 0 €
What would you do, and why?",11.15151858,1,"The generated answer and the ground truth present the exact same economic decision rule, making them semantically identical.",1,"The generated answer is faithful. [cite_start]The context provides a diagram of the economic transaction and explicitly states the economic decision rule for the investor as: â€žInvestâ€œ iff ð‘â‹…ð‘ˆ(15,15)+ (1âˆ’ð‘) â‹…ð‘ˆ(0,20) â‰¥ð‘ˆ(10,0) â€žNot investâ€œ otherwise[cite: 1454].",1,"The context provides a diagram of the economic transaction and explicitly states the economic decision rule for the investor as: â€žInvestâ€œ iff ð‘â‹…ð‘ˆ(15,15)+ (1âˆ’ð‘) â‹…ð‘ˆ(0,20) â‰¥ð‘ˆ(10,0) â€žNot investâ€œ otherwise."
"According to the document, what are two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses?",A short-run dip and a long-run rebound.,"According to the document, the two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses are:

1.  **Initial dip:** A short-run dip in productivity is measured, often due to heavy up-front investment.
2.  **Long-run rebound:** Productivity accelerates once complementary investments are in place, driven by learning effects, scale economies, and network externalities.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind",9.53039813,1,The generated answer and the ground truth are semantically identical. Both describe the two phases as a short-run dip followed by a long-run rebound.,1,The generated answer is faithful. [cite_start]The context includes a slide titled 'Expect a J-Curve' that defines the phenomenon as a 'two-phase productivity pattern that emerges when a GePuT diffuses' and details the two phases as a 'Short-run dip' and a 'Long-run rebound'[cite: 1465].,1,"The context directly answers the question. A slide titled ""Expect a J-Curve"" describes the phenomenon as a ""two-phase productivity pattern"" consisting of a ""Short-run dip"" and a ""Long-run rebound."""
What is the date of the first lecture?,45771,"The date of the first lecture is April 24, 2025.","24. April 2025
General informationLecture and tutorialLecture to introduce topics and cover theoryTutorial to work on applications (case studies & coding)Six tutorials by Charlotte Knickrehm How can you ask questions?Always ask questions on the runSend an e-mailOffice hours by appointmentWhat is expected of you?Prepare readingsIn class participation (lecture & tutorial)
20
This course is discussion-driven and relies a lot on your participation. 24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung 24. April 2025
Any questions on course structure?",3.502524853,1,"The generated answer and the ground truth state the exact same date, making them semantically identical.",1,"The generated answer is faithful. [cite_start]The context contains a slide that lists lecture information, including the date range 'wÃ¶chentlich vom 24.04.2025 bis 24.07.2025' (weekly from April 24, 2025 to July 24, 2025), confirming the start date[cite: 1482].",1,"The context directly answers the question. A slide titled ""General information"" clearly states the date of the first lecture: ""wÃ¶chentlich vom 24.04.2025 bis 24.07.2025"" (weekly from 24.04.2025 to 24.07.2025)."
What is the purpose of the tutorial sessions?,To work on applications (case studies & coding).,"The purpose of the tutorial sessions is to help participants develop a solid understanding of ML tools and to recognize their usefulness for management decisions. They also focus on practical exercises using the Python programming language to implement ML projects, even for those without prior knowledge.","23. Juli 2025
Clear & Specific
10
Before prompting•understand the context and the task you use the LLM for•think about what you want to achieveEffective prompts•clearly define objectives•feature direct and specific instructions•Give context•separate inputs and instructions consistently•Break up complex sentences Trainieren
selber gar nicht lange und
wir können sehr viele Kniffe 
benutzen,
wie wir dieses diesen
Trainingsprozess in einer 
gewissen Art
und Weise automatisieren können,
indem wir zum Beispiel
die Modellstruktur, die 
Modellauswahl,
sogar die Auswahl der relevanten
Inputinformationen durch 
bestimmte sogenannte
Cross Validation Verfahren durch
die Daten selber identifizieren 
lassen.
Ich hasse diesen Satz,
aber hier passiert dann sehr 
häufig dieses Let
the data speak so wenig anhaben 
wie möglich machen.
Wir versuchen einfach, das beste 
Modell zu finden.
Und auch das sei wieder betont, 
ist fundamental.
Uns geht es darum, ein Modell zu 
finden,
was nicht unsere historischen
Daten so gut es geht erklärt,
sondern sobald das Modell da 
ist,
möchten wir ein Modell haben,
was es uns ermöglicht,
Vorhersagen für die Zukunft zu 
machen,
also Vorhersagen für Beispiele,
Observationen, die das Modell 
noch nicht gesehen hat.
Denken Sie an das 
EiscremeBeispiel zurück.
Es ging nicht darum, perfekt zu 
erklären,
wie sich in der
Vergangenheit unsere 
Eisverkäufer abhängig
von bestimmten 
Wetterinformationen dargestellt 
haben,
sondern es ging darum Ich habe 
einen bestimmte
Logik identifiziert aus den 
historischen Daten. Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen.",5.997855902,0,"The generated answer is incorrect. It describes the purpose as developing an understanding of ML tools, which is a broader educational goal, whereas the ground truth correctly identifies the specific purpose of the tutorial sessions, which is to work on 'applications (case studies & coding)'.",0.5,"The generated answer is only partially faithful. [cite_start]While the context does state that a focus of the course is on 'praxisnahen Ãœbungen' (practical exercises) with Python [cite: 1491] [cite_start]and aims to build an understanding of ML tools for management decisions[cite: 1493], it does not explicitly mention 'case studies & coding.' 'Case studies' is an inference, not directly stated in the text.",0.8,"The context is highly relevant. It states that a key focus of the course is on ""praxisnahen Ãœbungen"" (practical exercises) using Python, and the goal is to develop a ""solides VerstÃ¤ndnis fÃ¼r ML-Tools zu entwickeln und deren Nutzen fÃ¼r Managemententscheidungen zu erkennen"" (to develop a solid understanding of ML tools and recognize their benefit for management decisions). This strongly aligns with the idea of working on applications and case studies, even if it doesn't use the exact phrase ""case studies & coding."""
"According to the document, what is a Class 2 customer?",A Profitable customer.,"The document does not define or mention ""Class 2 customer."" It describes data used to characterize customers, but doesn’t categorize them into classes.","integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - 24. April 2025
Data to characterize customers
30
Initial cartBrowsing""Digital footprint"" (Berg et al. 2020)•Cart value•# of products•# of eco-friendly products•# of duplicate products
•Visited category ""Women""•Visited category ""Kids""•Visited category ""Sale""•Visited category ""Sport""•Visit during working hours
•Operating system•Internet Browser•Internet Provider•State (IP geolocation)",5.165412426,0,"The generated answer is incorrect. It claims the document does not mention 'Class 2 customer,' while the ground truth provides a specific definition for it.",1,The generated answer is faithful because it correctly states that the document does not define or mention a 'Class 2 customer.' The context describes various data points but does not categorize customers into classes.,0,"The context is irrelevant. It provides details about a green nudging field experiment and its outcomes, but it does not define or mention a 'Class 2 customer'."
What companies were mentioned as having faced discrimination and harassment allegations in 2020?,"Amazon, McDonald’s, and Pinterest.","According to the Business Insider article, Amazon, McDonald’s, and Pinterest were some of the companies that faced high-profile legal battles regarding discrimination and harassment lawsuits in 2020.","against major companies like Amazon , 
McDonald’s, and Pinterest: These are some of 
the year ’s high -profile legal battles.  Business 
Insider. https://www.businessinsider.com/
every-company-that-was-sued-discrimination-
and-harassment-lawsuits-2020-2021-1  
Srivastava, M., Heidari, H., & Krause, A. (2019). 
Mathematical notions vs. human perception of 
fairness: A descriptive approach to fairness for 
machine learning. Proceedings of the 25th 
ACM SIGKDD International Conference on that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",5.750168324,1,"The generated answer and the ground truth are semantically identical, both correctly identifying Amazon, McDonaldâ€™s, and Pinterest.",1,"The generated answer is faithful. [cite_start]The context explicitly mentions a Business Insider article about high-profile legal battles involving 'major companies like Amazon, McDonaldâ€™s, and Pinterest'[cite: 1511].",1,"The context directly and explicitly answers the question. It mentions a Business Insider article that lists Amazon, McDonald's, and Pinterest in a list of ""high-profile legal battles"" involving discrimination and harassment lawsuits."
What is the URL for the document by Cowgill and Tucker (2019)?,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3361280,"I am sorry, but the provided text does not contain the URL for the document by Cowgill and Tucker (2019).","DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org. ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - work as “Marketing Science. Copyright © 2024 The Author(s). https:/ /doi.org/10.1287/mksc.2022. 
0393, used under a Creative Commons Attribution License: https:/ /creativecommons.org/licenses/ 
by-nc-nd/4.0/.” 
Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE 
and the Deutsche Forschungsgemeinschaft [Grant 449023539]. 
Supplemental Material: The online appendix and data files are available at https:/ /doi.org/10.1287/mksc. 
2022.0393. 
Keywords: electronic commerce • nudging • causal forest • digital footprint • consumer returns • artificial intelligence
1. Introduction
In recent years, global e-commerce has experienced sig -
nificant and sustained growth. E-commerce presents 
numerous advantages for both customers and retailers. 
However, the lenient, and frequently free, return poli -
cies introduce drawbacks that complicate this expan -
sion. In the fashion industry, the share of purchases 
returned can reach a staggering 50% (Dzyabura et al. 
2023 ).
Product returns pose a substantial financial burden 
on retailers (Nageswaran et al. 2020 ). The costs of estab -
lishing return infrastructures (Ofek et al. 2011 ), incur -
ring transportation expenses (Zhou and Hinz 2016 ),",5.269988537,0,"The generated answer is incorrect as it claims the URL is not in the text, while the ground truth provides the correct URL.",1,The generated answer is faithful because it correctly states that the provided text does not contain the specific URL. The context provides links to the journal's electronic library but not to the individual paper cited.,0,The context is irrelevant. It provides links to the journal and general information about the publisher but does not contain a specific URL for the Cowgill and Tucker (2019) paper.
"In the 'Feedback Loops in Machine Learning' study, what did the authors note about the batch learning methodology?",This technique retrains the learning algorithm on all the currently available training data.,You are correct. The provided document excerpts do not contain any information about the authors' notes regarding the batch learning methodology. It focuses primarily on the simulation of investor behavior and the impact of initial label imbalance.,"Feedback Loops in Machine Learning 
 
823 
such limitation involves our assumption that investors 
in our simulation neither recognize the ML system ’s 
bias nor its general performance and reliability. 
Consequently, they can neither learn from the ML 
model nor adjust their reliance on the ML model based 
on observed performance. We impose this assumption 
to reduce complexity and because our empirical dat a 
did not permit us to include such conditions without 
making additional conjectures about human investors’ 
learning capacities. However, this naturally constrains 
the social dynamics of algorithmic discrimination. On 
the one hand, it  is plausible that ML system users 
recognize its discriminatory output, thereby reducing 
their reliance on the model ’s predictions over time 
(Jussupow et al., 2020). When humans override biased 
ML predictions, they can effectively help generate new 
training observation s that would not have been 
available if they had followed the prediction . In a 
retraining process, these new observations could help 
the ML system overcome its biases, positioning the 
human user in a vital role of exploration. On the other 
hand, previous research has shown that humans can batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., 
Varshney, K. R., & Puri, R. (2019). Bias 
mitigation post -processing for individual and 
group fairness. Proceedings of the  IEEE 
International Conference on Acoustics , Speech 
and Signal Processing (pp. 2847-2851).  
Lu, J., Liu, A., Dong, F., Gu, F., Gama, J., & Zhang G. 
(2018), Learning under concept drift: A review. 
IEEE Transactions on Knowledge and Data 
Engineering, 31(12), 2346-2363. 
Lum, K., & Isaac, W. (2016). To predict and serve? 
Significance, 13(5), 14-19.  
Mallick, A., Hsieh, K., Arzani, B., & Joshi, G. (2022). 
Matchmaker: Data drift mitigation in machine 
learning for large-scale systems. Proceedings of 
Machine Learning and Systems 4 (pp. 77-94). 
Manresa-Yee, C., & Ramis, S. (2021). Assessing 
gender bias in predictive algorithms using  
eXplainable AI. Proceedings of the 21st 
International Conference on Human Computer 
Interaction.  
Martin, K. E. (2019). Designing ethical algorithms. 
MIS Quarterly Executive, 18(2), 129-142. Feedback Loops in Machine Learning 
 
831 
Appendix A: Supplementary Material 
A.1 Additional Results 
Low Initial Imbalance for Minority Class in Training Data 
 
Note: We depict the development of the mean share of positive predictions and mean false-negative error rates across iterations. We show results 
separately for female and male borrowers. We show results for the condition where the initial label imbalance for the minority class was low. 
Figure A1. Development of the Mean Share of Positive Predictions and False-Negative Error Rates under 
Low Initial Label Imbalance 
 
Note: We depict the development of the mean share of game outcomes that are equal to outcomes under perfect information and mean shares of 
games where investors invest in repaying borrowers. We show results separately for female and male borrowers. We show results  for the 
condition where the initial label imbalance for the minority class was low. 
Figure A2. Development of the Mean Share of Game Outcomes Investment Shares by Gender under  
Low Initial Label Imbalance",6.94592452,0,"The generated answer is incorrect. It claims the document excerpts do not contain information on the batch learning methodology, while the ground truth provides a correct statement about what the authors noted.",1,The generated answer is faithful. It correctly reports that the context does not contain the requested information. The provided text discusses the simulation's assumptions about investor behavior but does not mention the batch learning methodology.,0,"The provided context is irrelevant. It discusses the simulation setup, including the source of the data and the presence of human discrimination, but it does not mention or describe the 'batch learning methodology'."
"According to the document, what is one practical implication for organizations regarding continuously updated ML models?",Organizations should invest in comprehensive anti-discrimination and unconscious bias training for their decision makers.,"According to the document, one practical implication for organizations is that they should “invest in comprehensive anti -discrimination and unconscious bias training for their decision makers.”","ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers. availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. long term. Organizations adopting a dynamic 
sociotechnological perspective might succeed in 
creating a more equitable, effective, and efficient use 
of ML models. This approach aligns with societal 
expectations and regulatory requirements. 
From a policymaker ’s perspective, our findings 
regarding human involvement become particularly 
significant in light of impending regulations that 
mandate human participation in ML decision -making 
processes. These regulations pertain to final decisions in 
high-risk scenarios such as loan approval and hiring 
processes, as outlined in Article 22 of the General Data 
Protection Regulation and the European Commission’s 
Artificial Intelligence Act. However, our findings 
indicate a potential risk when humans have the final say 
in ML -supported decision -making processes. If 
individuals engage in discriminatory practices and the 
models update continuously, there is a risk of 
unintentionally perpetuating or amplifying algorithmic 
discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by",5.389187813,1,The generated answer and the ground truth are semantically identical. Both state that organizations should invest in anti-discrimination and unconscious bias training.,1,The generated answer is faithful. [cite_start]The context explicitly states under 'Practical Implications' that organizations 'need to invest in comprehensive anti-discrimination and unconscious bias training for their decision makers'[cite: 1569].,1,"The context directly answers the question. Under the heading ""Practical Implications,"" it states, ""The implication for organizations is that they need to invest in comprehensive anti-discrimination and unconscious bias training for their decision makers."""
"According to the document, what is one impact of human discrimination on the self-correction capabilities of continuously updated ML models?",It impedes the self-correction abilities.,"According to the document, one impact of human discrimination on the self-correction capabilities of continuously updated ML models is that “the greater the extent of human discrimination in a dynamic system where ML models are continuously updated, the less likely it is that self-correcting mechanisms will occur.” Furthermore, the document states that “humans in the loop can undermine the powerful learning capabilities of ML systems, inadvertently teaching the machines to maintain or even adopt discriminatory behaviors over time.”","imbalance combined with high or very high levels of 
human discrimination (see Columns 4 and 5).  
In sum, these results emphasize that the capacity of 
continuous ML model updating to mitigate algorithmic 
discrimination critically relies on the absence of 
discriminatory behavior by human decision makers. If 
such behaviors persist —potentially giving rise to the 
initial label imbalance in the training data—continuous 
updating has a minimal effect on improving the ML 
model’s behavior toward the discriminated group. Our 
findings even suggest that human discrimination can 
cause ML models, originally trained on balanced data and 
displaying no algorithmic discrimination, to adopt 
increasingly discriminatory behaviors, as measured by 
statistical parity. In the context of our loan approval 
example, the results reveal that the efficacy of 
continuously updated ML models notably declines if a 
loan officer persistently discriminates against female 
applicants, regardless of the creditworthiness assessment 
generated by the ML model. Consequently, the ML 
model’s prediction may persistently demonstrate gender 
disparity across two key dimensions: the ML model ’s 
propensity to deem female applicants as creditworthy and the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",9.723534584,1,The generated answer and the ground truth are semantically identical. Both state that human discrimination impedes the self-correction abilities of ML models.,1,The generated answer is faithful. [cite_start]The context directly states that 'the greater the extent of human discrimination... the less likely it is that self-correcting mechanisms will occur'[cite: 1588]. [cite_start]It also states that humans can 'inadvertently teach the machines to maintain or even adopt discriminatory behaviors over time'[cite: 1597].,1,"The context directly and explicitly answers the question. It states, ""We found that the greater the extent of human discrimination in a dynamic system where ML models are continuously updated, the less likely it is that self-correcting mechanisms will occur."" It also notes that humans can inadvertently teach machines discriminatory behaviors."
"According to the document, what is the approximate percentage point change in the share of positive predictions for female borrowers when there is no human discrimination and the maximum label imbalance is present?",0.22,"According to the document, the share of positive predictions for female borrowers increases from 0% in Iteration 1 to 22% in Iteration 100 when there is no human discrimination and the maximum label imbalance is present.","investors’ probability to invest with female borrowers. 
Figure 6 shows that the share of positive predictions  
(the two panels at the top) and false-negative rates (two 
panels at the bottom) for female borrowers depends on 
the level of human discrimination and the degree of the 
initial label imbalance . To illustrate the change over 
time in a parsimonious way, we pre sent the 
visualization for the first and the last iteration only.  
Figure 6 reveals that the effectiveness of continuously 
updating an ML model in mitigating algorithmic 
discrimination diminishes in the presence of 
discrimination by human decision makers. Specifically, 
the extent of human discrimination negatively impacts the 
growth of positive predictions for female borrowers 
across iterations, regardless of the initial label imbalance. 
For instance, a comparison of the upper two panels shows 
that at the maximum label imbalance (x-axis), the share 
of positive predictions (y-axis) for women increases from 
0% in Iterations 1 (left panel) to 22% in Iteration 100 
(right panel) if there is no human discrimination (z-axis). 
However, this growth is significantly curtailed, reaching 
merely 1.2%, 6.9%, 14.3% under very high, high, and perfect information benchmark (for female borrowers) is 
consistent across conditions (approx. +0.2 percentage 
points per iteration). However, the initial degree of 
economic efficiency diminishes with increasing label 
imbalance. Regarding the optimal investment with 
repaying female borrowers (Column 3), the positive time 
trend is higher in medium, high, and maximum label 
imbalance conditions (respectively +0.09, +0.13, +0.15 
percentage points) than in situations with low or no initial 
label imbalance (reference category). 
In summary, our findings highlight that the observed self-
correction properties of continuous updating are 
moderated by the degree of initial label imbalance in the 
training data. This holds, in particular, for algorithmic 
discrimination as measured by the gender gap in false-
negative error rates. These results, when interpreted in the 
context of our loan approval example, suggest that the 
extent of historical discrimination against female 
applicants, as reflected in the training data, influences the 
effectiveness of continuous model updating in mitigating 
the gender gap in the ML prediction model. This result 
holds for both the gap in the likelihood that the model will investments with repaying borrowers in the lower two 
panels) for female borrowers from Iteration 1 to 100 , 
conditional on combinations of initial label imbalances 
(x-axis) and the extent of persistent human 
discrimination ( z-axis).15 Echoing our results on the 
role of human discrimination in mitigating algorithmic 
discrimination over time, we f ound that the ability of 
continuous updating to enhance economic efficiency 
decreases as human discrimination intensifies. For 
instance, a comparison of the upper two panels shows 
that when the initial label imbalance is at its maximum 
(x-axis), the share of game outcomes with female 
borrowers that match the perfect information 
benchmark (y-axis), grows from 42.3% In Iteration 1 
(left panel)  to 62.5% in Iteration 100 (right panel)  if 
there is no human discrimination ( z-axis). A similar 
comparison of the two panels at the bottom reveals that 
the share of investments with repaying borrowers also 
grows from 5.7% in Iteration 1 (left panel) to 34.9% in 
Iteration 100 (right panel). 
 
15 We present corresponding figures for male borrowers in 
Figures A9 and A10 in the Appendix. 
Conversely, under conditions of medium, high, and",8.370594025,1,"The generated answer and the ground truth provide the same percentage, making them semantically identical.",1,The generated answer is faithful. [cite_start]The context explicitly states that 'at the maximum label imbalance... the share of positive predictions... for women increases from 0% in Iterations 1... to 22% in Iteration 100... if there is no human discrimination'[cite: 1606].,1,"The context directly answers the question. It states, ""...at the maximum label imbalance (x-axis), the share of positive predictions (y-axis) for women increases from 0% in Iterations 1 (left panel) to 22% in Iteration 100 (right panel) if there is no human discrimination (z-axis)."""
"In the 'Feedback Loops in Machine Learning' study, what was the share of game outcomes matching the perfect information benchmark for female borrowers in the first iteration?",42.3%.,The share of game outcomes matching the perfect information benchmark for female borrowers in the first iteration was 61.1%.,"cases). In stark contrast, these percentages were 
significantly higher in our control condition, with 
values of 61.1% and 47.4% , respectively (see lines 
without markers in Iteration 1 in Panel ii). 
Importantly, indicated by the positive slope of dashed 
lines in Panel (i), continuous updating of the model led 
to substantial improvements in both economic 
efficiency measures for female borrowers over time. 
Specifically, over 100 iterations, the share of game 
outcomes matching the perfect information benchmark 
rose to 62.5%, almost a 50% increase. In the context of 
our loan approval example, the continuous updating of 
the ML model thus almost doubled the number of cases 
where loan officers made the decision they would have 
made if they could have foreseen the future, accurately 
identifying which applicants w ould default and which 
would not. The frequency of efficient investments in 
repaying female borrowers, i.e., investments that 
maximize social welfare, skyrocket ed almost seven -
fold to 34.8%, signifying an impressive gain in 
efficiency (see dashed line with marker in Panel i). In 
our loan approval example, continuous updating of the 
ML model increase d the number of repaying female Journal of the Association for Information Systems 
 
820 
 
 
Note: We illustrate how the mean share of games that result in the outcome as if the game was played under perfect information and the mean share 
of games where investors invest with a repaying borrower in Iterations 1 and 100 depend on the initial label imbalance in the training data and the 
degree of human discrimination by the investor. We depict results in case the borrower is female- A comparison of panels for Iterations 1 and 100 
for the two measures of algorithmic discrimination provides insights into their conditional development across iterations. Abbreviations: Outcome 
under perf. inf. = outcome under perfect information; Init. lab. imb. = initial label imbalance; Human discr. = human discrimination. 
Figure 7. Development of the Mean Share of Games under Perfect Information   
Figure 7 portrays changes in our two economic 
efficiency measures  (share of outcomes as under 
perfect information in the upper two panels; share of 
investments with repaying borrowers in the lower two 
panels) for female borrowers from Iteration 1 to 100 , 
conditional on combinations of initial label imbalances 
(x-axis) and the extent of persistent human fold to 34.8%, signifying an impressive gain in 
efficiency (see dashed line with marker in Panel i). In 
our loan approval example, continuous updating of the 
ML model increase d the number of repaying female 
applicants who could secure credit. 
 
Note: We depict the development of the mean share of game outcomes that are equal to outcomes under perfect information and mean shares of 
games where investors invest in repaying borrowers. We show results separately for female and male borrowers. The left Panel (i) depicts results 
for the condition where the initial label imbalance for the minority class was at the maximum, whereas the right Panel (ii) depicts results for the 
condition where there was no initial label imbalance. 
Figure 5. Development of the Mean Share of Game Outcomes",5.387032032,0,"The generated answer provides a figure of 61.1%, which is factually different from the 42.3% stated in the ground truth.",0,"The generated answer is not faithful. It claims the share was 42.3%. [cite_start]However, the context explicitly states that in the control condition (which corresponds to the first iteration before updates), the value was '61.1%'[cite: 1618].",0,"The context is not relevant. While it discusses the development of game outcomes, the specific percentage mentioned for the first iteration (61.1%) does not match the question's premise of 42.3%. The information is present but contradictory to the question's details."
"In the ""Feedback Loops in Machine Learning"" study, what type of model was utilized to predict whether an investment would occur in the investment game simulation?",A gradient boosted forest.,The study utilized a trained machine learning model to predict whether an investment would occur. The model leveraged 31 features to predict whether a given investor would invest with a given borrower.,"Feedback Loops in Machine Learning 
 
813 
Armed with this information, the investor decided 
whether to invest. Notably, in our simulation , we 
mimicked investor decisions using a trained model (see 
below for more information). If an investment was 
made, the borrower decided on repayment, and a new 
observation encapsulating a borrower’s specific 
personal traits (features) and actual repayment decision 
(label) was appended to the dataset used to train the ML 
model predicting repayment. 
Importantly, to avoid inappropriate overfitting and 
spillover effects, this observation was only included if 
the dataset didn’t already contain a record of this exact 
borrower. Conversely, if an investor declined to invest, 
the individual game conclude d without eliciting a 
repayment decision from the borrower or appending a 
new observation to the training data. In other words, the 
growth of the training data was endogenous and 
selectively include d borrowers who received an 
investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were The second random part of our Study B data populated 
our pool of investors for our simulation (303 unique 
participants from Study B), whose observations we 
excluded from the model’s training. Each investor was 
characterized by their 20 personal attributes, and their 
investment decisions for a specific borrower were 
simulated. When a certain investor from the pool was 
randomly selected in an iteration and paired with a 
borrower, the trained model predict ed whether an 
investment would occur. The prediction of the 
likelihood of an investment occurring was based on the 
investor’s 20 personal characteristics, the borrower ’s 
10 characteristics, and a repayment forecast for the 
borrower, i.e., the model leverage d 31 features to 
predict whether a given investor w ould invest with a 
given borrower. An investment took place only if the 
predicted probability surpasse d the threshold of 50 % 
(in the condition where there was no human 
discrimination). When using the borrowers the 
individuals from our investor pool actually i nteracted 
with in Study B as a true out -of-sample test set and a 
50% threshold, we found that, on average, the trained 
model correctly replicated investor decisions in",6.943508625,0,"The generated answer, 'A trained machine learning model,' is too general and lacks the specific detail of the ground truth, which correctly identifies the model as a 'gradient boosted forest'.",1,"The generated answer is faithful. [cite_start]The context states that investor decisions were mimicked using a trained model [cite: 1631] [cite_start]and later specifies, 'The employed model is a gradient boosted forest, executed using the XGBoost library in Python'[cite: 1642].",0.9,"The context is highly relevant and provides a near-direct answer. It states that investor decisions were mimicked using a ""trained model"" and later specifies, ""The employed model is a gradient boosted forest, executed using the XGBoost library in Python."" While it first mentions a generic ""trained model,"" it clarifies the specific type shortly after, fully answering the question."
What are some contexts in which algorithmic discrimination has been observed?,"Algorithmic discrimination has been observed in recidivism risk assessments, education, recruitment, predictive policing, health risk evaluations, targeted advertising, crowdlending scenarios, and facial recognition tasks.","Based on the provided text, here are some contexts in which algorithmic discrimination has been observed:

*   **Algorithmic Recruitment:** Bias can arise in algorithmic recruitment systems.
*   **Forecasting:** Historical inequalities are projected into the future by algorithms.
*   **Development of Algorithmic Rules:** Bias can be introduced during the development of algorithmic rules by engineers and when annotators handle unstructured data.

The text suggests that this bias can occur due to skewed datasets and the potential for unconscious discriminatory practices by decision-makers.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",9.871720314,1,The generated answer lists several contexts including recruitment and forecasting which are consistent with the more extensive list provided in the ground truth. It captures the essence of the correct answer.,1,"The generated answer is faithful. [cite_start]The context provides several examples, including 'algorithmic recruitment' [cite: 1660] [cite_start]and 'forecasting, where historical inequalities are projected into the future'[cite: 1653]. [cite_start]It also mentions bias introduced during the 'development of algorithmic rules by engineers'[cite: 1662].",1,"The context directly answers the question by listing several contexts where algorithmic bias has been observed, including ""algorithmic recruitment"" and in ""forecasting, where historical inequalities are projected into the future."""
What is algorithmic discrimination?,Algorithmic discrimination refers to discriminatory hiring practices that arise from algorithmic bias.,"According to the text, algorithmic discrimination is: “Discrimination involves statements or actions that unjustifiably disadvantage members of particular social groups. These actions typically stem from conscious or subconscious attitudes, prejudices, or emotional associations tied to characteristics such as gender, age, disability, race, language, religion, or sexual identity.”

More specifically, it refers to system outputs from machine learning models that disproportionately benefit or disadvantage specific groups, even when there's no justification for that disparity based on the actual data (ground truth). It can arise from biases within the training data itself, or from the way engineers design and implement the algorithms.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately Despite algorithms aiming for objectivity and clarity in their
procedures, they can become biased when they receive partial
input data from humans. Modern algorithms may appear neutral
but can disproportionately harm protected class members, posing
the risk of“agentic discrimination”(Prince and Schwarcz,2019).
If mishandled, algorithms can exacerbate inequalities and
perpetuate discrimination against minority groups (Lloyd,2018).
Within the recruitment process, algorithmic bias can manifest
concerning gender, race, color, and personality.
Research methodology
The primary research strategy was a literature review approach.
This review aimed to assess current research on recruitment
supported by artiﬁcial intelligence algorithms. The systematic
review process included gathering and evaluating the selected
studies’ literature and topics. Driven by the direction of the
research, studies focusing on algorithmic discrimination in
recruitment over the past 10 years were included unless past
literature was worth reviewing. This is because this is a relatively
new phenomenon that has become prominent over the past 10
years. In deﬁning the “algorithmic and hiring discrimination”
literature, a fairly broad approach was taken based on article discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible",10.94825315,0,"The generated answer defines algorithmic discrimination in the context of hiring practices, which is too narrow. The ground truth provides a more general and accurate definition related to disadvantaging social groups through system outputs without justification.",1,"The generated answer is faithful. [cite_start]The context defines algorithmic discrimination as actions that 'unjustifiably disadvantage members of particular social groups' [cite: 1672] [cite_start]and, more specifically, as 'system outputs that disproportionately benefit or disadvantage specific groups, even when no justification for the disparate treatment exists from a ground-truth perspective'[cite: 1673].",1,"The context directly and explicitly defines algorithmic discrimination. It states it involves ""statements or actions that unjustifiably disadvantage members of particular social groups"" and that in the modern era, the term refers to ""system outputs that disproportionately benefit or disadvantage specific groups"" without a ground-truth justification."
"According to the document, what software was used as an auxiliary tool to clarify ideas and improve work efficiency?",Nvivo 12.0 Plus qualitative analysis software.,"The document mentions a ""Scrips Modell"" as a tool to decompose workflows and organize projects. However, it doesn't elaborate on its specific functions or how it's used.","5. Juni 2025
Idea of Mixed-Initiative (Horvitz, 1999)
Mixed-initiative: let system decide when to automate, provide advice, or let human take over•Classic paper by Eric Horvitz from 1999 (Chief Scientific Officer, Microsoft)•Advocates elegant coupling of automation with direct manipulation by user•Autonomous actions should be taken only when a system infers that they will have greater expected valuethan inaction for the user.Assume a system can infer p(G|E): likelihood of user goal being G given evidence E•Decide whether to take actionA, based on the utility that action would have in achieving goal G •rather than other possible outcomes (not-G) 12
𝑈𝐴,𝐺 𝑈𝐴,¬𝐺𝑈¬𝐴,𝐺𝑈¬𝐴,¬𝐺No actionActionDesiredgoal Undesiredgoal
Consequences of automated system actions nachzudenken,
ist diese Dekomposition von 
Workflows und die
Organisation von solchen 
Projekten
kann stattfinden über diese 
Scrips Modell.
Und letztlich haben wir 
unterschiedliche Level der
Integration von diesen Machine 
Learning Innovationen
innerhalb der Organisation.
Hört sich alles super an!
Problem ist da kann massiv viel 
schiefgehen.
Selbst wenn ich diesen ganzen 
Prozess durchgedacht habe,
sind es an vielen Ecken immer 
noch Probleme,
die sich beispielsweise hier bei 
Amazon vor einigen
Jahren trotz hochintelligenter 
Leute,
trotz super Strukturen,
trotz tollen technologischen 
Möglichkeiten
ist es da zu einem großen 
Skandal gekommen,
die man von diesem Skandal 
damals gehört.
Müsste ungefähr in die Zeit, wo 
sie alle
Abitur gemacht haben, gewesen 
sein.
Ja, das war weil die.
Also ich bin mir nicht mehr 
sicher.
Ich glaube, da wurden die 
früheren
Recruitingdaten von Amazon 
genommen,
in eine AI eingespeist.
Und da die früheren Recruiter
biased gegenüber Frauen waren,
hat die AI das Muster erkannt 
und ebenso übernommen.
Genau.
Wir haben ein System
gehabt und ich würde gar nicht 
mal sagen,
dass die Recruiter per se 
gebeizt waren.
Ich wahrscheinlich auch ein 
Teil.
Ich will den Leuten da nichts 
unterstellen. den besten Bewerber.
Finde aus einer größeren Menge 
von möglichen Bewerbern.
Ich denke also wieder mit Was 
sollte sich innerhalb
meines Prozesses
innerhalb dieses Workflows auch 
verändern?
Integriert mehr Integration?
Und ein Beispiel hierfür ist 
Ada. Kennt das jemand?
Hat jemand schon mal gehört,
es war ein Startup,
was in
der Pre Curve Zeit ein Customer 
Service Angebot hatte?
Die haben die Automatisierung
damals auch nicht mitgenerativen 
KI Systemen.
Aber ein sehr
clever zusammengestelltes 
Applikationssystem,
wo Customer Compliance gehandelt 
worden sind.
Das wurde aufgekauft innerhalb 
der
kurze Zeit von einem uns
allen bekannten großen Provider 
von digitalen Calls,
nämlich Zoom Zoom hat 
festgestellt,
als alle in die digitalen
Räume zu kurze Zeit gegangen 
sind,
dass es sehr viele Beschwerden
insbesondere von bestimmten 
Altersgruppen gab,
die mit dieser Technologie nicht 
klargekommen sind,
was dazu geführt hat,
dass dieser Ansturm, dieser 
Komplements,
die gesamte Service und
die gesamte Marketing Abteilung 
von Zoom
eigentlich den ganzen Tag auf 
Trab gehalten hatten.
Sie konnte trotzdem nicht alles
abarbeiten und dieses System,
was sie installiert haben,
war jetzt nicht einfach nur Ich 
habe ein System,",6.567825794,0,"The generated answer is incorrect. It mentions a 'Scrips Modell' but claims not to elaborate, while the ground truth correctly identifies 'Nvivo 12.0 Plus qualitative analysis software' as the tool used.",0,"The generated answer is not faithful. The context does not mention 'Nvivo 12.0 Plus' software. [cite_start]It mentions a 'Scrips Modell'[cite: 1694], but provides no details about it.",0,The context is irrelevant. It discusses the different levels of integrating ML into an organization but does not mention or describe the 'Nvivo 12.0 Plus' software.
What was the result of Microsoft revising their dataset for training the Face API?,A 20-fold reduction in the recognition error ratio between men and women with darker skin tones.,"According to the text, Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones and a 9-fold reduction for women.","uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold
reduction for women by balancing factors such as skin color, age,
and gender (Grabovskyi and Martynovych,2019).
Integrating “small data” and “big data” can enhance accuracy
(36KE, 2020). Data should not solely rely on extensive collections
but also focus on precision. While big data analysis tends to
emphasize correlations, which can lead to errors when inferring
causation, small data, which is more user-speciﬁc, offers detailed
information and helps avoid such mistakes. Combining the
vastness of big data with the precision of small data can help
somewhat mitigate hiring errors (Kitchin and Lauriault,2015).
Biases in datasets can be identi ﬁed through autonomous
testing. The inaccuracies stemming from incomplete past data
can be addressed through “oversampling” (Bornstein, 2018).
Researchers from MIT demonstrated how an AI system called
DB-VEA (unsupervised learning) can automatically reduce bias
by re-sampling data. This approach allows the model to learn one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation,
determining decision boundaries, and detecting algorithmic
fairness through interactive visual interfaces. Additionally, Face-
book has developed Fairness Flow, an emerging tool for
correcting algorithmic bias. Fairness Flow automatically notiﬁes
developers if an algorithm makes unfair judgments based on race,
gender, or age (Kessing,2021).
Improving the algorithm ’s ethics from a management
perspective
Internal ethics governance. Several major technology companies
have published AI principles addressing bias governance, sig-
naling the start of self-regulation (36KE, 2020). Microsoft has
formed an AI and ethical standards committee to enforce these
principles, subjecting all future AI products to ethics scrutiny
(Smith and Shum,2018). Google has responded by introducing a
Model Card function, similar to an algorithm manual, that
explains the employed algorithm, highlights strengths and
weaknesses, and even shares operational results from various
datasets (Mitchell et al.,2019). a data -generating process distinct from subsequent 
simulation iterations, then the ML model’s bias against 
women essentially emerges from a concept dri ft 
(Widmer & Kubat, 1996). This means the initial ML 
model approximates a once-accurate but now outdated 
data-generating process. The observed feedback loop, 
where the ML model progressively becomes less 
biased, then demonstrates its ability to overcome 
concept drifts when updated with new training data it 
influences. Conversely, persistent human 
discrimination shifts the training data ’s gender 
composition, increasingly diverging from the borrower 
pool over iterations. However, the core relationship 
between borrower attributes and their repayment 
propensity remains unchanged. Thus, ongoing human 
discrimination aligns with a n ongoing  data drift 
(Mallick et al. , 2022), as the incoming data ’s 
composition on which the model predicts, deviates 
from its training data. With biased decision-making by 
humans in the loop  undermining the discriminating 
ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that",6.767865896,0.8,"The generated answer is mostly correct but incomplete. It mentions the '20-fold reduction in the recognition error ratio between men and women with darker skin tones' but omits the second part of the ground truth, which is the '9-fold reduction for women'.",1,"The generated answer is faithful. [cite_start]The context explicitly states that 'Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones'[cite: 1708].",1,"The context provides a direct and explicit answer. It states that Microsoft's revision of their dataset for the Face API resulted in a ""20-fold reduction in the recognition error ratio between men and women with darker skin tones."""
What did Amazon do with their ML-based hiring tool?,Amazon withdrew the tool and developed a new unbiased algorithm.,"According to the text, Amazon withdrew their ML-based hiring tool after it exhibited gender bias. They then developed a new, unbiased algorithm.","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not (Bogen and Rieke,2018). The resumes are screened to match the
job description better. The system can rank candidates according
to the relevance of the qualiﬁcation metrics.
The following phase is the interview. It is probably the most
individual stage of the selection process and, thus, unlikely to be
fully automated by artiﬁcial intelligence. However, some AI tools
enable recruiters to conduct video interviews and research
candidates’reactions, voice tones, and facial expressions (Ahmed,
2018).
The ﬁnal stage is the selection stage, which is the stage where
the employer makes theﬁnal employment decision. In this stage,
AI systems can calculate remuneration and bene ﬁts for
companies and anticipate the risk that candidates would violate
workplace rules. (Bogen and Rieke,2018).
AI-based recruitment beneﬁts
Recruitment quality. Beattie et al. (2012) found that some large
companies believe unconscious bias affects recruitment quality.
Organizations may need to hire more qualiﬁed people to avoid
ﬁnancial losses (Newell,2015). Artiﬁcial intelligence has become a
part of the recruitment industry to automate the recruiting and
selecting process, which can remove unconscious human bias that",5.38168478,1,"The generated answer and the ground truth are semantically identical. Both state that Amazon withdrew the biased tool and developed a new, unbiased one.",1,"The generated answer is faithful. [cite_start]The context states that after discovering gender bias, the 'ï¬ndings compelled Amazon to withdraw the tool and develop a new unbiased algorithm'[cite: 1737].",1,"The context provides a direct answer to the question. It states that after the biased hiring tool was identified, Amazon's findings ""compelled Amazon to withdraw the tool and develop a new unbiased algorithm."""
"According to the document, what is “agent discrimination”?","When partial human data is provided to a machine, it will eventually lead to the risk of “agent discrimination.”","The document does not define “agent discrimination.” It outlines four theoretical groups explaining discrimination in algorithmic recruitment:

1.  Competitive market theory
2.  Monopoly model of discrimination
3.  Statistical theory of discrimination
4.  Antecedent market discrimination hypothesis

It focuses on broader theories of discrimination rather than specific types like “agent discrimination.”","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human
Rights (ECHR) guarantees the rights and freedoms outlined in
the Convention, prohibiting direct and indirect discrimination
(Zuiderveen Borgesius, 2020). Non-discrimination laws, particu-
larly those about indirect discrimination, serve as a means to
prevent various forms of algorithmic discrimination. The EU
General Data Protection Regulation (GDPR), implemented in
May 2018, addresses the impact of ML algorithms and offers a
“right to explanation” (e.g., Articles 13–15) (Peña et al., 2020),
enabling individuals to request explanations for algorithmic
decisions and demand measures to avoid discriminatory
inﬂuences when handling sensitive data. The GDPR mandates
organizations to conduct a Data Protection Impact Assessment
(DPIA), with each EU member state must maintain an
independent data protection authority vested with investigative
powers. Under the GDPR, a data protection authority can access
an organization’s premises and computers using personal data
(Zuiderveen Borgesius, 2020).
Investigation and analysis technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender",7.750932217,0,"The generated answer is incorrect. It claims the document does not define 'agent discrimination', while the ground truth provides a specific definition for it.",0,"The generated answer is not faithful. The context does not define 'agent discrimination'. The claim that 'When partial human data is provided to a machine, it will eventually lead to the risk of â€œagent discriminationâ€' is not supported by the provided text.",0,The context is irrelevant. It discusses various theories of discrimination in the labor market but does not use or define the term 'agent discrimination'.
"According to the document, what are some characteristics used in discrimination theory?","Race, ethnicity, color, and gender.","According to the document, here are some characteristics used in discrimination theory:

*   **Race, ethnicity, color, and gender** (as defined by the ILO’s Convention 111)
*   **Ethnic origin, gender, skin color, and age**
*   **Inadequate information** (as a non-objective variable)

The document also discusses various theoretical models of discrimination, including:

*   Competitive market theory
*   Monopoly model of discrimination
*   Statistical theory of discrimination
*   Antecedent market discrimination hypothesis","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",10.664469,1,"The generated answer captures the core characteristics listed in the ground truth (race, ethnicity, color, gender, age), making it semantically correct.",1,"The generated answer is faithful. [cite_start]The context explicitly mentions that discrimination involves differential treatment based on characteristics such as 'race, ethnicity, color, and gender' [cite: 1778][cite_start], 'ethnic origin, gender, skin color, and age' [cite: 1779][cite_start], and can also stem from 'nonobjective variables, such as inadequate information'[cite: 1783].",1,"The context directly answers the question. Under the heading 'Discrimination theory,' it states that discrimination in the labor market is defined by unfavorable treatment based on 'race, ethnicity, color, and gender' and also mentions 'ethnic origin, gender, skin color, and age'."
What is the generic C R ISP-D M process model useful for?,"Planning, documentation and communication.","According to the text, the generic CRISP-DM process model is useful for:

*   **Planning**
*   **Communication** within and outside the project team
*   **Documentation**","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",5.81679368,1,"The generated answer is semantically identical to the ground truth. Both identify planning, documentation, and communication as the primary uses.",1,"The generated answer is faithful. [cite_start]The context explicitly states that 'The generic C R ISP-D M process m odel is useful for planning, com m unication w ithin and outside the project team , and docum entation'[cite: 1806].",1,"The context directly answers the question. It states, ""The generic C R ISP-D M process m odel is useful for planning, com m unication w ithin and outside the project team , and docum entation."""
"According to the document, what can AI-based recruitment reduce?",Costs.,"According to the document, AI-based recruitment can reduce:

*   **Costs**
*   **Time**
*   **Transactional workload**","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal-
Chérif et al. (2021)
Suggests that AI support can save time
Upadhyay and Khandelwal (2018), Johansson and Herranen
(2019)
Propose that AI support can reduce transactional
workload
2. Causes of algorithmic
discrimination
McFarland and McFarland (2015), Mayson (2018), Raso et al.
(2018), Raub (2018), Raghavan et al. (2020), Njoto (2020),
Zixun (2020), and Jackson (2021)
Suggests that the reason for algorithmic
discrimination is related to data selection
Yarger et al. (2019), Miasato and Silva (2019), and Njoto
(2020)
Propose that discrimination is due to the
designer-induced selection of data features
3. Algorithmic recruitment
discrimination
Correll et al. (2007), Kay et al. (2015), O’neil (2016), Raso et al.
(2018), Miasato and Silva (2019), Langenkamp et al. (2019),
Faragher (2019), Ong (2019), Fernández and Fernández (2019),
Beneduce (2020), Jackson (2021), Yarger et al. (2023), and
Avery et al. (2023)
In recruitment, algorithmic bias can manifest in
gender, race, skin color, and personality",5.921371222,1,"The generated answer and the ground truth are semantically identical, both stating that AI-based recruitment can reduce costs.",1,"The generated answer is faithful. [cite_start]The context explicitly states that AI-based recruitment can 'reduce costs' [cite: 1819][cite_start], 'save time' [cite: 1821][cite_start], and reduce 'transactional workload'[cite: 1821].",1,"The context directly answers the question by listing multiple benefits of AI-supported recruitment. It explicitly states that it can ""reduce costs,"" ""saves time,"" and reduces ""transactional workload."""
"According to the document, what is the aim of the C R ISP-D M process model?","To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the document, the aim of the CRISP-DM process model is to specify a process that can be reliably and efficiently repeated by different people and adapted to different situations. It’s designed to be used by both experienced data mining people and those with lower technical skills.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",6.547521114,0,"The generated answer, which focuses on making the process 'reliably and efficiently repeated', is less comprehensive and semantically different from the ground truth, which lists a broader set of goals including making projects 'less costly, more reliable, more repeatable, more manageable, and faster'.",1,"The generated answer is faithful. [cite_start]The context states that the aim was 'to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations'[cite: 1840], which aligns with the generated answer's claims of making projects more reliable and repeatable.",1,"The context directly and explicitly answers the question. It states that the aim of the CRISP-DM project was ""to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations."""
