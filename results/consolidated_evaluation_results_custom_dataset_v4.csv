question,ground_truth_answer,generated_answer,retrieved_context,response_time_seconds,correctness_score,correctness_reasoning,faithfulness_score,faithfulness_reasoning,relevance_score,relevance_reasoning
"In the paper about the Centaur foundation model, what happens to the model's internal representations after being fine-tuned on the Psych-101 dataset?",The model's internal representations become more aligned with human neural activity.,The provided text doesn't contain information about the Centaur foundation model or what happens to its internal representations after being fine-tuned on the Psych-101 dataset. It discusses a different machine learning model and a general approach to understanding how such models work.,"die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen. eher das Modell nehmen, was 
einfacher ist.
Der Gedanke, der dahinter 
steckt,
ist, dass wir vermeiden wollen,
dass die zufälligen Schwankungen 
in den
Daten mit höherer 
Modellkomplexität
in dem Modell erkannt werden als 
wichtige Muster.
Time ist jetzt auch ein bisschen 
drauf gucken,
dass ich meinen Time Management 
richtig betreibe,
weil wie Sie vielleicht gemerkt 
haben,
habe ich die Tendenz, viel zu 
reden.
Kommen Sie gerne nach der 
Vorlesung,
dann können wir darüber 
sprechen.
Wenn wir das verallgemeinern und 
jetzt nicht nur
über unser EiscremeBeispiel 
nachdenken,
sondern ganz, ganz, ganz High 
Level aus einer fast schon.
Und Uwe Walz würde sich freuen,
wenn diese Formel hier angezeigt 
wird.
Wahrscheinlich aus einer 
mikroökonomischen
Perspektive kann man
natürlich fragen Warum brauchen 
wir Vorhersagen?
Und aus einer ökonomischen
Sicht Wenn ich bestimmte 
Entscheidungsprobleme,
Nutzenmaximierungsprobleme,
Gewinnmaximierungsprobleme 
aufschreibe, dann ist immer so
ein bisschen in
fast allen Optimierungsproblemen 
uncertainty mit drin.
Bestimmte Sachen weiß ich 
einfach nicht.
Es kommt so ein bisschen darauf 
an,
was in der Zukunft passieren 
wird.
Wenn ich heute investiere,
wenn ich eigentlich mein",25.35150671,0,"The generated answer is a refusal, claiming the information is not in the text. The ground truth provides a specific, factual answer to the question. The two are semantically opposite.",1,"The generated answer is a faithful refusal to answer. The context, which is in German and discusses general machine learning models for technology adoption, does not contain any of the keywords from the question ('Centaur foundation model', 'Psych-101 dataset').",0,The context is entirely irrelevant. It discusses general concepts of machine learning and a specific model for technology adoption but makes no mention of the 'Centaur foundation model' or the 'Psych-101 dataset.'
"In the study on capturing human cognition, how does the Centaur model's performance compare to existing cognitive models?",Centaur captures the behaviour of held-out participants better than existing cognitive models.,"According to the text, the Centaur model, developed by Venkatesh and colleagues, achieved approximately 70% accuracy in explaining the variation in whether people use technology. The text highlights this as ""massively"" significant, especially within ""so-ciotechnical systems"" where human factors are crucial. 

However, the text doesn't directly compare the Centaur model's performance to *other* existing cognitive models. It simply states that its 70% accuracy is a substantial result. 

","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",10.74526358,0,The generated answer is incorrect. It misidentifies the model's purpose (technology adoption vs. capturing participant behavior) and incorrectly claims that no comparison to other models is made. The ground truth states the opposite: the model performs *better* than existing cognitive models.,0.8,"The generation is mostly faithful, accurately reporting the 70% figure, the 'massive' significance, and the lack of comparison to other models, all of which are in the context. However, it makes one significant unfaithful claim by incorrectly attributing the model developed by 'Venkatesh and colleagues' to be the 'Centaur model'.",0.3,"The context discusses the performance of a model by 'Venkatesh and colleagues' (which the generated answer mistakes for the 'Centaur model'), stating it explains 70% of variation in technology use[cite: 28]. This is tangentially related to the query's subject of model performance. However, the context completely fails to provide the crucial information requested: a comparison to *other existing cognitive models*."
"According to the paper on the Centaur foundation model, what is it designed to predict and simulate regarding human cognition?",It can predict and simulate human behaviour in any experiment expressible in natural language.,"The text doesn't provide information about a ""Centaur foundation model."" It describes the use of a mathematical model of a brain cell (a perceptron) and a modified investment game to simulate economic transactions. It focuses on the ethical and practical challenges of studying human cognition and doesn't detail a specific model like ""Centaur.""","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization robust and widely  accepted tool across multiple 
scientific fields, facilitating the analysis of dynamic 
processes (Kahalé, 2020). Such simulations also 
enable the generation of proofs of concept for novel 
ideas (Wong & Kwong, 2018). We opt for this research 
methodology primarily because the exogenous 
introduction of discrimination, particularly by humans, 
is not only ethically untenable but also practically 
unfeasible in experimental or field settings. 
Our simulation employs a modified version of the 
investment game (Berg et al., 1995). This game 
emulates the incentives and informational structure 
inherent in numerous economic transactions that 
transpire in environments lacking robust enforcement 
mechanisms (Dufwenberg & Kirchsteiger, 2004; Feh r 
& Fischbacher, 2003). In our game, two participants, 
an investor and a borrower, each begin with an 
endowment of 10 monetary units (MU). The investor 
first chooses whether to retain or invest the entire 10 
MU with the borrower. If the investor retains the sum, 
the game ends and both parties keep their initial 
endowment of 10 MU, representing a non -
transactional scenario, e.g., where a bank loan officer 
does not approve a loan. Alternatively, if the investor Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",8.281367302,0,"The generated answer is a refusal, stating the information is not present. The ground truth provides a direct and specific answer about the model's capabilities. The semantics are completely different.",1,"The generated answer is entirely faithful. It correctly states that the context doesn't mention the 'Centaur foundation model' and accurately summarizes the actual content of the context, which is about a perceptron model and a simulated investment game.",0,"The context is irrelevant. It does not mention the 'Centaur foundation model.' Instead, it discusses a 'perceptron' model [cite: 41] and a simulation of an investment game[cite: 44], which are completely different topics from what is asked in the question."
"For the Centaur model, what is the name and scale of the dataset used for fine-tuning to capture human cognition?","Psych-101, which covers trial-by-trial data from over 60,000 participants in 160 experiments.","The text doesn’t provide the name or scale of the dataset used for fine-tuning the Centaur model to capture human cognition. It primarily focuses on the importance of data cleaning and preparation before using machine learning models, particularly in the context of noisy industrial data.","10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization Training von Daten,
das würden wir alles schon 
wissen.
Beziehungsweise es würde uns
aktuell noch nicht 
interessieren.
Da kommen wir zu dem zweiten 
Teil der Vorlesung.
Aber ein Riesenteil.
Bevor wir überhaupt an Modelle 
denken können,
ist es dann, Daten sauber zu 
machen.
Damit werden wir sie noch 
einiges triezen.
Daten, die Sie erhalten,
sind typischerweise insbesondere 
aus der Industrie.
Noisy Bestimmte Observationen 
fehlen.
Dann haben unterschiedliche
Leute unterschiedliche Eingaben 
gemacht,
haben unterschiedliche
Arten und Weisen diese Eingaben 
zu machen.
Manche nutzen Komma, um
Zahlen um decimal points zu 
setzen,
andere nutzen den Punkt.
Die einen schreiben Doppelees, 
die Anschreiben,
scharfes S, Die anderen 
schreiben
groß am Anfang anschreiben, 
klein am Anfang.
Die einen denken das ist eine 
Eins,
die anderen denken das ist eine 
Zwei.
All diese Dinge, wo irrelevante 
Noise sources,
also statistische Fluktuationen,
die nicht zu bedeuten hat.
Das müssen Sie identifizieren.
Sie müssen das sauber machen.
Maschinen, wie wir sehen werden,
können nicht mit 
unstrukturierten
Daten wie Wörtern einfach so 
umgehen.
Wir haben ganz viel 
Datenvorarbeit zu betreiben
und es gibt bestimmte Machine 
Learning Modelle, wo Sie KI einsetzen können,
wann und wo mögliche 
Problemstellungen sind.
Aus diesem ersten Teil hier 
zumindest,
dass Sie uns das strukturiert 
wiedergeben
können und uns zeigen,
dass Sie diese Denkweise 
verstanden haben.
Wenn wir heute dann wie gesagt,
einsteigen und das Thema des 
ersten Blockes abschließen.
Kümmern wir uns jetzt noch mal 
darum,
was eigentlich der Mensch in 
diesem ganzen System
weiterhin noch macht und kümmern 
uns
um diese Mensch Maschine 
Interaktion.
Wenn Sie sich erinnern Sehr früh
in der zweiten Vorlesung haben 
wir festgestellt,
dass die meisten Machine 
Learning Anwendungen
es uns eigentlich erlauben,
Vorhersagen über oder unter 
Unsicherheit zu machen.
Wir kriegen Informationen, die 
uns nicht vorliegen,
und diese Informationen sind
irgendwie nützlich für uns in 
einem Entscheidungsprozess.
Sei es in dem Beispiel,
dass wir eine 
Investitionsentscheidung
treffen möchten und uns
dafür überlegen oder eine 
Vorhersage benötigen.
Wird diese 
Investitionsentscheidung, wie in
diesem Beispiel eine hohe
Rückzahlung für mich ergeben 
oder eine
niedrige Rückzahlung?
Mit dieser Information kann ich 
mir dann überlegen,
ob ich diese Investition
tatsächlich machen möchte oder 
eben nicht.",7.962281466,0,The generated answer is a refusal based on the retrieved context. The ground truth provides the specific name and scale of the dataset. The two are semantically opposite.,1,"The generated answer is completely faithful. It correctly identifies that the context lacks the requested information about the 'Centaur model' and accurately describes the actual topic of the provided text, which is the general importance of data cleaning.",0,"The context is entirely irrelevant as it does not mention the 'Centaur model' or any specific dataset for fine-tuning. It discusses general data preparation challenges in machine learning[cite: 61, 62, 68], which is not what the question is about."
In the paper 'A foundation model to predict and capture human cognition' what is the name of the computational model introduced?,Centaur.,The paper introduces a mathematical model of a brain cell: a **perceptron**.,"10. Juli 2025
A mathematical model of a brain cell: perceptron
10
#𝑦!=𝑓%""𝑤""⋅𝑥""+𝑏
Weighted linear combination of inputsOutput
Non-linearization(activation function) Intercept
Representation   +Evaluation+Optimization 12. Juni 2025
Please answer the following questions:(a) Label the components of the following equation according to our definition in the lecture. (b) Explain each component in 1-2 short sentences in your own word. (c) Based on the components, explain the overarching goal of machine learning models.
4
Learned modelFrom historic data
The objective of ML is to make predictions !𝒚given new data points 𝒙for which we do not know the true outcome 𝒚
!𝑓(𝑥) &𝑦Input information:Tabular dataImagesText…
Prediction:Loan defaultStock priceCustomer retention…
=
?
? ? Sie sind ein Machine Learning 
Modell.
Kommen wir jetzt zur heutigen 
Vorlesung.
Immer mit diesem Gedanken im 
Kopf.
Wir haben Inputdaten, wir haben
ein Modell und dieses Modell
macht auf den Inputdaten 
letztlich uns eine Vorhersage,
die möglichst präzise ist für 
Daten,
die Sie noch nicht gesehen 
haben.
Fangen wir jetzt wirklich an,
in diesem Teil eins rein zu 
gehen, um zu verstehen,
was können diese Modelle
eigentlich liefern aus einer 
ökonomischen,
betriebswirtschaftlichen, 
gesellschaftlichen Perspektive?
Wir werden jetzt in den ersten
Vorlesungen und es wird so
bis zur fünften Vorlesung gehen.
Da werden wir langsam anfangen,
auch in den zweiten Teil 
überzugehen.
Aber zunächst mal interessiert 
uns gar nicht,
wie wir auf dieses Modell 
kommen,
was dieses Modell genau ist.
Wir werden erstmal annehmen,
da ist irgendein Modell,
das fällt sozusagen vom Himmel.
Und was macht dieses Modell mit 
uns?
Was macht dieses Modell mit 
Organisationen?
Was macht dieses Modell mit 
Personen?
Was macht dieses Modell mit der 
Gesellschaft?
Und um so ein bisschen
eine Hands on Vorstellung zu 
haben,
was Machine Learning Modelle 
machen,
habe ich hier mal ein Beispiel 
mitgebracht,
wo wir jetzt als Gruppe Machine 
Learning Modell spielen.",4.599955559,0,The generated answer provides the incorrect name for the model ('perceptron'). The ground truth is 'Centaur'. The answer is factually incorrect.,1,The generation is completely faithful. The context explicitly contains a slide with the text 'A mathematical model of a brain cell: perceptron'.,1,"The context directly answers the question. A slide image explicitly states 'A mathematical model of a brain cell: perceptron'[cite: 78], which provides the name of the computational model discussed."
The paper 'Algorithmic Bias?' concludes that optimizing for what factor can lead to apparently discriminatory outcomes in ad delivery?,An algorithm that simply optimizes for cost-effectiveness.,"According to the paper, apparently discriminatory outcomes in ad delivery can be caused by nonobjective variables, such as inadequate information.","that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that 15. Mai 2025
Consequences of algorithmic biasIndividual consequences include•rejection or discontinuance of ML tools.•customers paying higher prices than usual Organisational consequences include •violating equal opportunity policies•high customer churn rate Societal-level consequences include •increased gaps between historically disadvantaged groups and others•Fostered polarization and human discrimination
17 significant consequences, such as in healthcare, loan 
approval, hiring, or criminal justice systems. 
Mitigating algorithmic discrimination : The 
expansive literature on algorithmic discrimination 
proposes various methods to identify and alleviate 
such biases. According to d ’Alessandro et al. (2017), 
we can divide these mitigation strategies into three 
categories: pre -processing, in -processing, and post -
processing. Pre -processing strategies strive to 
eliminate disparities inherent in the dataset. Methods 
such as those proposed by Kamiran and Calders (2012) 
and Zafar et al. (2017) employ data pre -processing 
techniques like feature massaging and reweighting to 
balance the dataset based on social group 
characteristics. In -processing strategies involve 
conventional learning algorit hms during model 
training. Researchers often use techniques like naive 
Bayes and decision tr ees for debiasing (Calders & 
Verwer, 2010; Zhang et al., 2018). Additional research 
suggests placing constraints on the classification 
model to mandate the satisfaction of a proxy 
(Woodworth et al., 2017). Lastly, post -processing 
strategies aim to adjust the output to counter biases. 
For example, Corbett -Davies et al. (2017)",5.375768423,0.3,"The generated answer ('nonobjective variables, such as inadequate information') is a different and less specific explanation than the ground truth ('optimizes for cost-effectiveness'). While both relate to causes of bias, they are not semantically the same concept.",1,"The generation is faithful. The context mentions 'nonobjective variables, such as inadequate information' as contributing to biased outcomes. While not a direct match to the ground truth ('cost-effectiveness'), it is a plausible and supported claim based on the provided text.",1,"The context directly addresses the question by explaining statistical discrimination theory. It states that discriminatory outcomes can be considered 'efï¬cient under imperfect information and aligns with the employerâ€™s proï¬t maximization goal'[cite: 99], directly implying that optimizing for cost-effectiveness or profit can lead to such outcomes."
"In the context of the STEM career ad study, what was the difference in click behavior between men and women who were shown the ad?","Among those who saw the ad, women were more likely to click on it than men.","According to the Lambrecht & Tucker (2019) study, “Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of STEM career ads,” women clicked on STEM career ads less frequently than men. Specifically, they found that women clicked on the ads 34% less often than men.","Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)
from an organizational behavior perspective. While recent studies
have primarily discussed discrimination theory in the traditional
economy’s hiring market, future theoretical research should
consider how advanced technology affects equity in hiring within
the digital economy.
Conclusion
The study concludes that the fo urth industrial revolution
introduced technological innovations signiﬁcantly affecting the
recruitment industry. It extends the analysis of statistical dis-
crimination theory in the digital age and adopts a literature
review approach to explore four themes related to AI-based
recruitment. The study argues that algorithmic bias remains an
issue while AI recruitment tools offer beneﬁts such as improved
recruitment quality, cost reduction, and increased ef ﬁciency.
Recruitment algorithms ’bias is evident in gender, race, color,
and personality. The primary source of algorithmic bias lies in
partial historical data. The personal preferences of algorithm
engineers also contribute to algorithmic bias. Technical mea-",9.020519972,0,"The generated answer is the semantic opposite of the ground truth. It claims women clicked *less* frequently, whereas the ground truth states women were *more* likely to click.",0,"The generation is not faithful. It makes a specific claim about women clicking on ads 34% less often than men, but the provided context is only a list of bibliographic references and contains no such data or statement.",0,"The retrieved context consists only of a list of bibliographic references. While it correctly cites the paper by Lambrecht & Tucker (2019) relevant to the question[cite: 112, 130], it provides no information, abstract, or text from the paper itself. Therefore, it is impossible to answer the question based on this context."
"In the empirical study of gender-based discrimination in STEM ad display, what reason is given for why fewer women saw the advertisement?","Younger women are a prized demographic and are more expensive to show ads to, so a cost-optimizing algorithm shows the ad to them less often.","According to the Lambrecht & Tucker (2019) study, the reason given for fewer women seeing the STEM career ads was apparent gender-based discrimination in the algorithmic display.","Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., present randomization checks for the predictors in 
Table 5 in Online Appendix A.2.
The initial customer cart and browsing data, including 
product types, quantities added to the cart, and visited 
shop categories, offer valuable insights into customer 
characteristics and preferences. We focus on the catego -
ries “Women,” “Kids,” and “Sale” (discounted pro -
ducts). The “Women” category likely indicates female 
customers, whereas the “Kids” category suggests infor -
mation about the customer’s age and family status. Fre -
quent choices from the “Sale” category may indicate 
price sensitivity. We also examine the initial cart’s value, 
the total number of products, the count of products 
branded as eco-friendly, and the presence of duplicate 
items varying in size or color. Duplicate items in the first- 
view cart could indicate bracketing—a premeditated 
plan to return one or more items. Furthermore, customer 
responses to green nudging may differ based on whether 
they shop during weekends or weekdays as other online 
shopping studies have shown (Grahl et al. 2023 ).
Individuals’ digital footprints contain meaningful 
information (Berg et al. 2020 ). In our CML model, we use Das hat Google jetzt anscheinend 
durch Filter so gemacht,
dass wenn man Krankenpfleger 
eingibt,
dass sowohl Männer und Frauen 
gezeigt werden.
Aber bis vor zwei Jahren war
es immer noch ein sehr gutes 
Beispiel.
Wenn ich Sie frage, googeln Sie 
mal bitte nach Images.
Dann war Krankenpfleger zu 99 %
weiblich und Arzt zu 99 % 
männlich,
obwohl diese Bilder natürlich 
von beiden
eigentlich schon vorhanden sein 
sollen.
Das heißt, wir haben hier
irgendwelche gewachsenen 
technischen Grundlagen,
Datenbanken, die das einfach 
noch nicht abbilden können.
Having said that, die 
Unternehmen sind
sehr sensibilisiert durch diese 
ganzen PR Probleme,
die sie damit erlitten haben und 
haben angefangen,
diese Daten zu bereinigen.
Aber das erfordert eben massiv 
viel Investitionen
in Datenbereinigung,
wo wir wieder in dem 
Kriegsmodell,
bei dem Data Preparation sind.
Sie müssen sich klar sein Wie 
sind
Ihre Strukturen der Daten 
eigentlich aufgebaut?
Soweit ich weiß es auch manchmal
das Labeling von Menschen 
gemacht.
Richtig? Genau. Es könnte so 
sein,
dass wir haben eine Gruppe von 
Menschen,
die nicht politisch korrekt 
denken.
Und dann würden sie es bewusst 
falsch.
Richtig. Auch ein massives 
Problem.
Wir haben auf der einen Seite",7.164181948,0.1,"The generated answer is vague and tautological ('discrimination was caused by discrimination'), failing to capture the specific economic reason provided in the ground truth (that younger women are more expensive to advertise to).",1,The generation is faithful. The claim that the reason for fewer women seeing the ads was 'apparent gender-based discrimination' is directly supported by the title of the cited paper in the context: 'an empirical study of apparent gender-based discrimination in the display of stem career ads'.,0,"The context is a disjointed collection of sources. While it references the relevant Lambrecht & Tucker paper[cite: 139], it doesn't provide any content from it. The other pieces of context, discussing gender bias in Google image searches [cite: 159, 160] and customer demographics in e-commerce[cite: 150, 151], are completely unrelated to the specific STEM ad study in question."
"Regarding the display of STEM career ads, what was the empirical result of the ad's delivery in the 'Algorithmic Bias?' paper?",Fewer women saw the ad than men.,"According to the provided text, the empirical result of the ad’s delivery in the “Algorithmic Bias?” paper (Lambrecht & Tucker, 2019) was that apparent gender-based discrimination was evident in the display of STEM career ads. Specifically, the study found that women were less likely to see ads for STEM careers compared to men.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)
from an organizational behavior perspective. While recent studies
have primarily discussed discrimination theory in the traditional
economy’s hiring market, future theoretical research should
consider how advanced technology affects equity in hiring within
the digital economy.
Conclusion
The study concludes that the fo urth industrial revolution
introduced technological innovations signiﬁcantly affecting the
recruitment industry. It extends the analysis of statistical dis-
crimination theory in the digital age and adopts a literature
review approach to explore four themes related to AI-based
recruitment. The study argues that algorithmic bias remains an
issue while AI recruitment tools offer beneﬁts such as improved
recruitment quality, cost reduction, and increased ef ﬁciency.
Recruitment algorithms ’bias is evident in gender, race, color,
and personality. The primary source of algorithmic bias lies in
partial historical data. The personal preferences of algorithm
engineers also contribute to algorithmic bias. Technical mea-",9.566085339,1,The generated answer and the ground truth are semantically identical. Both state that fewer women saw the ad compared to men.,1,The generation is faithful. The claim that there was 'apparent gender-based discrimination' is directly supported by the title of the cited paper in the context: 'an empirical study of apparent gender-based discrimination in the display of stem career ads'. The claim that women were less likely to see the ads is a direct and accurate summary of this finding.,0.8,"The context is highly relevant. It repeatedly cites the correct paper on algorithmic bias in STEM ad display[cite: 180, 182]. More importantly, it includes a conclusion stating that 'Recruitment algorithms â€™bias is evident in gender...'[cite: 197]. While it doesn't give the specific finding that 'fewer women saw the ad,' it directly confirms the core empirical result of gender-based bias, which is the essence of the question."
"In the study on gender-based discrimination in STEM career ads, what was the explicit intention for the ad's delivery?",The ad was explicitly intended to be gender-neutral in its delivery.,"According to the provided text, the study on gender-based discrimination in STEM career ads aimed to **empirically study apparent gender-based discrimination in the display of STEM career ads.** 

Specifically, Lambrecht & Tucker (2019) investigated whether algorithms were displaying gender-based biases in the presentation of STEM career advertisements.","Journal of the Association for Information Systems 
 
828 
Köchling, A., & Wehner, M. C. (2020). Discriminated 
by an algorithm: a systematic review of 
discrimination and fairness by algorithmic 
decision-making in the context of HR 
recruitment and HR development. Business 
Research, 13(3), 795-848.  
Kusner, M. J., & Loftus, J. R. (2020). The long road to 
fairer algorithms. Nature, 578(7793), 34-36. 
Lahey, J. N. (2008). Age, women, and hiring an 
experimental study. Journal of Human 
Resources, 43(1), 30-56.  
Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., 
& Mullainathan, S. (2017). The selective labels 
problem: Evaluating algorithmic predictions in 
the presence of unobservables. Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining  (pp. 
275-284).  
Lakkaraju, H., & Rudin, C. (2017). Learning cost -
effective and interpretable treatment regimes. 
Proceedings of the 20th International 
Conference on Artificial Intelligence and 
Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981. Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D., REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",8.864394426,0,"The generated answer describes the intention of the *study* (to investigate discrimination), while the ground truth describes the intended *ad delivery strategy* (to be gender-neutral). These are different concepts, and the generation fails to answer the question correctly.",1,The generation is faithful. The claim that the study aimed to 'empirically study apparent gender-based discrimination in the display of STEM career ads' is directly verifiable from the title of the paper by Lambrecht & Tucker (2019) which is cited multiple times in the context.,0.2,"The context is only slightly relevant. It provides the title of the correct paper, which is 'an empirical study of apparent gender-based discrimination in the display of stem career ads'[cite: 208, 210]. This title hints at the *purpose of the research*, but it does not state the *explicit intention for the ad's delivery* itself (i.e., whether the advertiser intended it to be neutral). The rest of the context is from a different paper on AI recruitment[cite: 219]."
The findings of the 'Feeback Loops in Machine Learing' paper underscore the complex nature of algorithmic discrimination as what kind of phenomenon?,A sociotechnological phenomenon.,"According to the text, the findings underscore algorithmic discrimination as a **dynamic and sociotechnological phenomenon** – specifically, one that can diminish or emerge due to the interaction between ML systems and human users in environments where continuous model updating is essential. It highlights how humans can inadvertently teach machines to maintain or even adopt discriminatory behaviors over time.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",8.524200916,1,The generated answer and the ground truth are semantically consistent. Both identify the phenomenon as 'sociotechnological'.,1,The generation is faithful. The key claimsâ€”that algorithmic discrimination is a 'dynamic and sociotechnological phenomenon' and that 'humans can inadvertently teach machines to maintain or even adopt discriminatory behaviors'â€”are both explicitly stated in the provided context.,1,"The context directly and explicitly answers the question. It contains the sentence: 'Theoretically, our findings underscore the sociotechnological nature of algorithmic discrimination'[cite: 238]."
What type of setting was used in the simulations of the 'Feedback Loops in Machine Learning' paper to examine the ML model's evolution?,"A setting that mirrors sequential interactions, such as loan approval decisions.","The simulations in the “Feedback Loops in Machine Learning” paper were based on a setting that mirrored sequential interactions, such as loan approval decisions. Specifically, they used data from an incentivized investment game involving students (Study A) and investors (Study B) who interacted with an ML model.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. Feedback Loops in Machine Learning 
 
805 
generate predictions (Parisi et al., 2019). However, in 
dynamic, nonstationary environments, the underlying 
data-generating process that ML models aim to 
approximate can change over time. This phenomenon, 
referred to as drift (Lu et al., 2018), can lead to  
progressively poorer predictions. The decrease in 
prediction performance can originate from changes in 
the underlying relationship between the input features 
and the target variable —concept drift (Widmer & 
Kubat, 1996)—or changes in the distributions of t he 
input features or the target variable—data drift (Mallick 
et al., 2022). As a consequence, to maintain or even 
improve prediction accuracy over time, it is imperative 
to continuously update ML models on recent data that 
reflect new patterns in the data -generating process 
(Gama et al., 2014). 
The efficacy of continuous updating inherently links to 
the quality and structure of the collected data. Ideally, 
updating processes should empower models to learn 
from their past mistakes —that is, identify cases for 
which they generate inaccurate predictions. However, 
ML models that support or automate human decisions 
typically do more than merely provide neutral Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were",8.254996777,1,"The generated answer and the ground truth are semantically identical. Both state that the simulation setting mirrored sequential interactions, such as loan approval decisions.",1,The generation is faithful. All claims made in the generated answerâ€”that the setting mirrored sequential interactions like loan approval and that it used data from an incentivized investment game with students and investorsâ€”are explicitly stated in the provided context.,1,"The context directly answers the question. It states that the paper uses 'comprehensive simulations... in a setting that mirrors sequential interactions, such as loan approval decisions'[cite: 247]. It also specifies the simulations were built on data from an 'incentivized investment game'[cite: 259, 261]."
What is the key finding regarding the continuous updating of ML models?,Continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time.,"According to the text, a key finding is that continuous updating of ML models can help mitigate algorithmic discrimination and enhance economic efficiency *over time*. However, the text also highlights a significant caveat: human decision-makers who override ML predictions can actually impede self-correction and even induce initially unbiased models to become discriminatory over time.","availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers. generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical 
exploration of the long -term dynamic effects of 
continuously updating ML models  is lacking , 
particularly in terms of the role of human decision-
making based on these predictions. We aim to fill this 
research gap. Regarding contexts where a continuously 
updated ML system both supports human decision -
making and shapes the data available for updating, we 
pose the following questions: 
RQ1: How does continued updating affect the fairness 
and economic efficiency of an ML model that 
initially discriminates against a certain social 
group due to a label imbalance in the training 
data? 
RQ2: How does persistent discrimination by human 
decision makers against a certain social group 
affect the impact of continuous updating on 
model fairness and economic efficiency?
 
 
Figure 1. Process Where Machine Learning Predictions Endogenously Influence the Availability of Data 
for Future Retraining of the Model",7.415048122,1,The generated answer captures the essential information of the ground truth. Both state that continuous updating can mitigate discrimination and improve economic efficiency.,1,"The generation is completely faithful. Every claim made in the generated answer is explicitly stated in the context, including that continuous updating can mitigate discrimination but that human overrides can impede this self-correction.",1,"The context directly answers the question by stating, 'Our findings indicate that continuous updating can help mitigat e algorithmic discrimination and enhance economic efficiency over time'[cite: 270]. It also includes the important nuance that humans in the loop can impede this process[cite: 271]."
How do ML models endogenously shape their future training data?,"By influencing human decisions, which then generate new data points (or a lack thereof) for future model training.","According to the text, ML models endogenously shape their future training data by:

*   **Generating new data based on their predictions:** The models’ predictions themselves create new data points that are then used to update the model.
*   **Reinforcing discriminatory patterns:** When human decision-makers override the model’s predictions (especially if those predictions are discriminatory), this feedback loop can cause the model to learn and perpetuate those discriminatory patterns over time.","endogenous composition of future training data, a 
problem referred to as selective labeling (Lakkaraju et 
al., 2017; Little & Rubin, 2019), can maintain or even 
exacerbate inaccurate or discriminatory predictions. 
In this paper, we explore the dynamic interplay 
between the continuous updating of ML models and 
algorithmic discrimination when ML predictions 
endogenously shape the composition of additional data 
available for model updating. In particular, our work 
focuses on the role of ongoing discrimination by 
human decision  makers who have the final say in an 
ML-supported decision-making task. A steady stream 
of previous work has examined how algorithmic 
discrimination can reproduce and perpetuate 
discriminatory patterns ingrained in societal structures 
(Lambrecht & Tucker, 2019; Morse et al., 202 2; 
Teodurescu et al., 2021). Some studies have developed 
theoretical arguments concerning the emergence of 
feedback loops when ML models continually learn 
from the discriminatory outcomes they help ed to 
generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. endogenously help generate, can enhance 
performance, alleviate algorithmic discrimination, and 
boost the economic efficiency of ML models initially 
grappling with label imbalances in the original training 
dataset. The ability for self -correction inversely 
depends on the degree of initial label imbalance: the 
more biased the ML model ’s predictions are at the 
outset, the le ss capacity it has for recovery through 
continuous updating over time. Crucially, our research 
indicates that persistent discrimination by humans —
who make the final decisions in ML-supported tasks—
significantly impedes the self -correction abilities 
associated with the continuous updating of ML 
models. Our findings even suggest that ongoing human 
discrimination can lead ML models, originally trained 
on balanced data and demonstrating no algorithmic 
discrimination, to exhibit increasingly discriminatory 
behavior over time. 
5.2 Theoretical Contributions and 
Implications  
Our study responds to recent calls for empirical 
investigations into the sociotechnological nature of 
algorithmic discrimination in environments where ML 
models continue to learn based on data they help to 
generate (e.g., Kordzadeh & Ghasemaghaei, 2022;",9.360168219,1,The generated answer and the ground truth are semantically consistent. Both explain that ML models shape future data by influencing decisions which then generate the data for subsequent model training.,0.5,"The generation is only partially faithful. The second claim about 'Reinforcing discriminatory patterns' through human overrides is supported by the context. However, the first claim that models shape data by 'Generating new data based on their predictions' is not explicitly stated. The context says the *outcomes of algorithmic decisions* provide new data, which is a more nuanced concept that involves the human response to the prediction, not just the prediction itself.",1,The context directly explains the mechanism. It states that feedback loops 'arise when the outcomes of algorithmic decisions provide new endogenous training data for subsequent model updates'[cite: 287]. It further clarifies that this happens when ML predictions 'endogenously shape the composition of additional data available for model updating'[cite: 284].
What effect can human decision-makers have on the self-correction of discriminatory models when they are kept in the loop?,Human decision-makers in the loop may impede the self-correction of discriminatory models and can even cause initially unbiased models to become discriminatory over time.,"According to the text, human decision-makers can negatively impact the self-correction of discriminatory models when they are kept in the loop. Specifically, biases in human decision-making can directly influence the behavior of continuously updated ML models, and humans can interfere with the endogenous creation of new observations, further reducing the chance that the existing training data can be supplemented with observations enabling the model to make better predictions for the initially disadvantaged group.","discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by 
organizational decision makers. These measures could 
serve as essential complementary elements to ensure that 
continuously updated ML models demonstrate desirable 
behaviors when organizations implement regulatory 
requirements that involve humans in the decision loop. 
5.4 Limitations and Future Research  
Like any study, this study also has limitations—which, 
we believe , provide valuable directions for future 
research to enhance our understanding of how 
algorithmic discrimination evolves in environments 
where ML models undergo continuous updates. One the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” Journal of the Association for Information Systems 
 
824 
Indeed, self-correcting mechanisms may require time 
to take effect, during which disadvantaged social 
groups may continue to endure discriminatory 
practices. Rather, we perceive our findings as a 
testament to the substantial learning capabilities of 
modern ML models undergoing continuous updating. 
We argue that these capabilities can complement other 
interventions aimed at addressing algorithmic 
discrimination, such as the proper preprocessing of 
training data. This approach is particularly effective 
when organizations take measures to ensure that their 
employees do not engage in (subconscious) 
discriminatory practices during ML -supported 
decision-making processes. However, overreliance on 
the self -correcting aspects of continuous updating 
processes could cause significant damage to both an 
organization’s economic performance and reputation if 
ML systems are deployed hastily without rigorous ly 
testing for and addressing algorithmic discrimination.",9.310372114,1,The generated answer and the ground truth are semantically identical. Both state that human decision-makers can impede self-correction and even cause unbiased models to become biased.,1,The generation is faithful. Every claim made is directly supported by the provided text. The context explicitly states that human biases can 'impede the self-correcting capacities of ML models' and that 'unbiased models learn biased behaviors from humans over time'.,1,The context explicitly answers the question. It states that 'the greater the extent of human discrimination... the less likely it is that self-correcting mechanisms will occur ' [cite: 303] and that 'unbiased models learn biased behaviors from humans over time'[cite: 304].
What is a key challenge with opaque algorithmic designs in recruitment?,"They hinder the detection and correction of biases, making it difficult for HR teams to ensure equitable hiring.","According to the text, a key challenge with opaque algorithmic designs in recruitment is that they can amplify future hiring inequalities. The text states: ""Predicting future hiring outcomes by observing historical data can amplify future hiring inequalities."" This suggests that if the data used to train the algorithm reflects existing biases, the algorithm will perpetuate and potentially worsen those biases in its hiring decisions.","technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data
selection. Data collection tends to prefer accessible,“main-
stream” organizations unequally dispersed by race and
gender. Inadequate data will screen out groups that have
been historically underrepresented in the recruitment
process. Predicting future hiring outcomes by observing
historical data can amplify future hiring inequalities. Yarger
et al. (2019), Miasato and Silva (2019), and Njoto (2020)
propose that discrimination is due to the designer-induced
selection of data features.
3. The third theme is which algorithmic recruitment dis-
crimination exists. According to Correll et al. (2007), Kay
et al. (2015), O’neil (2016), Raso et al. (2018), Miasato and
Silva (2019), Langenkamp et al. (2019), Faragher (2019),
Ong (2019), Fernández and Fernández (2019), Beneduce
(2020), Jackson (2021), Yarger et al. (2023), and Avery et al.
(2023), when partial human data is provided to a machine,
so the algorithm is biased, it will eventually lead to the risk",8.459100008,0,"The generated answer is incorrect. It suggests that opaque designs 'amplify future hiring inequalities', which is a consequence, not the core challenge itself. The ground truth correctly identifies the challenge as hindering the *detection and correction* of these biases.",1,The generation is faithful. The claim that 'Predicting future hiring outcomes by observing historical data can amplify future hiring inequalities' is stated verbatim in the provided context.,0.8,"The context is highly relevant but does not provide a complete answer. It explains that historical data can 'amplify future hiring inequalities' [cite: 335] and that data collection can prefer '""main-streamâ€ organizations unequally dispersed by race and gender'[cite: 333]. This strongly implies that opacity is a problem for detecting bias but doesn't state it directly in relation to HR teams."
"According to some studies cited, what are two benefits of AI-supported recruitment?",It can reduce costs and save time.,"According to the provided text, two benefits of AI-supported recruitment are:

1.  **Reduced costs**
2.  **Saves time**","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly",5.895046949,1,"The generated answer and the ground truth are semantically identical, both listing cost and time savings as benefits.",1,The generation is faithful. The context explicitly lists studies that 'state that it can reduce costs' and others that 'suggest it saves time'.,1,The context directly answers the question. It explicitly states that some studies found AI-supported recruitment 'can reduce costs' [cite: 345] and other studies 'suggest it saves time'[cite: 347].
What are two recommended categories of measures to mitigate algorithmic bias in recruitment?,Technical measures (like unbiased dataset frameworks and improved transparency) and management measures (like internal ethical governance and external oversight).,"According to the text, the two recommended categories of measures to mitigate algorithmic bias in recruitment are:

1.  **Technical measures:** This includes things like “unbiased dataset frameworks and improved algorithmic transparency.”
2.  **Management measures:** This includes “internal corporate ethical governance and external oversight.”","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender Theme IV. How decreasing algorithmic recruitment
discrimination
Changes should be made at the technical and regulatory levels to
ensure that AI algorithms do not replicate existing biases or
introduce new ones based on the provided data (Raub,2018).
Building fair algorithms from a technical perspective
Constructing a more unbiased dataset. Unfair datasets are the root
cause of bias. Therefore, a direct approach to addressing algo-
rithmic bias is reconﬁguring unbalanced datasets. Using multiple
data points can yield more accurate results while carefully elim-
inating data points that re ﬂect past biases. However, this
approach incurs signiﬁcant costs (Bornstein,2018).
Another method is to correct data imbalances by using more
equitable data sources to ensure fair decision-making (36KE,
2020). Understanding the underlying structure of training data
and adjusting the signi ﬁcance of speci ﬁc data points during
training based on known latent distributions makes it possible to
uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold",7.201745033,1,The generated answer and the ground truth are semantically identical. Both list technical measures (like unbiased datasets and transparency) and management measures (like ethical governance and oversight).,1,"The generation is faithful. Both claims are explicitly stated in the context: 'To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight.'",1,"The context directly answers the question. It recommends implementing 'technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight'[cite: 367]."
"Besides gender and race, what other characteristic is mentioned as a basis for discriminatory hiring practices by AI?",Personality traits.,"According to the text, personality traits are also mentioned as a characteristic that can lead to discriminatory hiring practices by AI systems.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- Thirdly, concerns are raised regarding potential hiring
discrimination perpetrated by machines. This can arise from AI
tools, such as partial source data, or users unfamiliar with user
interfaces and operations.
Fourthly, intrinsic factors like personality and IQ, as well as
extrinsic factors like gender and nationality, have been observed
to inﬂuence the accurate identi ﬁcation and judgment of AI
systems concerning hiring discrimination.
Fifthly, respondents offer recommendations for combating
discrimination by machines, including technical and non-
technical approaches.
Recommendations for future studies
This study conducted a literature review to analyze algorithmic
recruitment discrimination’s causes, types, and solutions. Future
research on algorithmic recruitment discrimination could explore
quantitative analysis or experimental methods across different
countries and cultures. Additionally, future studies could examine
the mechanics of algorithmic recruitment and the technical rules
that impact the hiring process. It would be interesting to analyze
the psychological effects of applying this algorithmic recruitment
technique on various populations (gender, age, education level)",5.325101376,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that 'algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits'.",1,"The context directly answers the question by stating that 'algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits'[cite: 385]."
"According to the study of the paper 'Ethics and discrimination in artificial intelligence-enabled recruitment practices', what are two primary sources from which algorithmic bias stems?",Algorithmic bias stems from limited raw data sets and biased algorithm designers.,"According to the study, the two primary sources from which algorithmic bias stems are:

1.  **Limited raw data sets** lacking diverse representation.
2.  **Biased algorithm designers** – specifically, when engineers prioritize certain features or variables based on their own biases.","REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not",8.019837379,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The two claims, that bias stems from 'Limited raw data sets' and 'Biased algorithm designers' (specifically citing engineers' personal biases in feature selection), are both explicitly mentioned and explained in the provided context.",1,The context directly answers the question. It states that 'algorithmic bias stems from limited raw data sets and biased algorithm designers'[cite: 410].
What does the CRISP-DM User Guide provide that the Reference Model does not?,"The User Guide gives more detailed tips and hints for each phase and task on how to do a data mining project, whereas the Reference Model describes what to do.","The Reference Model provides a general process, but the CRISP-DM User Guide provides more advantageous communication – both within and outside the project – by presenting the project plan and status reports in terms of the process model.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",5.713265657,0,"The generated answer is semantically inconsistent with the ground truth. The generation claims the User Guide provides better communication, while the ground truth correctly states the User Guide provides more detailed 'how-to' instructions compared to the Reference Model's 'what-to-do' description.",0,"The generation is unfaithful. It claims the User Guide provides 'more advantageous communication', which is a misinterpretation. The context actually states that using the *process model itself* (not specifically the User Guide) for communication 'w as m uch m ore advantageous than w e originally anticipated'.",0.5,"The context provides a partial answer. It states the User Guide gives 'more detailed tips and hints' [cite: 443] and the Reference Model describes 'what to do'[cite: 443], but the more helpful detail from the generated answer ('how to do a data mining project' vs 'what to do') is inferred rather than explicitly stated."
Which task is mentioned as often being the lengthiest in the Data Preparation phase?,The 'Clean data' task.,"According to the text, the task that is “likely to be performed multiple times” and “the most tedious part of the work (approx. 60-80% of time)” is **Selection, integration, transformation, and cleaning of data**.","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and 15. Mai 2025
Building & evaluating AI3. Prepare data•Most tedious part of the work (approx. 60-80% of time)•Selection, integration, transformation, cleaning of data4. Training the ML model•Build and optimize a model (in our case: with DL algorithms)•Some methods require additional data collection and preparation5. Define decision logic and evaluate AI system •Define how predictions translate into decisions•Evaluate system against business objectives•Go back several steps or deploy model
8
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining G eneric
Tasks
Specialized
Tasks
C ontextC ontext
R eference M odel U ser G uide
• check lists
• questionaires
• tools and techniques
• sequences of steps
• decision points
• pitfalls
Phases
Process
Instances
Figure 1: Four Level B reakdow n of the C R ISP-D M  M ethodology for D ata M ining
The C R ISP-D M  m ethodology distinguishes betw een the Reference M odel and the U ser G uide.
W hereas the R eference M odel presents a quick overview  of phases, tasks, and their outputs, and
describes w hat to do in a data m ining project, the U ser G uide gives m ore detailed tips and hints
for each phase and each task w ithin a phase and depicts how  to do a data m ining project.
3.2 T he G eneric C R ISP-D M  R eference M odel
The C R ISP-D M  reference m odel for data m ining provides an overview  of the life cycle of a data
m ining project. It contains the phases of a project, their respective tasks, and their outputs.
The life cycle of a data m ining project is broken dow n in six phases w hich are show n in Figure 2.
The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase",6.28096199,0,"The generated answer is incorrect. It claims the entire data preparation phase ('Selection, integration, transformation, and cleaning') is the most tedious part. The ground truth correctly identifies the specific sub-task of 'Clean data' as the lengthiest.",1,"The generation is faithful. The context contains a slide that describes the 'Prepare data' phase as the 'Most tedious part of the work (approx. 60-80% of time)' and lists 'Selection, integration, transformation, cleaning of data' as its components. The generated answer accurately reflects this.",0.9,"The context is highly relevant and nearly provides a direct answer. A slide lists 'Prepare data' as having the characteristic 'Most tedious part of the work (approx. 60-80% of time)' and lists 'Selection, integration, transformation, cleaning of data' as sub-tasks[cite: 439]. While it doesn't single out 'Clean data' as the lengthiest, it strongly implies the entire preparation phase, which includes cleaning, is the most time-consuming."
What is the stated aim of the CRISP-DM process model?,"To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the text, the stated aim of the CRISP-DM process model is to be reliably and efficiently repeated by different people and adapted to different situations.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",4.740267038,0,"The generated answer is semantically different from the ground truth. The generation claims the aim is to be 'reliably and efficiently repeated', which is a characteristic of the process, but not its ultimate goal. The ground truth states the aim is to make projects 'less costly, more reliable, more repeatable, more manageable, and faster', which are the desired outcomes.",1,The generation is faithful. The context states that the aim of the project was 'to specify a process w hich can be reliably and efficiently repeated by different people and adapted to different situations'.,1,The context directly answers the question. It states that the aim of the project was to 'specify a process w hich can be reliably and efficiently repeated by different people and adapted to different situations'[cite: 448].
What does CRISP-DM stand for?,Cross Industry Standard Process for Data Mining.,CRISP-DM stands for Cross Industry Standard Process for Data Mining.,"15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",3.121572495,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context contains a slide titled 'Structuring an AI project: CRISP-DM' which includes the text 'Cross Industry Standard Process for Data Mining'.,1,The context directly answers the question. A slide is titled 'Structuring an AI project: CRISP-DM' and includes the text 'Cross Industry Standard Process for Data Mining'[cite: 457].
What are the six phases of the CRISP-DM process model?,"The six phases are Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.","Based on the text provided, the six phases of the CRISP-DM process model are:

1.  Business Understanding
2.  Data Understanding
3.  Data Preparation
4.  Modeling
5.  Evaluation
6.  Deployment","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining D ata
U nderstanding
D ata
Preparation
M odelling
D ataD ataD ata
B usiness
U nderstanding
D eploym ent
Evaluation
Figure 2: Phases of the C urrent C R ISP-D M  Process M odel for D ata M ining
In the follow ing, w e outline each phase briefly:
• Business U nderstanding
This initial phase focuses on understanding the project objectives and requirem ents from  a
business perspective, and then converting this know ledge into a data m ining problem
definition, and a prelim inary project plan designed to achieve the objectives.
• D ata U nderstanding
The data understanding phase starts w ith an initial data collection and proceeds w ith activities
in order to get fam iliar w ith the data, to identify data quality problem s, to discover first
insights into the data, or to detect interesting subsets to form  hypotheses for hidden
inform ation.
There is  a close link betw een B usiness U nderstanding and D ata U nderstanding. The
form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be 15. Mai 2025
Overview CRISP-DM
9",5.895742178,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context includes a diagram that clearly labels the six phases of the CRISP-DM model: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.",1,"The context directly answers the question. A diagram clearly lists the six phases in order: 'Business Understanding', 'Data Understanding', 'Data Preparation', 'Modeling', 'Evaluation', and 'Deployment'[cite: 462]."
What three characteristics define a General Purpose Technology (GPT)?,"They are pervasive, improve over time, and lead to complementary innovation.","Based on the text provided, here are three characteristics that define a General Purpose Technology (GPT):

1.  **Applicable in Various Sectors:** GPTs can be used “in a specific domain, in a specific area, but also in different sectors.”
2.  **General Purpose:** The text specifically uses the term “General Purpose” to describe GPTs.
3.  **Driven by User Adoption:** “It is central for how a technology can actually have an influence on our social life, on our economic processes, on companies and ultimately of course on productivity.” – This refers to the need for user willingness to interact with the system.

","in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben. Studierende machen sich mit der Funktionsweise von generativen KI-Modellen wie GPT vertraut. Sie lernen, diese durch gezieltes Prompting für Managemententscheidungen einzusetzen und kritisch zu bewerten, insbesondere unter wirtschaftlichen und ethischen Gesichtspunkten. gemacht hat,
das User Interface war
die reine Technologie dieser 
großen Sprachmodelle.
Die gibt es theoretisch schon 
seit 2017.
Die wurden natürlich immer 
besser.
Wir hatten am Ende die Version 
drei Punkt irgendwas,
was dann integriert wurde.
Aber es wurde integriert.
Dieses fundamentale KI Modell in
eine ausreichend kompatible und
nutzerfreundliche Umgebung,
nämlich Chat GPT, was
noch mal das Grundmodell GPT
plus weitere Systeme sind, die 
hier zusammenspielen.
Das heißt Forgeat GPT.
Gab es diese großen 
Sprachmodelle schon?
Die haben sich wie gesagt immer 
weiter verbessert.
Grundsätzlich haben
wir aber alle erst damit 
angefangen und
haben eine ganze Industrie 
eigentlich damit angefangen,
es kommerziell zu benutzen.
Ab dem Moment, wo dieses
User Interface richtig 
hinbekommen wurde
und die User des Systems 
hochgegangen ist.
Das ist eine der Kern messages
die Bereitschaft des Nutzers,
mit den Systemen zu 
interagieren.
Generell, wo wir
häufig von Adoption reden 
Technologie use.
Wenn wir ganz, ganz breit sind.
Das ist zentral dafür,
wie eine Technologie tatsächlich
letztlich Einfluss haben kann 
auf unser Sozialleben,
auf unsere ökonomischen 
Prozesse,
auf Unternehmen
und letztlich natürlich auf 
Produktivität.",11.55193281,1,"The generated answer and the ground truth are semantically identical. Both identify pervasiveness, improvement over time, and spawning complementary innovations as the three defining characteristics.",0.7,"The generation is mostly faithful but contains one unsupported claim. The claims about being 'Applicable in Various Sectors' (Pervasive) and leading to 'complementary innovation' are supported by the text. However, the claim that GPTs are 'Driven by User Adoption' is an interpretation, not a direct statement from the text. The text says user interaction is 'central for how a technology can actually have an influence', which is different from being a defining characteristic of a GPT.",1,"The context provides all the necessary information to answer the question. It mentions that the technology can be applied in 'verschiedenen Sektoren' (pervasive) [cite: 467], that it gets better utilized over time ('Ã¼ber die Zeit immer besser genutzt wurden') [cite: 473], and that it leads to 'weiteren komplementÃ¤ren Innovationen' (complementary innovation)[cite: 474]."
"In the paper 'THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES', by how much higher was the adjusted measure of Total Factor Productivity (TFP) compared to official measures at the end of 2017?",The adjusted TFP was 15.9% higher than official measures at the end of 2017.,"I understand you're asking about a specific figure from a paper titled ""THE PRODUCTIVITY J-CURVE: HOW INTANGIBLES COMPLEMENT GENERAL PURPOSE TECHNOLOGIES."" However, the provided context doesn't contain the answer to your question. It describes a J-Curve phenomenon, Receiver Operating Characteristic (ROC) analysis, and a general overview of machine learning. It does not include any data or figures related to TFP measurements in 2017.

Therefore, I cannot answer your question about how much higher the adjusted measure of TFP was compared to official measures at the end of 2017.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 3. Juli 2025
What then?
Consider technical performance metric that is independent of threshold
• Solution: Receiver Operating Characteristic
• Simulate thresholds from 0 to 1 and calculate TPR and FPR for every threshold value
• Plot every (TPR, FPR) tuple with TPR being on the y-axis and FPR on the x-axis
18",13.56643987,0,"The generated answer is a refusal, stating the information is not in the provided text. The ground truth provides a specific numerical answer. The two are semantically opposite.",1,"The generation is faithful. It correctly states that the provided context does not contain the answer to the question and accurately summarizes the actual content of the text (J-Curve, ROC analysis, machine learning overview).",0,"The context is irrelevant. It describes the J-Curve phenomenon [cite: 489], ROC analysis[cite: 494], and general machine learning concepts but contains no specific data or figures about Total Factor Productivity (TFP) from the year 2017."
What did Robert Solow's productivity paradox state?,That one 'can see in the computer age everywhere but in the productivity statistics',The provided text doesn't contain information about Robert Solow's productivity paradox. It discusses a phenomenon where productivity changes can initially be negative due to investments needed to integrate new technologies into organizations.,"der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, sample field experiment. Review of Economics 
and Statistics, 96(1), 119-134. 
Falk, A. & Kosfeld, M. (2006). The hidden costs of 
control. American Economic Review , 96(5), 
1611-1630. 
Favaretto, M., De Clercq, E., & Elger, B. S. (2019). 
Big Data and discrimination: perils, promises 
and solutions. A systematic review . Journal of 
Big Data, 6(1), 1-27.  
Fazelpour, S., & Danks, D. (2021). Algorithmic bias: 
Senses, sources, solutions. Philosophy 
Compass, 16(8), Article e12760. Produktivitätsveränderungen 
negativ sein kann.
Das ist so dieser untere 
Halbkreis von dem J.
Und das kann verschiedene Gründe 
haben.
Unter anderem eben, dass diese
Technologien noch nicht 
ausreichend in den
Organisationen mit
komplementären Investitionen 
integriert worden sind.
Und dann sind wir noch 
technischer
geworden aus einer 
mikroökonomischen Perspektive.
Wir haben kurz über die Trust 
Game Struktur gesprochen,
wo es darum geht.
Ich mache Entscheidungen, 
ökonomisch
relevante Entscheidungen unter 
Unsicherheit.
Und in diesen Szenarien kann 
Machine Learning uns helfen.
Wenn Sie sich wieder vorstellen,
Sie sind hier der Investor.
Sie haben 10 Millionen € zur 
Verfügung,
Sie entweder in ein Startup 
investieren
können oder eben auch nicht.
Wenn Sie nicht investieren, 
haben Sie
10 Millionen Cash und das 
Startup hat nichts.
Wenn Sie es investieren,
dann kann das Startup entweder 
sich anstrengen,
viel arbeiten, kann diese Summe,
die Sie investieren,
verdreifachen und gibt Ihnen die 
Hälfte zurück.
Oder aber das Startup strengt
sich ein bisschen an und läuft 
weg.
Dann haben wir gesagt Wenn wir 
uns
das als 
Nutzenmaximierungsstrategie
aufschreiben aus dieser 
mikroökonomischen,
schrägstrich spieltheoretischen",6.063587904,0,"The generated answer is a refusal, claiming the information is not in the text. The ground truth provides the exact quote defining the paradox. The two are semantically opposite.",1,The generation is faithful. It correctly states that the context doesn't contain information about Robert Solow's paradox and accurately describes the content that is present (a phenomenon of an initial productivity dip due to investment).,0,"The context does not contain the answer. It describes a 'J-curve' phenomenon where productivity initially dips with new technology [cite: 495, 496] but makes no mention of Robert Solow or his specific productivity paradox statement."
What are some examples of complementary investments required by General Purpose Technologies (GPTs) like AI?,"Business process redesign, co-invention of new products and business models, and investments in human capital.","According to the text, some examples of complementary investments required by General Purpose Technologies (GPTs) like AI are:

*   **Sensors and connected devices:** These are new inputs that are spurred by the rise of AI.
*   **Synthetic data:** Another new input.
*   **“Orgware”:** This refers to new data pipelines and AI-first processes – essentially, the tools and processes needed to support and utilize the AI.
*   **Adapting organizational structures:** The text specifically mentions needing to “adapt our organizations,” mirroring the changes made when introducing electricity like the steam engine.","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind Studierende machen sich mit der Funktionsweise von generativen KI-Modellen wie GPT vertraut. Sie lernen, diese durch gezieltes Prompting für Managemententscheidungen einzusetzen und kritisch zu bewerten, insbesondere unter wirtschaftlichen und ethischen Gesichtspunkten.",11.44655132,0.8,"The generated answer is mostly correct but less comprehensive than the ground truth. It correctly identifies new inputs ('sensors', 'synthetic data') and organizational changes ('orgware', 'data pipelines'). However, it omits the key examples from the ground truth: 'business process redesign' and 'co-invention of new products and business models'.",1,"The generation is faithful. The context explicitly lists 'New inputs: sensors, connected devices, synthetic data' and 'New â€œorgwareâ€: data pipelines, ML-ops, AI-first processes' as complements. It also mentions needing to 'adaptieren' (adapt) organizations, directly supporting the claim about organizational redesign.",1,"The context directly provides examples of complementary investments. It lists 'New inputs: sensors, connected devices, synthetic data' and 'New â€œorgwareâ€: data pipelines, ML-ops, AI-first processes'[cite: 521]. It also mentions the need to 'adaptieren' (adapt) organizations, similar to how factories were redesigned for electricity[cite: 526]."
"According to the text, what is the 'Productivity J-Curve' phenomenon?","It is a phenomenon where, as firms adopt a new General Purpose Technology (GPT), total factor productivity (TFP) growth is initially underestimated because resources are spent on unmeasured intangible capital. Later, measured productivity growth overestimates true productivity as the benefits from those intangibles are realized.","According to the text, the “Productivity J-Curve” phenomenon is a two-phase productivity pattern that emerges when investing in General Purpose Technologies (like GPT). It’s characterized by:

*   **Initial Dip:** Productivity initially falls or stalls due to heavy up-front investment in things like data, skills, process redesign, and complementary tech. This is like being on the “lower half-circle” of the J.
*   **Long-Run Rebound:** After this initial dip, productivity accelerates once the complementary investments start paying off. This happens due to learning effects, scale economies, and network externalities.

Do you want me to elaborate on a specific part of this explanation?","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, mussten und überall Kolben die 
Energie
eigentlich weiterbringen 
mussten.
Das heißt,
die Dampfmaschine hat nicht nur 
eine Effizienz,
die Elektrizität, als wir damit 
angefangen haben,
hat nicht nur eine effizientere
Energieproduktion geschaffen,
sondern sogar wenn wir dem
Faktor Produktion Beispiel 
bleiben,
hat es ganze Fabriken im Design 
verändern.
Das hat dazu geführt, dass wir 
länger
die Straßen beleuchten konnten.
Das wiederum hat Effekte darauf 
gehabt,
wie viel wir bauen konnten,
wie lange wir unterwegs sein 
können,
was Mobilität erhöht hat usw usw
alles ausgehend auf dieser 
Kerninnovation,
die wir haben oder hatten damals 
nämlich Elektrizität.
Ja, jeder nimmt Elektrizität 
jetzt verkantet
einfach hier an den Stecker 
gehen
und Elektrizität Energie ziehen.
Und das ist natürlich ein 
massiver Treiber von Wachstum,
von Produktivität.
War aber nicht immer so.
Am Anfang hat es super lange 
gedauert,
bis diese Elektrizität irgendwas
gebracht hat und die
Frage werde ich ihn gleich 
stellen,
Wie lange sie denn glauben,
dass es gedauert hat, wenn wir 
auf der nächsten
Slide zu den JWs sind
auch wieder mikroökonomisch 
motiviert?
Ja, alles. Aber lassen Sie uns
ganz kurz noch mal darüber 
nachdenken.",13.08153749,1,"The generated answer and the ground truth are semantically consistent. Both describe the J-Curve as an initial dip in productivity due to unmeasured intangible investments, followed by a later rebound as those investments pay off.",1,"The generation is faithful. Every claim made about the J-Curveâ€”that it's a two-phase pattern, has an initial dip due to upfront investment in data/skills/redesign, and a later reboundâ€”is explicitly stated in the provided context.",1,"The context directly defines the Productivity J-Curve. It describes it as a 'two-phase productivity pattern' with a 'Short-run dip' followed by a 'Long-run rebound'[cite: 536]. It also explains that the dip is due to upfront investments in 'unmeasured intangible capital' like data, skills, and process redesign[cite: 536]."
"What societal benefit, in terms of CO2 emissions, would a 2.6% return decrease translate to based on U.S. figures from 2022?","It would result in a reduction of 624,000 metric tons of CO2 emissions.","Based on the provided text, a 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions.","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation 24. April 2025
Summary and impact: what ML does
https://www.readycloud.com/info/14-facts-about-ecommerce-product-returns
up to 4.6% decrease
•Our green nudges can be easily implemented at low costin all online shops •Our ""smart"" implementation uses open-source libraries and may predict impact on individual-level profit of any nudge•Our return reduction on global scale would save…•…up to 200,000 tons of CO2 emissions •…up to 120,000 tons of waste
Annual impact for industry partner: in the six figures",7.629763365,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states: 'the 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions'.",1,"The context directly answers the question, stating, 'From a societal per - spective, based on U.S. figures from 2022, the 2.6% return decrease would result in a reduction of 624,000 metric tons of CO2 emissions'[cite: 553]."
What is the estimated annual savings in processing costs for the industry partner due to the green nudge?,"The effect corresponds to annual savings in processing costs of approximately $340,000.","The estimated annual savings in processing costs for the industry partner due to the green nudge is approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than annually. This amount is roughly equivalent to the 
annual emissions from the electricity consumption of 
121,000 U.S. homes.
3
Notably, the actual savings in 
emissions are likely to be even higher both as the vol -
ume of returns continues to grow further (Calma 2023 ) 
and because emissions through packaging and the pro -
duction process of discarded goods are not included.
From a business perspective, according to internal 
figures of our industry partner, the estimated annual 
profit increase quickly amortizes costs associated with 
the development, implementation, and maintenance of 
green nudging. In fact, the developed dual nudge 
turned out to be so successful that our industry partner 
opted to keep the green nudge running in the German 
online shop. The project’s success is affirmed by a prac -
tice executive who corroborates the effectiveness of the 
implemented dual-nudge approach by stating that
[t]he reduction in returns without a negative impact 
on revenue was huge for us. So huge, in fact, that we 
opted for keeping the experiment live and showing 
the successful treatment to all of our German online 
customers. —Practice partner executive
Apart from the immediate business benefits through effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge",6.107186079,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that 'the observed effect translates into an estimated annual reduction in processing costs of about $340,000'.",1,"The context directly answers the question. It states: 'For our industry partner, the observed effect translates into an estimated annual reduction in processing costs of about $340,000'[cite: 569]."
"In the 'Smart Green Nudging' paper, what was the sample size (n) of the randomized field experiment discussed?","The experiment had a sample size of 117,304 participants (n = 117,304).","The randomized field experiment in the “Smart Green Nudging” paper involved 73,235 visitors who made a purchase.","The rest of the paper is structured as follows. Section 
2 provides the theoretical background by reviewing 
related literature. Section 3 presents the setting and 
results of our randomized field experiment. Section 4
details the configuration and results from our causal 
machine learning model, along with the evaluation of 
smart green nudging. Section 5 concludes by discuss -
ing the practical implications of our work, its limita -
tions, and potential avenues for future research.
2. Related Literature
Our work relates to three streams of literature, namely 
(i) product returns, (ii) nudging, and (iii) treatment het -
erogeneities and causal machine learning.
2.1. Product Returns
The surge in product returns and the corresponding busi -
ness and environmental repercussions have prompted 
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
2 Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 
Downloaded from informs.org by [141.2.113.143] on 24 April 2025, at 02:36 . For personal use only, all rights reserved. significant variability in how individuals respond to 
nudging (Hummel and Maedche 2019 ), rendering the 
outcomes of green nudging unpredictable (Lehner et al. 
2016 ). Additionally, few researchers have explored the 
impact of green nudges on a large scale (see He et al. 
2023 for an exception), leaving a gap in our understand -
ing and a need to reconcile these divergent findings.
We extend prior research by conducting a large- 
scale, randomized field experiment to test the effective -
ness of green nudges in reducing product returns. Our 
green nudge effectively leverages customers’ desire to 
sustain a positive self-image through environmentally 
friendly behavior (Schubert 2017 ). It heightens aware -
ness of the negative environmental impact linked to 
returning products during both the prepurchase and 
postpurchase phases of the customer journey.
2.3. Treatment Heterogeneities and Causal 
Machine Learning
There often exists a notable heterogeneity in customer 
reactions to marketing strategies (El Kihal and Shehu 
2022 ), especially nudging techniques (Lehner et al. 
2016 , Mirsch et al. 2017 , Hummel and Maedche 2019 ). 
Accordingly, relying solely on average treatment nudge group (cart and reminder prompt). During the 
study, millions of visitors browsed the website. Our 
randomization checks (see Table 5 in Online Appendix 
A.2) confirmed the success of the randomization pro -
cess across various characteristics, including the inter -
net browser, the state associated with the IP 
geolocation, and the product count in the initial (i.e., 
preintervention) cart. Of the visitors across the three 
groups, 73,235 made a purchase. The conversion rate 
aligns with typical figures in the fashion retail industry. 
It is important to note that customers can view their 
cart at any time but need to do so at least once to com -
plete a purchase.
In our field experiment, we tracked several customer 
outcomes with our industry partner and its service pro -
viders, including sales, returns, and duplicate product 
purchases, as detailed in Table 1 . The key variable of 
interest is the customer’s decision to return at least one 
purchased product. This approach is driven by our pri -
mary scientific goal to reduce environmental externali -
ties, such as packaging waste and greenhouse gas 
emissions, associated with product returns. Specifi -
cally, our industry partner mandates returning all",6.131923437,0,"The generated answer is incorrect. It states the sample size was 73,235, which was the number of visitors who made a purchase, not the total sample size of the experiment, which is correctly stated in the ground truth as 117,304.",1,"The generation is faithful. The context explicitly states, 'Of the visitors across the three groups, 73,235 made a purchase.' The generated answer accurately reports this number.",0.8,"The context strongly implies the answer. It states that '73,235 made a purchase' across the three experimental groups[cite: 600]. The question asks for the sample size 'n' of the *experiment*, which is different from the number of purchasers. A different part of the document abstract mentions 'n = 117,304'[cite: 611], which is the correct total sample size. The retrieved context is slightly confusing by focusing only on purchasers."
What are the two types of returns mentioned in the context of online shopping?,'Real returns' (where the customer intended to keep the item) and 'opportunistic returns' (where the customer orders multiple options with the intent to return some).,"According to the context, the two types of returns mentioned are:

1.  **Returns arising from discrepancies between expectations and reality** (often referred to as returns due to dissatisfaction).
2.  **Returns related to the environmental impact of returns** (the “green nudging” approach).","Accepted: April 18, 2024 
Published Online in Articles in Advance: 
August 8, 2024 
https://doi.org/10.1287/mksc.2022.0393 
Copyright: © 2024 The Author(s)
Abstract. In e-commerce, product returns have become a costly and escalating issue for 
retailers. Beyond the financial implications for businesses, product returns also lead to 
increased greenhouse gas emissions and the squandering of natural resources. Traditional 
approaches, such as charging customers for returns, have proven largely ineffective in 
curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper 
investigates the effectiveness of informing consumers about the negative environmental 
consequences of product returns (“green nudging”) to curtail product returns through a 
large-scale randomized field experiment (n � 117,304) conducted with a leading European 
fashion retailer’s online store. Our findings indicate that implementing green nudging can 
decrease product returns by 2.6% without negatively impacting sales. We then develop 
and assess a causal machine learning model designed to identify treatment heterogeneities 
and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation marketing researchers and practitioners to examine dri -
vers of product returns (e.g., Harris 2008 , Pei and Paswan 
2018 ) and to seek solutions to curb them (P˚ alsson et al. 
2017 , Abdulla et al. 2019 ). Particularly problematic are 
opportunistic return behaviors. An example for such 
behavior is the practice of “bracketing,” where customers 
purchase multiple variants of an item with the intention 
of returning some (Altug et al. 2021 ). A plausible initial 
approach to address this issue is the adaptation of return 
policies. The literature differentiates between lenient poli -
cies, such as “no-questions-asked” full money-back guar -
antees, and more restrictive ones that, for example, 
impose restocking fees or nonrefundable handling 
charges (Su 2009 , Nageswaran et al. 2020 ). Although for -
going lenient policies can decrease returns (Shehu et al. 
2020 ), they also pose business drawbacks. For example, 
money-back guarantees can serve as quality signals 
(Moorthy and Srinivasan 1995 , Altug et al. 2021 ) to boost 
sales and profits (Davis et al. 1995 ). These business ramifi -
cations call into question the efficacy of exclusively rely -
ing on restrictive return policies from a practitioner’s 
standpoint. returns arising from discrepancies between expecta -
tions and reality (Zhou and Hinz 2016 ). Examples of 
customer-based interventions include virtual try-on 
sessions, customer reviews, and online consultations 
(Minnema et al. 2016 ). In essence, monetary and proce -
dural interventions operate in the postorder phase, 
whereas customer-based instruments work before and 
during the order process (Walsh and M ¨ohring 2017 ). 
Accordingly, monetary and procedural interventions 
are rather reactive measures, trying to motivate indivi -
duals to keep products that have already been pur -
chased. By contrast, consumer-based instruments are 
preemptive, aiming to avoid purchases of products 
that are highly likely to be returned.
Our study contributes to the literature on product 
returns by introducing a simple intervention that high -
lights the environmental impact of returns to custo -
mers. This nudging approach, a customer-based 
instrument, is distinct in its minimally invasive, low- 
cost nature, seeking simply to inform customers about 
the repercussions of their postpurchase actions. Closely 
related to our research, Ghose et al. ( 2023 ) recently 
assessed the effectiveness of pressure-oriented and self-",7.70994401,0.8,"The generated answer is mostly correct but uses different terminology. It identifies 'returns arising from discrepancies between expectations and reality' and returns related to 'environmental impact', which correspond to the concepts of 'real returns' and 'opportunistic returns' (where environmental nudging might play a role), but it does not use the more standard terms from the ground truth.",1,The generation is faithful. It accurately identifies the two main types of returns discussed in the context: 'returns arising from discrepancies between expectations and reality' and returns related to environmental impact which is the focus of the 'green nudging' approach.,0.5,"The context is partially relevant. It discusses 'opportunistic return behaviors' like 'bracketing' [cite: 615] and returns due to discrepancies between expectations and reality[cite: 621]. These align with the concepts of 'opportunistic' and 'real' returns, but the context doesn't use these exact terms, requiring inference."
"According to the paper, by how much did the smart green nudge decrease product returns compared to having no nudge at all?",The smart green nudge resulted in a 6.7% decrease in product returns compared to no nudge at all.,"According to the paper, the reduction in product returns achieved by smart green nudging compared to having no nudge at all was 5.19%.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than our test data and report the results in Table 4 .
Table 4 shows return shipments, the return value, 
the conversion rate of the viewed carts, and (gross) 
sales. Additionally, we depict the “Share treated” col -
umn presenting the proportion of customers subjected 
to green nudging.
We observe a pronounced improvement in effective -
ness of smart green nudging in reducing returns; com -
pared with the naive approach of nudging all 
customers, smart green nudging can approximately 
double the impact, curbing returns by an additional 
3.15%. When compared against no nudging, the reduc -
tion is 5.19%. This decline in returns is not only mean -
ingful from a business standpoint but also holds 
considerable ecological weight.
Although the results from Table 4 look promising, 
we now follow Athey et al. ( 2023 ) and examine varying 
shares of treated customers with accompanying confi -
dence intervals for a more thorough examination. 
Figure 4 presents the mean returned shipments (on the 
vertical axis) as we vary the policy π by applying the 
intervention to different shares of the test data (on the 
horizontal axis). In essence, the curve shows the overall 
efficacy of smart green nudging as we progressively purchase, ensuring an unbiased and comprehensive 
assessment of green nudging effects (e.g., encompass -
ing effects on the conversion rate).
3.2.1. Overall Effects of Green Nudging. Table 2 pre -
sents individual summary statistics for key business 
metrics from our field experiment. Specifically, we out -
line average customer responses in terms of binary return 
decisions, the value of returned products, the cart conver -
sion, and sales. We display these results separately for 
the control group (without nudging) and the two treat -
ment groups (single nudge and dual nudge). For all three 
groups, we visualize the key business metrics and confi -
dence intervals relative to the control group in Figure 2 .
Our results suggest that the dual-green-nudge inter -
vention can significantly reduce product returns with -
out harming sales. On the extensive margin, customers 
in the dual-nudge treatment are on average 2.6% less 
likely to return at least one product and thus, make a 
return shipment compared with the control group (0.24 
versus 0.26, respectively; p < 0.03, Mann–Whitney U 
test). We also observe an impact on the intensive mar -
gin. On average, customers exposed to the dual nudge",6.416945696,0,"The generated answer is factually incorrect. It states the reduction was 5.19%, while the ground truth correctly states the reduction was 6.7%.",1,"The generation is faithful. The context explicitly states, 'When compared against no nudging, the reduction is 5.19%.' The generated answer provides a different, incorrect figure (6.7%) which is not supported by the context.",1,"The context directly answers the question, stating, 'When compared against no nudging, the reduc - tion is 5.19%'[cite: 640]."
"In the 'Smart Green Nudging' study, which variable was identified by SHAP values as the most pivotal for the prediction?",The initial cart value.,"According to the study, SHAP values indicated that the initial cart value was the variable most pivotal for the prediction.","contribute to the predictions (Senoner et al. 2022 ). In 
our case, SHAP values indicate that the initial cart 
value is the variable most pivotal for the prediction. We 
report SHAP values from our CML model in Online 
Appendix A.10. However, it is crucial to differentiate 
between variables that predict the treatment effect and 
those that act as causal moderators. We encourage 
future research to rigorously investigate the causal role 
of variables in moderating the effectiveness of green 
nudging.
Our study also underscores the effectiveness of using 
customer carts and digital footprint data to personalize 
marketing interventions. Our analyses reveal multiple 
characteristics that induce heterogeneous treatment 
effects, including the type of internet browser, the inter -
net provider, the federal state, and the weekday.
7
For 
example, weekend shoppers are more effectively 
nudged than weekday shoppers, possibly because of 
lower stress levels, aligning with similar literature find -
ings (Grahl et al. 2023 ). Similarly, the internet browser 
and other characteristics of the digital footprint can 
hint at underlying customer traits and demographics 
(Berg et al. 2020 ). Incorporating these insights, practi - task. In the following, we show SHAP values for initially biased and unbiased ML models. Each figure contains 
information on the overall importance of each feature for the prediction (higher position in the vertical ranking) and 
the features’ marginal effects for predictions about individual instances (position on the x-axis). Each dot represents a 
SHAP value for a feature and an instance. The color of the dots depicts the value of the feature from low (blue dots) 
to high (red dots). Plots jitter identical SHAP values vertically, providing insights into the SHAP value distribution per 
feature. Overall, Figures C1 and C2 show the relationship between the features (borrower characteristics) and the 
prediction (likelihood of this person mak ing a repayment) by revealing their marginal contribution to predictions (in 
terms of probability) conditional on feature values. 
 
Figure C1. SHAP Values, Example of Biased ML Model (max. initial label imbalance) 
The figure for the biased ML reveals that prior to any retraining, the gender attribute (0 = female, 1 = male) is the most 
relevant feature to predict whether a borrower will make a repayment. Our findings indicate that being female always 22. Mai 2025
Shapley scores: weighted average
•Local accuracy: attributions sum (exactly) to the prediction difference•Consistency: if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.•Missingness: unused features get SHAP value of 0 x*=0→𝜙+•Additivity: SHAP values can be added to show the joint contribution of several features to a prediction
22
𝜙%𝑓,𝑥=	2&!⊆	)!𝑧*!𝑀−𝑧*−1!𝑀! (𝑓)𝑧*−𝑓)𝑧*\i)Shapley value for i= savings
Model f
For individual x with a specific combination of features
Weighted sum over allSubsets z’of (transformed) data point x; whereM is total number of features in full set
Difference inModel prediction
𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛!=𝐸(-𝑓𝑋)	+	3""𝜙!",5.686816216,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states, 'SHAP values indicate that the initial cart value is the variable most pivotal for the prediction'.",1,"The context directly answers the question. It states, 'In our case, SHAP values indicate that the initial cart value is the variable most pivotal for the prediction'[cite: 654]."
"In the field experiment from the 'Smart Green Nudging' paper, what was the initial aim of the academic team members regarding the prompts they were testing?",The initial aim was to isolate the effects of the cart prompt and the reminder prompt.,"The initial aim of the academic team members was to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects.","Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con -
strained our options, permitting only the testing of one 
of the design elements in isolation. Ultimately, our 
industry partner opted to test the prompt displayed 
after consumers finished their purchasing. This choice 
was based on the belief that the reminder displayed 
after the purchase posed a lower risk of negatively 
impacting sales.
3.1.3. Study Details. We conducted a large-scale, ran -
domized field experiment over seven weeks in our 
industry partner’s German online shop to test the dual- 
nudge and single-nudge interventions.
2
The study ran -
domly assigned website visitors using stationary 
devices to one of three groups upon their first view of 
the shopping cart: the control group (no nudging), the 
single-nudge group (reminder prompt), or the dual- 
nudge group (cart and reminder prompt). During the 
study, millions of visitors browsed the website. Our 
randomization checks (see Table 5 in Online Appendix 
A.2) confirmed the success of the randomization pro - significant variability in how individuals respond to 
nudging (Hummel and Maedche 2019 ), rendering the 
outcomes of green nudging unpredictable (Lehner et al. 
2016 ). Additionally, few researchers have explored the 
impact of green nudges on a large scale (see He et al. 
2023 for an exception), leaving a gap in our understand -
ing and a need to reconcile these divergent findings.
We extend prior research by conducting a large- 
scale, randomized field experiment to test the effective -
ness of green nudges in reducing product returns. Our 
green nudge effectively leverages customers’ desire to 
sustain a positive self-image through environmentally 
friendly behavior (Schubert 2017 ). It heightens aware -
ness of the negative environmental impact linked to 
returning products during both the prepurchase and 
postpurchase phases of the customer journey.
2.3. Treatment Heterogeneities and Causal 
Machine Learning
There often exists a notable heterogeneity in customer 
reactions to marketing strategies (El Kihal and Shehu 
2022 ), especially nudging techniques (Lehner et al. 
2016 , Mirsch et al. 2017 , Hummel and Maedche 2019 ). 
Accordingly, relying solely on average treatment The rest of the paper is structured as follows. Section 
2 provides the theoretical background by reviewing 
related literature. Section 3 presents the setting and 
results of our randomized field experiment. Section 4
details the configuration and results from our causal 
machine learning model, along with the evaluation of 
smart green nudging. Section 5 concludes by discuss -
ing the practical implications of our work, its limita -
tions, and potential avenues for future research.
2. Related Literature
Our work relates to three streams of literature, namely 
(i) product returns, (ii) nudging, and (iii) treatment het -
erogeneities and causal machine learning.
2.1. Product Returns
The surge in product returns and the corresponding busi -
ness and environmental repercussions have prompted 
von Zahn et al.: Product Returns, Digital Footprints, and Causal Machine Learning 
2 Marketing Science, Articles in Advance, pp. 1–16, © 2024 The Author(s) 
Downloaded from informs.org by [141.2.113.143] on 24 April 2025, at 02:36 . For personal use only, all rights reserved.",6.39922905,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that 'the academic team members initially aimed to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects'.",1,"The context provides a direct answer, stating, 'Notably, the academic team members initially aimed to isolate the effects of the cart prompt and the reminder prompt, seeking to pinpoint their causal effects'[cite: 672]."
"In the 'Smart Green Nudging' paper, what were the estimated annual savings in processing costs for the industry partner as a result of the green nudging intervention?","The estimated annual savings in processing costs were approximately $340,000.","According to the paper, the estimated annual reduction in processing costs for the industry partner as a result of the green nudging intervention was approximately $340,000.","the potential of smart green nudging as an effective 
personalization strategy for both businesses and the 
environment.
5. Discussion and Conclusion
5.1. Contribution
Our study addresses the challenge of product returns 
by developing and empirically validating a green 
nudging intervention. Green nudging prompts shop -
pers toward more environmentally conscious and 
thoughtful behavior, effectively reducing product 
returns. Our large-scale field experiment shows that 
green nudging lowers product returns by 2.6% without 
negatively impacting sales. For our industry partner, 
the observed effect translates into an estimated annual 
reduction in processing costs of about $340,000 and an 
8.7% boost in profits. On a larger scale, estimations 
based on public data from the United States suggest 
that the achieved reduction in returns could signifi -
cantly decrease U.S. fashion industry-wide costs by 
$650 million (Zheng 2023 ) and carbon emissions by 
624,000 metric tons (Calma 2023 ). Additionally, our 
causal machine learning model identifies notable varia -
tions in consumer responses to the nudge. Subsequent 
off-policy evaluation indicates that exploiting these 
variations (i.e., nudging “smartly”) could more than variation in treatment effects across different states (see 
Online Appendix A.9). However, excluding indivi -
duals from Baden-W ¨urttemberg who, on average, 
exhibit a notable adverse reaction to the green nudge 
results in a modest improvement of 0.1 percentage 
points in the effectiveness of the green nudge.
Overall, the results for smart green nudging offer an 
optimistic outlook. From a business perspective, imple -
menting smart green nudging could potentially double 
the cost savings compared with the naive strategy. 
Equally significant is the environmental benefit. 
Applied across the United States in logistics, this 
approach could double the effect on CO
2 
emissions to 
an impressive total of more than 1 million tons annu -
ally (Calma 2023 ). These dual advantages underscore 
Table 4. Summary Statistics from the Analyses of Smart Green Nudging on Test Data
Share treated, %
Returns (shipment) Returns (value) Cart conversion Sales
Mean (SE) Mean (SE) Mean (SE) Mean (SE)
No nudging 0.0 0.262 (0.005) 18.61 (0.58) 0.628 (0.006) 51.96 (0.90)
Naive green nudging 100.0 0.256 (0.005) 18.22 (0.56) 0.631 (0.005) 52.49 (0.87)
Difference from no nudging, % � 2.10 � 2.10 0.51 1.01 Kihal et al. 2021 , Pfrang and Spreer 2022 ). This paper 
aims to contribute to this discussion by empirically 
evaluating the effectiveness of an informational inter -
vention, referred to as green nudging, in decreasing 
product returns.
In this study, we partnered with a leading European 
fashion retailer. Like many in the industry, the retailer 
grapples with the challenges and costs arising from 
online product returns. Seeking to align profitability 
with environmental responsibility, we collaborated 
with our partner to craft a strategy that not only curtails 
product returns but also, that resonates with the retai -
ler’s sustainable ethos and is economically viable. The 
jointly developed green nudge comprises two prompts: 
informing online customers about the negative envi -
ronmental impact of product returns throughout 
(shopping cart prompt) and at the conclusion of their 
online shopping journey (reminder prompt). We con -
ducted an extensive field experiment with 117,304 par -
ticipants in the retailer’s online shop to empirically 
validate the green nudge. We then apply modern 
causal machine learning (CML) methods (Athey et al. 
2019 ) to analyze treatment heterogeneities based on",7.285413265,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states, 'For our industry partner, the observed effect translates into an estimated annual reduction in processing costs of about $340,000'.",1,"The context directly answers the question. It states that for the industry partner, the effect 'translates into an estimated annual reduction in processing costs of about $340,000'[cite: 694]."
Where is INFORMS located?,"INFORMS is located in Maryland, USA.","INFORMS is located in Maryland, USA.","This article was downloaded by: [141.2.113.143] On: 24 April 2025, At: 02:36
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA
Marketing Science
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org
Smart Green Nudging: Reducing Product Returns
Through Digital Footprints and Causal Machine
Learning
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian
Speicher, Oliver Hinz
To cite this article:
Moritz von Zahn, Kevin Bauer, Cristina Mihale-Wilson, Johanna Jagow, Maximilian Speicher, Oliver Hinz (2024)
Smart Green Nudging: Reducing Product Returns Through Digital Footprints and Causal Machine Learning.
Marketing Science
Published online in Articles in Advance 08 Aug 2024
. https://doi.org/10.1287/mksc.2022.0393
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The Informationen zur Vorlesung: Gehalten am Do. 12:00 bis 16:00; wöchentlich vom 24.04.2025 bis 24.07.2025; Ort: Hörsaalzentrum Westend - HZ 12 DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org.",3.088981152,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context includes an image with the text 'INFORMS is located in Maryland, USA'.",1,"The context provides a direct answer. It includes a line that says, 'INFORMS is located in Maryland, USA'[cite: 710]."
What programming language will be used in the course?,Python.,The course will use the programming language Python.,"24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen. 23. Juli 2025
Yourhelpisappreciated
2
Survey on LearnMateChatbot for this course",2.805145264,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that 'Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt fÃ¼r Schritt an die Umsetzung von ML-Projekten herangefÃ¼hrt' (With the help of the programming language Python, participants will be guided step-by-step through the implementation of ML projects).",1,"The context provides a direct answer. It states, 'Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt fÃ¼r Schritt an die Umsetzung von ML-Projekten herangefÃ¼hrt' (With the help of the programming language Python, participants will be guided step-by-step through the implementation of ML projects)[cite: 715]."
"According to the text, what are four useful applications of Large Language Models (LLMs) in everyday life?","Four useful applications of LLMs are: synthesizing information (such as summarizing and translating), acting as an argumentative sparring partner to identify weak points in arguments, brainstorming creative ideas, and assisting with code-related tasks like documentation and translation between programming languages.","The text doesn’t list four specific “everyday life” applications of Large Language Models (LLMs). However, it does highlight these potential uses:

1.  **Synthesizing information:** Summarizing and translating blocks of information.
2.  **Argumentative sparring partner:** Pointing out weak points in arguments.
3.  **Brainstorming:** Co-creation of creative ideas.
4.  **Code documentation or transformation:** Generate docs for software and translate across languages.

It also mentions that the willingness of users to interact with systems is central to how a technology can influence social life, economic processes, and productivity.","23. Juli 2025
Useful in everyday life
20
•Synthesizing information: Summarizing and translating blocks of information, e.g., bullet points, into structured text•Argumentative sparring partner: pointing out weak points in arguments•Brainstorming: co-creation of creative ideas•Code documentation or transformation: generate docs for software and translate across languages Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. gemacht hat,
das User Interface war
die reine Technologie dieser 
großen Sprachmodelle.
Die gibt es theoretisch schon 
seit 2017.
Die wurden natürlich immer 
besser.
Wir hatten am Ende die Version 
drei Punkt irgendwas,
was dann integriert wurde.
Aber es wurde integriert.
Dieses fundamentale KI Modell in
eine ausreichend kompatible und
nutzerfreundliche Umgebung,
nämlich Chat GPT, was
noch mal das Grundmodell GPT
plus weitere Systeme sind, die 
hier zusammenspielen.
Das heißt Forgeat GPT.
Gab es diese großen 
Sprachmodelle schon?
Die haben sich wie gesagt immer 
weiter verbessert.
Grundsätzlich haben
wir aber alle erst damit 
angefangen und
haben eine ganze Industrie 
eigentlich damit angefangen,
es kommerziell zu benutzen.
Ab dem Moment, wo dieses
User Interface richtig 
hinbekommen wurde
und die User des Systems 
hochgegangen ist.
Das ist eine der Kern messages
die Bereitschaft des Nutzers,
mit den Systemen zu 
interagieren.
Generell, wo wir
häufig von Adoption reden 
Technologie use.
Wenn wir ganz, ganz breit sind.
Das ist zentral dafür,
wie eine Technologie tatsächlich
letztlich Einfluss haben kann 
auf unser Sozialleben,
auf unsere ökonomischen 
Prozesse,
auf Unternehmen
und letztlich natürlich auf 
Produktivität.",13.20371652,0,"The generated answer is a refusal, stating the information is not in the text. The ground truth provides a direct, factual answer to the question. The two are semantically opposite.",1,The generation is faithful. It correctly states that the text does not list four 'everyday life' applications and then accurately lists the four specific uses that *are* mentioned in the context.,1,"The context directly lists the four applications mentioned in the answer: 'Synthesizing information', 'Argumentative sparring partner', 'Brainstorming', and 'Code documentation or transformation'[cite: 725]."
For what types of tasks are basic chat models the most suitable choice?,Basic chat models are most suitable for low-stake tasks where a fast response is required.,"According to the text, basic chat models are the most suitable choice when a task is “rather low stake and fast response is required.”","23. Juli 2025
Many models, but which one should you use?
18
Reasoning model: Choose when task is rather complex, results are important, and time to receive response
Basic chat model: Choose when task is rather low stake and fast response is required.
Example 1: •Which vegetable has the highest vitamin C amount?•Act as a nutritionist and create a vegetable salad with at least 500𝜇𝑔vitamin C and 200𝜇𝑔vitamin D.Example 2: •What is an LLM?•Act as a computer scientist and explain to me what the advantage of a reasoning model over a chat model is. 15. Mai 2025
Recap: a task-based approach
4
Task 1Task 2... Task N-1 Task N
Workflow
Decomposition of workflowsto identify suitable individual tasks that AI can perform•Evaluate AI applicability based on task attributes (routine, analytical, standardized)•Identify (hidden) prediction problemsin tasks, e.g., customer needs, optimal ad textRethink processesand imagine what AI systems can help with•Installation of individual AI often entail further changes, e.g., new tasks for humans•Understand what is necessary for AI systems to work properly•Notion of complements and substitutesin organizational structures 23. Juli 2025
Give examples whenever possible (few-shot)
13
Humans often rely on previous examples to work on tasks•Examples can also be useful for LLMs (example = shot)•Help show LLMs the structure of desired output•Zero shot-prompting may be ineffective if response needs to be very specificIf you want specific styles and formats try giving examples•Add example(s) as part of prompt•Instruct LLM to follow examples•Beware too many examples!",4.939769268,1,"The generated answer and the ground truth are semantically identical. Both state they are for low-stake, fast-response tasks.",1,The generation is faithful. The context contains a slide that says for a 'Basic chat model: Choose when task is rather low stake and fast response is required'.,1,The context directly answers the question. A slide has a section titled 'Basic chat model' with the instruction: 'Choose when task is rather low stake and fast response is required'[cite: 740].
"According to the document, what is the population of Munich?",1578567,"According to the document, the population of Munich is 1,578,567.","23. Juli 2025
Human v. LLM ”Thinking”
7
Is the following statement true? “Munich’s population is 2.12 times that of Frankfurt.”A potential Human (cognitive) process:• Think about knowing these numbers; most likely not• Use Wikipedia tool: get both of these populations•Munich: 1,578,567•Frankfurt: 773,068• Try to divide the numbers in your head; rough idea bit not exact number• Use calculator tool: 1,578,567/ 773,068 = 2.042• Reflection about correctness of result; number sounds reasonable• Forming an answer• Start writing an answer: “Yes, the statement is correct.”• Checking the entire solution process, the answer sentence and maybe rewrite it to “The statement is true.”
This is not how LLMs “think”! decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were 
recruited to participate in an ongoing field study. The study 
was conducted by a subset of this paper ’s authors and 
aimed to examine correlations between academic 
performance, personal characteristics, and behavioral 
measurements, including outcomes of sequential social 
dilemmas such as the investment game. The study, 
accessible via a link sent to students ’ email addresses, 
comprised a comprehensive survey on personal 
demographics, socioeconomic background, cognitive 
abilities, personality traits, and, crucially for our 
simulations, a sequential social dilemma game 
incorporating our previously described investment game (a 
detailed description of this game can be found in Appendix 
B.1). Participants’ decisions were incentivized, allowing 
for the measurement of revealed preferences, an approach 
superior to simple self -reported measures due to its 
potential to reduce presentation and demand effects 
(Camerer and Hogarth, 1999). The incentives involved 
earning real money based on the game outcomes with other 15. Mai 2025
Accurate predictions are often not enough
13
Laws against discrimination in many domains includingCredit (Equal Credit Opportunity Act),Education (Civil Rights Act of 1964; Education Amendments of 1972), Employment (Civil Rights Act of 1964), Housing (Fair Housing Act), Public Accommodation (Civil Rights Act of 1964)
§Born and raised in Germany§90% of population§data describes them accurately §accurate predictions (95% accurate)
§Migrated to Germany in past years§10% of population§data describes them poorly§poor predictions (50% accurate)
90.5% accurate prediction about creditworthiness
Consider an example where machine learning model predicts individual creditworthiness
ML model",4.764998436,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly lists 'Munich: 1,578,567' as part of a sample human thought process.",1,"The context directly answers the question. A list in the document under a section about a human cognitive process states, 'Munich: 1,578,567'[cite: 745]."
How many parameters did GPT-2 have?,1.5B parameters.,GPT-2 had 1.5B parameters.,"17. Juli 2025
Evolution of OpenAI’s GPT modelsGPT:117M parameters (Radford et al., 2018)•Transformer decoder with 12 layers•Trained on BooksCorpus: over 7000 unique books (4.6GB text)GPT-2:1.5B parameters (Radford et al., 2019)•Same architecture as GPT but more flexible•Also: trained on much more data: 40 GB of internet text dataGPT-3:175B parameters (Brown et al., 2020)•Over 600GB of text dataGPT-4:1.76T parameters (undisclosed)•Mixture of Expert: different expert models with >200B parameter each•10 trillion words ~ 40 Billion pages (exact size unknown)
35 17. Juli 2025
GPT training
34
Source: State of GPT | BRK216HFS
Stage 1Stage 2 Stage 3 17. Juli 2025
GPT
33
GPT Architecture
It is “straightforward” to build a GPT model…~100 lines of code would do it. But training it is another story!",3.172445297,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context includes a slide titled 'Evolution of OpenAIâ€™s GPT models' which contains the line item 'GPT-2: 1.5B parameters'.,1,The context directly answers the question. A slide detailing the 'Evolution of OpenAIâ€™s GPT models' has a line item that reads 'GPT-2: 1.5B parameters'[cite: 754].
What is the intuition behind how an 'attention head' works in a language model?,"An attention head represents a word as a weighted combination of itself and all the words that came before it. The weights in this combination are based on the similarity between the words, which helps the model understand the context of the sentence.","Based on the provided text, there isn’t a direct explanation of “attention heads” in language models. However, we can glean some related concepts that hint at their function. Here’s a breakdown of the intuition, drawing from the context:

*   **Focus on Relationships:** The core idea is that language models need to understand *relationships* between words within a sentence (or sequence). It’s not just about processing each word in isolation.

*   **“Human in the Loop” and Decision Making:** The text emphasizes a human role in systems like Chat GPT – a user deciding whether information is “good” or “bad” for their decision-making. This suggests the model is trying to identify the *most relevant* information, and attention heads likely play a role in this.

*   **Understanding Preferences:** The example of asking “wo kann ich guten Urlaub machen?” highlights the model’s ability to understand user preferences. Attention heads would likely be involved in weighting different aspects of the user’s input (e.g., “guten Urlaub”) to determine the best response.

*   **Modulators and Relationships:** The concept of “moderators” – variables like age or experience – influencing the relationship between performance expectancy and intention to use, suggests attention heads are dynamically adjusting their focus based on these factors.

**Therefore, an ‘attention head’ likely works by:**

1.  **Calculating Relevance:** It assesses the importance of each word in the input sequence relative to other words.
2.  **Dynamic Weighting:** It assigns different weights to these words, allowing the model to focus on the most relevant parts of the input.
3.  **Contextual Understanding:** By dynamically adjusting these weights, the attention head helps the model understand the context and relationships within the sequence.

**It’s important to note:** The text doesn’t provide a technical definition of attention heads. It’s a conceptual explanation based on the broader themes of relevance, relationships, and human interaction within language models.

To get a more detailed explanation, you'd need to consult resources specifically about the Transformer architecture and the mechanics of attention.","des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses die Entscheidungsgewalt haben,
der Mensch in diesem Loop
würde nie auf die Maschine 
hören.
Alle möglichen Effizienzgewinne, 
die wir hätten,
werden gar nicht realisiert.
Es kommt immer auf die Nutzung 
anderer
will ich sie heute 
sensibilisieren.
Und da gibt es eine 
Riesenliteratur zu
ganz viel aktive Forschung,
wie wir eigentlich erreichen 
können,
dass Menschen vernünftig mit
diesen Maschinen interagieren.
Richtig hinterfragen,
welche Informationen sie wie 
gewichten sollen.
Und wenn Sie jetzt mal an Chat 
GPT
denken und andere große 
Sprachmodelle,
wo Sie der Nutzer sind Sie 
machen Eingaben,
Sie kriegen Vorschläge an 
Ausgaben.
Wir sind noch nicht bei KI 
Agenten,
wo ich sage ich möchte einen 
Urlaub gebucht haben.
Klick macht das alles, sondern 
diese Systeme.
Sie können hingehen und fragen 
wo
kann ich guten Urlaub machen?
Dieses und jenes sind meine 
Präferenzen.
Eine solche und solche Art von 
Urlaub möchte ich machen.
Und dann kriegen Sie einen 
Vorschlag von diesem System.
Ihre Rolle dabei ist dann 
letztlich zu entscheiden
Ist diese Information gut oder 
schlecht?
Ist es eine Information,
die Sie nutzen sollten in
Ihrer Entscheidungsfindung oder 
nicht?
Und da sehen wir mehr und mehr,
dass ich eben diese Aber den umgekehrten Effekt.
Wir haben die Behavioral,
diese kognitive Komponente 
davon,
die sich eigentlich in unserem 
Kopf geschaffen wird,
wo wir diese Intention to use 
entwickeln,
die wiederum von diesen drei 
Faktoren in diesem Modell
hier ganz grob auf einem sehr 
hohen Level.
Den Zusammenhang erklärt wird.
Und diese Beziehungen hängen
wiederum von den Moderatoren ab.
Ja, ja, erklären, warum das was 
Negatives ist.
Muss ich sie an das Papier 
verweisen, dass keine Ahnung,
wann ich das letzte Mal gelesen 
habe,
steht da bestimmt drin. Kann ich 
ihnen hochladen?
Bzw. Das ist glaube ich sogar 
Open Source.
Das hat mittlerweile glaube ich 
70.000
Situationen oder so,
also das ist wirklich eines der 
fundamentalen Modelle,
wo wir immer noch versuchen zu 
verstehen,
was beeinflusst eigentlich die 
Nutzung von wie gesagt,
Technologie breiter gefasst, 
aber für uns ist relevante.
Mit dem Schwerpunkt unseres 
Kurses sprechen wir natürlich
über die sogenannte Mensch 
Maschine Kollaboration,
was letztlich ist die Nutzung 
und
Akzeptanz von der KI 
Technologie.
Wir versuchen uns in der 
Wirtschaftsinformatik
gerade so ein bisschen wieder 
auf das Neue,
auf die neue Kuh, die durchs
Dorf getrieben wird, zu 
konzentrieren.",36.38728094,0,"The generated answer gives a fabricated, non-technical explanation. The ground truth gives the correct technical explanation. The two are completely different.",0,"The generation is unfaithful as it fabricates an explanation. The context does not contain any information about 'attention heads'. The answer attempts to synthesize an explanation from unrelated concepts in the text like 'moderators' and 'human in the loop', but these are not technically related to the attention mechanism.",0,"The context is irrelevant and does not contain information about how an 'attention head' works. It discusses concepts like mediation and moderation in technology adoption models [cite: 772, 774] and human interaction with systems like ChatGPT[cite: 781], but it lacks any technical explanation of the attention mechanism."
What is Word2Vec and what is its function?,Word2Vec is a simple neural network that is used to generate word embeddings from a text corpus.,"Based on the provided text, here’s what Word2Vec is and its function:

**What is Word2Vec?**

Word2Vec is a simple neural network that generates word embeddings given a text corpus. The core idea is to predict words from their surrounding context. The learned weights of the network become the word embeddings.

**What is its function?**

Its function is to create a fixed dictionary of vectors for words. Words that appear in similar contexts will have similar embeddings (vectors). This allows you to represent words as numerical vectors, which can then be used in various natural language processing tasks.

**Key aspects from the text:**

*   It’s trained on large datasets (like Wikipedia).
*   It uses a continuous bag of words (CBOW) approach, predicting surrounding words from a word itself.
*   It often uses negative sampling to speed up training.","17. Juli 2025
Learning word embeddings with Word2VecWord2Vec: simple neural networkto generate word embeddings given a text corpus•Idea: predict words from their surrounding context•Learned weights are embeddings•Through optimization: words in similar context have similar embeddings•Gives us a fixed dictionary of vectors for words we can utilizeIntuitive example: assume we have a text corpus with two sentences
15
Training dataPizza is awesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layer Output layer 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
Word2Vec example (continuous bag of words version)
16
Training dataPizzaisawesomePizza is greatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1001
Words àOne-hot Vector One-hot VectoràWords0100
Training dataPizza is awesomePizzaisgreatPizzaisgreatawesome
PizzaisgreatawesomeInput layerEmbeddings layerOutput layer
1010
Words àOne-hot Vector One-hot VectoràWords0100",16.1652658,1,The generated answer and the ground truth are semantically identical. Both describe Word2Vec as a neural network that generates word embeddings from a text corpus.,1,"The generation is faithful. Every claim madeâ€”that Word2Vec is a simple neural network, that it predicts words from context, that the learned weights are the embeddings, and that words in similar contexts have similar embeddingsâ€”is explicitly stated in the provided text.",1,The context directly defines Word2Vec and its function. It states that Word2Vec is a 'simple neural network to generate word embeddings given a text corpus' [cite: 803] and that its purpose is to create a 'fixed dictionary of vectors for words' [cite: 803] where words in similar contexts have similar embeddings.
What is one potential problem with representing words as indexed tokens?,Lacking interpretability of distance.,"According to the text, a potential issue with representing words as indexed tokens is “Lacking interpretability of distance.” The text suggests using one-hot encoding to address this.","17. Juli 2025
TokenizationMost simple view: words are discrete and independent tokens•Represent words as indexed tokens
What is a potential problem here?•Lacking interpretability of distance •Idea: Employ one-hot-encoding so that distances are all the same
10
WordIndex tokenHello 0… …King 145Queen156… …
WordIndex tokenOne-hot vectorHello 0 [1,0,…,0,0,…,0]… … …King 145 [0,0,…,1,0,…,0]Queen156 [0,0,…,0,1,…,0]… … … 23. Juli 2025
Current LLMs
5
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token
Input sentence 17. Juli 2025
Current LLMs
28
These are the two main components:Feed forward: New representation of weighted embedding (~2/3 of weights)Attention head: “Let embeddings talk to each other”
Predict the likelihood of what the next tokens isOne probability for each possible token",6.137668133,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context presents a table of 'Word' and 'Index token' and then asks 'What is a potential problem here?', immediately followed by the answer 'Lacking interpretability of distance'.",1,"The context directly answers the question. In a slide discussing representing words as indexed tokens, a bullet point asks 'What is a potential problem here?' followed by the answer 'Lacking interpretability of distance'[cite: 805]."
"According to the gradient descent, what two steps are taken to decrease a model's error during training?","To decrease the error, the model's weights (w) are changed in the direction of the negative slope, and the size of this change is scaled by the learning rate (η).","According to the provided text, the two steps taken to decrease a model’s error during training using gradient descent are:

1.  Change 𝑤 in the direction of negative slope.
2.  Scale the size of the change by the learning rate (η, here: 0.01).","10. Juli 2025
Gradient descent: the intuition
22
To decrease error, change 𝑤in the direction of negative slopeScale the size of change by learning rate 𝜂(here: 0.01)
𝑤1=𝑤0−𝜂⋅𝜕ℒ𝑤0𝜕𝑤0=1−0.01⋅82=0.18
ℒ𝑤
𝑆𝑙𝑜𝑝𝑒	𝑎𝑡	𝑤0=𝜕ℒ(𝑤0)𝜕𝑤0
Error(Loss) 10. Juli 2025
Gradient descent: the optimization algorithm
27
We solve the following problem during training
Complexity of problem requires a step-wise solving using gradient descent:1.Initialize 𝑤,𝑏randomly2.Loop until convergence of loss1.Compute gradients: ,ℒ,-, ,ℒ,.(through back-propagation algorithm)2.Update weights and biases: 𝑤=𝑤−𝜂⋅,ℒ,-, 𝑏=𝑏−𝜂⋅,ℒ,.3.Compute new loss: ℒA𝑦𝑤,𝑏,𝑦3.Return weights
argminY,Z	ℒ/𝑦𝑥|𝑤,𝑏,𝑦 10. Juli 2025
Batches and epochs
28
Did you notice? We always computed the error over all training examples•With many training examples computing the derivatives can take long•Solution: Stochastic gradient descent1.Randomly partition overall sample 𝑋 into X1,X2,…,Xm mini-batches2.Conduct gradient descent for each mini-batch sequentially3.Once all mini-batches used once (epoch) start over•Improves convergence, decreases redundancies
𝜕ℒ𝜕𝑤=∑𝑗=1𝑁𝜕ℒ𝑗𝜕𝑤𝑁≈∑𝑗=1𝑀𝜕ℒ𝑗𝜕𝑤𝑀All dataSubset of data
Works because:",8.715072393,1,"The generated answer and the ground truth are semantically identical. Both state that the weights (w) are changed in the direction of the negative slope, scaled by the learning rate (Î·).",1,The generation is faithful. The context explicitly lists the two steps for gradient descent: 'change w in the direction of negative slope' and 'Scale the size of change by learning rate Î·'.,1,"The context directly answers the question. A slide titled 'Gradient descent: the intuition' explicitly lists two steps: 'To decrease error, change w in the direction of negative slope' and 'Scale the size of change by learning rate Î·'[cite: 808]."
"According to the text, what are hyperparameters and what is their primary function in machine learning?","Hyperparameters are variables that determine the structure of a model and its learning process. Their primary function is to regularize the model, which helps to control for overfitting and underfitting. For example, in a neural network, hyperparameters include the number of layers and neurons.","According to the text:

**Hyperparameters** are the number of trees (parameters) and other choices on the model architecture (e.g., tree depth). They determine the structure of the model and the learning process, allowing us to regularize our model and shape over- and underfitting outcomes. Different models and learning algorithms have different hyperparameters.","3. Juli 2025
Hyperparameters
The number of trees (parameters) and other choices on the model 
architecture (e.g., tree depth) are hyperparameters 
Hyperparameters are variables that 
• determine the structure of the model and the learning process
• allow us to regularize our model and shape over- and underfitting outcomes 
• different models and learning algorithms have different hyperparameters
• For example: neural networks’ hyperparameters include
• # layers
• # neurons per layer
• activation functions
By tuning hyperparameters, we can find a “good model complexity”, but 
of course the question is how we do that given the first rule of testing?
24 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
19
Representation +Evaluation +Optimization
Adjustmodel according to an algorithmic procedure until stop criterionis met.In linear regression: minimize the sum of squares (closed form) or gradient descent (iterative optimization).
OLS: min%⁄""&∑'𝑦−𝑦'
𝐿𝛽
𝛽
𝐿
𝛽∗ Journal of the Association for Information Systems 
 
814 
known to the investor. Again, the underlying model was 
a gradient boosted forest for which we automatically 
optimized the same seven hyperparameters on the current 
training dataset using a threefold cross-validation strategy 
with a directed Bayesian search.11 We used the remaining 
30% of the data from Study A to populate the pool of 
borrowers in our simulation, from which we randomly 
drew and matched 50 borrowers with randomly drawn 
investors from the corresponding investor pool. On 
average, the unbiased ML model’s accuracy for the share 
of observations not included in the training before the first 
updating iteration was 68.4%. 12  As we previously 
measured borrowers’ repayment decisions using  the 
strategy method in Study A, we knew whether they would 
make a repayment if there was an investment. Hence, we 
were able to determine whether the investment game 
between a given investor and borrower would result in (1) 
no investment, (2) investment but no repayment, or (3) 
investment and repayment by matching the unconditional 
investor with the conditional borrower decisions. 
Treatments: We introduce d treatment variations",9.525609732,1,"The generated answer and the ground truth are semantically consistent. Both define hyperparameters as variables that determine model structure and the learning process, and mention their function in regularizing the model to control for overfitting and underfitting.",1,"The generation is faithful. All claims made in the answerâ€”that hyperparameters determine model structure and the learning process, that they are used to regularize and control for over/underfitting, and that they include things like the number of layers/neurons in a neural networkâ€”are directly stated in the provided context.",1,The context directly answers the question. It defines hyperparameters as variables that 'determine the structure of the model and the learning process' [cite: 813] and 'allow us to regularize our model and shape over- and underfitting outcomes'[cite: 813].
What are the cores advantages of ensemble methods?,"Improved accuracy, Reduction of overfitting and Handling High Dimensionality.","According to the provided text, the core advantages of ensemble methods are:

1.  **Improved Accuracy:** Ensemble methods result in a prediction accuracy that is better than any individual model could achieve.
2.  **Reduction of Overfitting:** Because they use multiple learners, ensemble methods tend to be more robust and less prone to overfitting.
3.  **Handling High Dimensionality:** Ensemble methods can effectively handle large amounts of data with high-dimensional spaces.","3. Juli 2025
(Dis)advantages of ensembles
Core advantages of ensemble methods include:
1. Improved Accuracy: Ensemble methods result in a prediction accuracy that is better than
any individual model could achieve
2. Reduction of Overfitting: Because they use multiple learners, ensemble methods tend to
be more robust and less prone to overfitting, i.e., typically generalize rather well
3. Handling High Dimensionality: Ensemble methods can effectively handle large amounts
of data with high-dimensional spaces.
Core disadvantages of ensemble methods include:
1. Increased Complexity: Can be computationally expensive and require significant 
resources to train because they involve training multiple models instead of a single one.
2. Interpretability: Ensemble models are difficult to interpret compared to individual models. 
3. Risk of Overfitting with Noise: Although ensemble methods are generally more robust to 
overfitting, they can still overfit if the data is noisy. 
14 3. Juli 2025
Ensemble methods
Ensemble methods rely on different models that are simultaneously applied to the same problem
Predictions are aggregated producing the final prediction
• Hard vote: majority of individual votes
• Soft vote: average individual predicted probabilities and one final vote
• Meta model: train machine learning model to aggregate prediction (stacking)
Models‘ errors should be as uncorrelated as possible so that they cancel out through aggregation
6
Model 1
Model 2
Model 3
Model N-1
Model N
…
Aggregator Overall 
prediction 10. Juli 2025
Last time: ensemble methods
•Different models that are simultaneously applied to the same problem•However: only ”one layer” of models, i.e., no (comprehensive) stacking•Today: layers if modelsfeedingintoeachother•Idea(smallexample):
2
Model 1Model 2Model 3
Model N-1Model N… AggregatorOverall prediction
Model 1, Layer 1Model 2, Layer 1Model 1, Layer 2Model 2, Layer 2Final model(Aggregator)Overall prediction",10.12172461,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly lists the core advantages as 'Improved Accuracy', 'Reduction of Overfitting', and 'Handling High Dimensionality'.",1,"The context directly answers the question. It states that core advantages include 'Improved Accuracy' [cite: 823], 'Reduction of Overfitting' [cite: 823], and 'Handling High Dimensionality'[cite: 823]."
What are some core disadvantages of decision trees?,"Overfitting, instability, and optimization difficulty.","According to the provided text, some core disadvantages of decision trees include:

1.  **Overfitting:** They can quickly lead to overfitting when the tree is too complex (too many nodes, levels, or leaves).
2.  **Instability:** Decision trees may be remarkably unstable conditional on variation in training data, leading to different trees.
3.  **Optimization difficulty:** Algorithms to build trees are typically heuristics that cannot guarantee global optimality.","3. Juli 2025
Last time: decision trees
Core disadvantages of decision trees include:
1. Overfitting: Can quickly lead to overfitting when tree is too complex, i.e., when it has too 
many nodes, levels, or leaves
2. Instability: Decision trees may be remarkably unstable conditional on variation in training 
data, leading to different trees
3. Optimization difficulty: Algorithms to build tree are typically heuristics that cannot 
guarantee global optimality
2
#  No de fau lt: 1 80  (9 0%)
#  Def au lt: 2 0 ( 10 %)
Ov era ll s ample
#  No de fau lt: 5 3 ( 91 .4 % )
#  Def au lt: 5  (8 .6%)
#  No de fau lt: 1 27  (8 9. 4%)
#  Def au lt: 1 5 ( 10 .6%)
Age = Young Age = O ld
#  No de fau lt: 3 2 ( 10 0%)
#  Def au lt: 0  (0 %)
Income 
>  50 .00 0
Income 
< = 5 0.0 00
#  No de fau lt: 2 1 ( 80 .8 % )
#  Def au lt: 5  (1 9.2 %)
#  No de fau lt: 1 26  (9 7. 7%)
#  Def au lt: 3  (2 .3%)
Uni degree =
y es
Uni degree =
no
#  No de fau lt: 1  (7 .7 % )
#  Def au lt: 1 2 ( 92 .3%)
Representation   + Evaluation + Optimization
How good is our current model?
Uncertainty according to target 
variable
How to improve our current model?
Recursively split data
until stop criterion is met (e.g., no 
more reduction in uncertainty) 26. Juni 2025
(Dis)advantages of decision / regression treesCore advantages of decision trees include:1.Handling numerical and categorical variables: Easy to implement and do not require a lot of preprocessing2.Non-parametric: No assumptions about distribution of variables and the classifier structure, allowing to non-linearities3.Interpretability: Easy to understand and to interpret so that one can detect possible inconsistenciesCore disadvantages of decision trees include:1.Overfitting:Can quickly lead to overfitting when tree is too complex, i.e., when it has too many nodes, levels, or leaves2.Instability:Decision trees may be remarkably unstable conditional on variation in training data, leading to different trees3.Optimization difficulty:Algorithms to build tree are typically heuristics that cannot guarantee global optimality
30 26. Juni 2025
Decision tree classifier: terminology
6
Decision trees belong to the hierarchical methods of classification, classifying data through recursive partitioningA decision tree consists of nodes (data) and edges (splits)•Topmost node: root•Connected posterior nodes: children•Connected prior nodes: parents•Node with parent and child: interior node•Node without child: leafAn interior node has at least two child nodesA tree where every non-leaf node has 2 children: binaryEach internal node tests one attribute 𝑋and splits data•Branch from internal node selects value of 𝑋•Prediction in leaf: 𝑝(𝑌!|𝑋!∈𝑙𝑒𝑎𝑓)
Leaf
Root
Interior
LeafInterior
LeafLeaf",9.980744839,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly lists the core disadvantages of decision trees as 'Overfitting', 'Instability', and 'Optimization difficulty'.",1,"The context directly answers the question. It lists the core disadvantages as: 'Overfitting' [cite: 831], 'Instability' [cite: 831], and 'Optimization difficulty'[cite: 831]."
"According to the document, what is the formula for Balanced Accuracy (Bal. Acc.)?",Bal. Acc. = (Sensitivity + Specificity) / 2,"According to the document, the formula for Balanced Accuracy (Bal. Acc.) is:

**Bal. Acc. = (Accuracy + Specificity) / 2** 

It also mentions a related formula:

**F1 = 2 * Sensitivity * Precision**","26. Juni 2025
Technical performance on unseen data (2/2)
Accuracy can be misleading, esp. in case of imbalanced samples, hence, we may also want to consider
Combining these measures results in other important measures:
𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚=𝑻𝑷𝑻𝑷+𝑭𝑵𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏=𝑻𝑷𝑻𝑷+𝑭𝑷
𝑩𝒂𝒍.	𝑨𝒄𝒄.=𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚𝟐 𝑭𝟏=𝟐∗𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚∗𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏𝑺𝒆𝒏𝒔𝒊𝒕𝒊𝒗𝒊𝒕𝒚+𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏
Tree A prediction$𝑦!	 True outcome(𝑦) Tree Bprediction$𝑦""	1 1 11 1 11 0 01 1 01 0 00 1 00 1 00 0 00 0 00 0 0
Confusion matrix
𝑺𝒑𝒆𝒄𝒊𝒇𝒊𝒄𝒊𝒕𝒚=𝑻𝑵𝑻𝑵+𝑭𝑷
What is the Bal. Acc. And F1for the two models?
𝑦=1𝑦=0$𝑦=13 2$𝑦=02 3
𝑦=1𝑦=0$𝑦=12 0$𝑦=03 5
Tree A Tree B accuracy and establishing itself as a reliable tool (Chen,2022). It can collect and evaluate large
amounts of data that may exceed human analytical capacities, enabling AI to provide decision
recommendations (Shaw, 2019).
Modern technologies, including artiﬁcial intelligence solutions, have revolutionized work and
contributed to developing human resources management (HRM) for improved outcomes
(Hmoud and Laszlo,2019). One signiﬁcant area where their impact is felt is in the recruitment
process, where AI implementation can potentially provide a competitive advantage by enabling a
better understanding of talent compared to competitors, thereby enhancing the company’s
competitiveness (Johansson and Herranen,2019).
AI receives commands and data input through algorithms. While AI developers believe their
algorithmic procedures simplify hiring and mitigate bias, Miasato and Silva (2019) argue that
https://doi.org/10.1057/s41599-023-02079-x OPEN
1 College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China.✉email: njuczs@nuaa.edu.cn
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x 1
1234567890():,; 15. Mai 2025
Different types of solutions
10
Balanced Scorecard§Überarbeitung der Inhalte§Wiedereinführung der BSC zum kontinuierlichem Controlling
ISO-Zertifizierung§Interne Audits im HWS und FSS§ISO INTEGRA 1x1§Verpflichtende Besprechung von Prozessen auf Ressort-/ Stabstreffen
Point solutions•Definition:A solution that addresses a single, specific problem or need.•Scope:Narrow and focused.•Integration:Typically standalone; not integrated with other systems.•Use case: Quick wins, easy to implement, often first step in solving a broader challenge. •Example:Fraud detection model (Anomaly detection)Application solutions•Definition:A complete software or tool that solves a broader problem•Scope: Medium; focused on a domain or function•Integration: May integrate with other applications or systems.•Use case: Useful when a domain-specific tool is needed •Example: Customer support app (Classification + UI + reporting)",8.094142675,0,"The generated answer is incorrect. It claims the formula for Balanced Accuracy is (Accuracy + Specificity) / 2, while the ground truth correctly states it is (Sensitivity + Specificity) / 2.",1,The generation is faithful. The context provides the exact formula: 'Bal. Acc. = (Sensitivity + Specificity) / 2'. The second formula for F1 is also present.,0.8,"The context contains the correct formula but with a slight variation from the ground truth. It states 'Bal. Acc. = (Sensitivity + Specificity) / 2'[cite: 841]. The ground truth answer is identical. However, the generated answer incorrectly states 'Bal. Acc. = (Accuracy + Specificity) / 2'. The context itself is perfectly relevant and correct."
"According to the text, what is information gain?",Information gain is the improvement in an evaluation measure (such as the reduction in entropy) that is achieved by splitting the data at a parent node (level t) into its child nodes (level t+1).,"The text doesn't explicitly define ""information gain."" It mentions ""information gain: example"" in the context of choosing an attribute during data analysis (specifically, node t𝐺𝑎𝑖𝑛()*=...). It seems to be a technical term related to decision tree algorithms, but the text doesn't elaborate on its meaning.","26. Juni 2025
Information gain: example
12
We choose the attribute with the largest information gain, i.e., Income
ID AgeIncomeRepayment1 Old HighYes2 YoungHighYes3 Old HighYes4 Old HighNo5 YoungHighYes6 Old LowNo
Data at node t𝐺𝑎𝑖𝑛()*=0.92−46⋅1+26⋅0=0.25
𝐺𝑎𝑖𝑛+,-./*=0.92−56⋅0.72+16⋅0=0.32 integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können.",7.575633526,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct definition.",1,The generation is faithful. The context does not define 'information gain' but gives an example calculation. The generated answer correctly reports that the context does not provide a definition.,0,The context is irrelevant. It does not define 'information gain'. It provides an example calculation for 'Gain' related to different attributes in a dataset [cite: 852] but offers no definition of the underlying concept.
What is KNN used for?,KNN is a supervised learning algorithm used for classification and regression.,The provided text doesn't contain information about what KNN (K-Nearest Neighbors) is used for. It discusses general concepts of digital nudges and a technology called GPT.,"Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 2017 ). They are minimally invasive interventions that 
maintain freedom of choice (Thaler and Sunstein 2003 ) 
and can enhance economic efficiency by facilitating 
superior choices with minimal expenditure.
In digital environments, “digital nudges” (Wein -
mann et al. 2016 ) commonly manifest as design features 
in the user interface. Online marketplaces deploy digi -
tal nudges to stimulate product purchases, with retai -
lers using a variety of persuasive strategies to bolster 
profits (Ghose et al. 2023 ). However, such nudges can 
undermine consumer benefits by exploiting cognitive 
weaknesses (“dark patterns”) (Narayanan et al. 2020 , 
Posner et al. 2023 ) (e.g., by inducing fear of missing out 
through signals of scarce product availability (Hu et al. 
2019 )). Our work relates to prior research on digital 
nudging in the customer journey (Sch ¨ar and 
Stanoevska-Slabeva 2019 ) to encourage “greener shop -
ping behaviors” (i.e., more environmentally conscious 
shopping, fostering greater awareness and consider -
ation for the ecological impacts of product returns).
Recently, there has been growing interest in “green 
nudging,” which aims to promote environmentally 
friendly behaviors (White et al. 2019 ). Green nudging in einer bestimmten Domäne,
in einem bestimmten Bereich 
angewendet werden kann,
sondern eben in verschiedenen 
Sektoren.
Es wird überall gebraucht.
Also General Purpose und
vorher war mal und ich finde den 
Namen furchtbar.
Aber um sie nicht zu verwirren,
habe ich eine andere als die 
herkömmliche Abkürzung,
die Sie in vielen ökonomischen
Büchern noch finden, nämlich 
GPT.
Das ist General Purpose.
Technologie hat natürlich Open 
AI
uns alle so ein bisschen 
versaut.
Dieses Wort zu benutzen,
insbesondere wenn man irgendwie 
an der
Schnittstelle zu Machine 
Learning arbeitet,
weil alle direkt an das 
Sprachmodell denken.
Deswegen hier diese furchtbare
Abkürzung General purpose
Tonnen general purpose 
technology.
Diese Technologien haben
die gesamte Zivilisation der
Menschheit schon immer massiv 
verändert.
Nicht nur, weil sie auf der 
einen Seite weit
eingesetzt worden sind und
auf der anderen Seite eben über
die Zeit immer besser genutzt 
wurden,
erkannt wurde, wo wir sie noch 
einsetzen
kann Economies of Scale 
eingesetzt haben.
Viele Leute nutzen es wird immer 
knapper.
Sondern auch, weil diese 
Innovationen,
diese Kerninnovationen,
letztlich zu weiteren
komplementären Innovationen 
geführt haben.",5.724995613,0,"The generated answer is a refusal, which is incorrect as the information is available. The ground truth provides the correct answer.",1,"The generation is faithful. It correctly states that the provided text does not contain information about KNN and accurately summarizes the content that is present (digital nudges, GPT).",0,The context is irrelevant. It discusses general concepts like digital nudges [cite: 869] and General Purpose Technologies (GPTs) [cite: 877] but contains no information about the K-Nearest Neighbors (KNN) algorithm.
"According to the document, what are the three components of the training process of supervised machine learning?",Representation +Evaluation +Optimization,"According to the document, the three components of the training process of supervised machine learning are:

*   Representation
*   Evaluation
*   Optimization","12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion•In very general terms, we can distinguish three components of three training process•We will first talk this through using simple linear regression•Same logic will be applied to more complex models
16
Representation +Evaluation +Optimization 12. Juni 2025
Components of supervised MLSupervised machine learning methods fit a model to data based on an evaluation criterion
17
Representation+Evaluation +Optimization
The model architecture that stores the learned knowledge.Now: linear regressionthat comprises learnable the parameters called coefficients.
'𝑦=)𝑓𝒙=+𝛽!++𝛽""𝑥""+⋯++𝛽#𝑥$ Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung.",5.332345009,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context includes a diagram with the three components clearly labeled: 'Representation + Evaluation + Optimization'.,1,The context directly answers the question. A diagram displays the three components of supervised ML as 'Representation + Evaluation + Optimization'[cite: 882].
"In the case study with the Sparkasse, is the combination of a human and a machine advisor better or worse than the machine alone?",Worse.,"According to the document, the combination of a human and a machine advisor (“Mensch plus Maschine”) is perceived as “viel besser und viel vertrauenswürdiger” (much better and much more trustworthy). The customer finds it leads to a reduced risk aversion and has someone to blame if things go wrong.","Ende.
So suggerieren unsere 
Ergebnisse,
wenn Mensch plus Maschine da 
sind.
Nicht weil es denkt es besser, 
sondern einfach,
weil da jemand da ist, der ihnen 
die Hand hält.
Ähm, ja.
Takeaways, das habe ich glaube 
ich,
schon so zusammengefasst.
Es geht vor allem um dieses 
Handholding.
Maschine plus Mensch ist
zumindest nicht schädlich in der 
Beratung,
was die Qualität der Beratung 
angeht.
Aber der Mensch, der Kunde von 
Ihnen,
wird es Ihnen danken
und wird Ihre Empfehlung mehr 
annehmen.
Das ist auch das Wort zum 
Sonntag.
Ich hoffe, Sie konnten sich ein 
bisschen was mitnehmen.
Wieder. Und wir sehen uns 
nächste
Woche mit einem kurzen Quiz
und dann dem Einstieg in
die technische Seite. Vielen 
Dank. technische Performance der 
Maschine,
die für diese beiden Subgruppen 
vergleichbar sind.
Und grundsätzlich ist dieses 
Fehlverhalten von Maschinen,
wenn systematisch basierend auf
bestimmten soziodemographischen 
Faktoren,
die wir rechtlich oder
als Gesellschaft für wichtig 
erachten.
Wenn dort Unterschiede 
vorliegen,
die in der Realität aber 
eigentlich keine Rolle spielen,
also ohne Rechtfertigung,
unterscheidet die Maschine in
ihrer Vorhersagegenauigkeit,
in welchem Maß auch immer wir 
das jetzt messen,
zwischen diesen beiden Gruppen,
dann sprechen wir von 
algorithmischer Diskriminierung.
Algorithmische Diskriminierungen 
gibt es,
ein ganz tolles Papier hier von 
zwei
Kollegen werde ich Ihnen auch 
noch hochladen.
Und diese algorithmische 
Diskriminierung
hat ein ganz großes Problem.
Nämlich wir haben nicht, das es
vorher nicht auch schon hätte 
sein können,
dass Ihr Bankberater in dieser
einen Sparkasse
gegen bestimmte Minoritäten 
diskriminiert hat.
Aber diese Maschinen erzeugen 
oder erzeugen dieses
Muster auf eine Art und Weise,
auf eine skalierbare Art und 
Weise,
wie sie eine einzelne Person nie 
hätte darstellen können.
Und das hat auf der einen Seite 
eben technische Aspekte,
aber auch soziale Aspekte. Kunde ist.
Sie haben die Beratung,
die gleiche identische Beratung 
von dem Mensch
plus KI erhalten und
Ihr alternatives Ich von der 
reinen KI,
dem Robo Advisor das erhalten.
Dann haben Sie in dem Mensch 
plus Maschine Fall.
Haben Sie der Maschine 
signifikant,
haben Sie dem der Beratung 
signifikant häufiger.
Das sind zwei verschiedene 
Sachen
der Beratung des 
Risikoeinschätzung
und die eigentliche 
Investitionsempfehlung
haben Sie dem signifikant 
häufiger gefolgt.
Jetzt können Sie fragen Ist es 
gut oder schlecht,
dass Sie sagen Na ja,
wenn mensch dabei ist, finde 
ich, dass alles
viel besser und viel 
vertrauenswürdiger.
Nachricht ist gut, denn der 
Mensch hilft Ihnen,
exzessive Risikoaversion zu 
umgehen.
Und es scheint so ein bisschen,
wie der Kommilitone vorhin 
gesagt hat zu sein,
dass wenn mensch dabei ist und 
sie 100 € verlieren,
dann haben Sie wenigstens 
jemanden,
auf den sie sauer sein können.
Wenn Sie der Maschine gefolgt 
sind,
hilft Ihnen das nicht ganz so 
viel.
Das ist tatsächlich so ein 
bisschen intuitiv.
Auch die der Mechanismus,
auf den alles hindeutet.
Wir haben uns angeschaut. Glaubt 
der Mensch einfach.
Das glaubt der Kunde, dass 
Maschine
plus Mensch besser berät.
Also dass es genauer ist,",9.203197241,0,"The generated answer claims the combination is 'much better', which is what the *customer perceives*. The ground truth correctly answers the question from an objective performance standpoint, which is that the combination is actually worse than the machine alone. The generation confuses subjective perception with objective performance.",1,"The generation is faithful. All claimsâ€”that the human+machine option is perceived as 'much better and much more trustworthy', that it reduces risk aversion, and provides someone to blameâ€”are explicitly stated in the provided context.",0.8,"The context is highly relevant but presents a slightly different perspective than the generated answer implies. The context states that customers find the human-plus-machine combination 'viel besser und viel vertrauenswÃ¼rdiger' (much better and much more trustworthy) [cite: 906], which leads them to follow the advice more often [cite: 905] and helps them overcome excessive risk aversion[cite: 907]. This indicates the combination is 'better' from a customer trust and adoption perspective, directly contradicting the 'Worse' from the generated answer."
"What is the concept of 'Mixed Initiative' in human-AI interaction, as exemplified by a smartwatch asking if it should record a workout?","'Mixed Initiative' is a middle ground between full automation and full manual control, where the system proactively suggests an action, but the human still makes the final choice to accept or reject it.","Based on the provided text, the concept of “Mixed Initiative” in human-AI interaction is being explored through the Sparkasse study. Here’s a breakdown of what it means in this context:

**It’s about a dynamic, collaborative approach where the human and the AI *both* propose and drive the interaction, rather than one solely dictating the process.**

Here's how it’s illustrated by the smartwatch example:

*   **Initial AI Proposal:** The smartwatch *initially* asks, ""Should I record a workout?"" – This is the AI proposing a course of action.
*   **Human Response Determines Next Step:** The human’s response (yes or no) then *determines* the next step. If the human says yes, the AI continues to gather data. If the human says no, the AI stops.

**The key is that the human isn't just passively accepting the AI’s suggestion; they are actively participating in deciding what happens next.**

**The Sparkasse study is investigating whether customers prefer a fully automated (robo-advisor) approach or a collaborative “Human AI Team” approach, where the human and AI work together.**

**Important Note:** The text doesn't explicitly define ""Mixed Initiative"" as a formal term. It’s being demonstrated through the study’s design – a system where the human and AI take turns driving the interaction based on their responses.

Do you want me to elaborate on any specific aspect of this concept, such as the goals of the Sparkasse study or the implications of this approach?","So legen wir los. Herzlich 
willkommen.
Vorlesung fünf sozusagen.
Fast schon Bergfest, oder?
Heute ist, glaube ich, Bergfest,
wenn man die restlichen 
Vorlesungen zusammenzählt.
Auch weil ich jetzt schon 
mehrfach gefragt wurde.
Dazu mit der Klausur.
Das hier ist jetzt die letzte
Vorlesung aus diesem ersten 
Teil,
wo es darum geht, ein 
grundsätzliches
Verständnis dafür zu schaffen,
was KI bewirken kann,
wie Menschen mit KI 
zusammenarbeiten können,
was potenzielle Schwachpunkte 
und Probleme sind,
wenn wir KI einsetzen.
Heute sprechen wir über Mensch 
Maschine Interaktion,
bauen sozusagen auf der 
Erklärbarkeit.
Vorlesungen von vor zwei Wochen
auf und dann ist dieser erste 
Block abgeschlossen.
Dann fängt es nächste Woche
an mit dem eher technischen 
Blog,
wo wir jetzt wirklich uns dann 
damit befassen werden,
wie wir diese Systeme bauen,
was die Mathematik dahinter ist,
wo die Stärken und die
Schwächen von bestimmten 
Modellen liegen.
Und zu Beginn der nächsten
Vorlesung werden wir dafür dann 
aber zum Abschluss dieses
ersten Blockes so eine Art 
kleinen Recap machen,
wo ich Ihnen auch
ein paar Fragen interaktiv
mit der Gruppe durchspielen 
werde.
Von der Struktur der Frage 
können Sie dann auf die
Struktur der Fragen in was man denken sollte.
Hier zum Beispiel von Microsoft 
Guidelines
für Human Centered AI,
also Guidelines an, was sie in 
welchen
Process Steps immer wieder 
denken sollten.
Ich werde das jetzt nicht hier 
alles vorlesen.
Die Idee ist wirklich, dass sie 
schon in
der bevor überhaupt mit dem 
System interagiert wird.
Bestimmte Fragen immer stellen,
immer wieder sich klar machen 
was kann das System
und nicht das Ganze als einmal 
sehen,
sondern über die Zeit dynamisch 
immer wieder schauen.
Interagieren Menschen 
ausreichend damit?
Muss das System irgendwo 
abgewandelt werden.
Immer den Menschen in den 
Mittelpunkt zu stellen.
Irgendwelche Fragen bis hierher?
Weil die letzten Minuten und 
dadurch,
dass ich mich länger bei dem 
Output aufgehalten habe,
als ich gedacht habe,
wird das jetzt in fünf Minuten 
abgehandelt,
habe ich ihn noch mal wieder 
eine Case Study
mitgebracht von einem Papier von 
Olli Hinz,
mir und zwei Kollegen aus Paris 
von der Chaussee,
wo wir uns hier mit der 
Sparkasse tatsächlich
im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob im Taunus angeschaut haben,
wie Menschen tatsächlich mit KI 
interagieren.
Ob es eigentlich gut ist,
wenn Ihr Sparkassenberater eine 
KI benutzt,
um Ihnen Investmentberatung zu
geben und wie Sie eigentlich
darauf reagieren oder ob
das Ganze vielleicht 
automatisiert ablaufen sollte.
Das Papier ist jetzt
also kommt bald im Management 
Science Journal raus.
Wie gesagt, die letzten fünf 
Minuten.
Aber vorher noch mal 
irgendwelche
Unklarheiten zu dem Thema,
was wir oder Themen,
die wir bisher besprochen haben.
Cool. Wie gesagt, wir haben
uns in diesem Projekt
Sparkasse kam auf uns zu und hat 
gesagt naja,
wir haben hier eine Idee
oder wir möchten gerne mal 
rausfinden,
ob unsere Kunden eigentlich eher 
wollen,
dass sie von einem Robo Advisor 
beraten werden,
also einem automatisierten 
System,
wo kein Mensch mehr dabei ist 
oder von Mensch Berater,
der mit der KI zusammenarbeitet
und gemeinsam sozusagen als
Human AI Team diese Beratung an 
den Kunden gibt.
Der Kunde ist eben darüber 
informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,",27.02820277,1,The generated answer and the ground truth are semantically identical. Both describe the concept as a middle ground where the system suggests an action but the human makes the final choice.,1,"The generation is faithful. It correctly describes the smartwatch example provided in the context, where the system proposes an action ('Should I record a workout?') and the human's response determines the next step, as an illustration of the concept being discussed in the Sparkasse study.",0.3,"The context is tangentially related but does not define 'Mixed Initiative' clearly. It gives the smartwatch example where the system proactively asks the user a question ('Soll ich das eigentlich aufzeichnen?')[cite: 943], but it doesn't use the term 'Mixed Initiative' to describe this specific interaction. It uses the term later in a more conceptual way related to a paper by Horvitz[cite: 948], but the connection to the smartwatch example is not explicit."
What did the speaker say about the possibility of the smartwatch automatically starting to record a workout?,The speaker said the smartwatch could estimate the probability that the user wants their sport recorded.,"The speaker explained that the smartwatch could potentially assess the probability that the user would *want* to record their workout (based on things like their pulse). However, it wouldn’t automatically start recording without the user’s consent. They described this as “Not Action,” meaning the smartwatch wouldn’t initiate the recording itself.","Wenn Sie entscheiden würden, 
nicht zu delegieren,
dass Sie auf jeden Fall das 
selber machen, hilft es ja.
Ja, das ging jetzt gerade,
was sie gesagt hatten.
Naja, muss es denn immer das 
eine oder das andere sein?
Nee, muss es natürlich nicht.
Denken Sie an diejenigen im 
Raum,
die irgendeine Smartwatch haben.
Wenn Sie Sport machen und 
plötzlich
das System Sie fragt Soll ich 
das eigentlich aufzeichnen?
Sind Sie gerade am Sport machen?
Das ist irgendwie so eine Art 
Mischung davon.
Also weder das System selber 
entschieden,
jetzt irgendwas aufzuzeichnen 
ohne Ihre Einwilligung.
Sie haben also diese 
Entscheidung
des automatisierten 
Aufzeichnungs nicht
an die Maschine delegiert.
Aber Sie haben auch nicht aktiv 
gesagt Na ja,
ich gehe jetzt hin und such das 
raus.
Sondern die Maschine von sich 
entscheidet,
ihnen etwas vorzuschlagen in the 
first place.
Und das nennen wir grundsätzlich
The Idea of Mixed Initiative.
Und es geht auch schon auf ein 
sehr frühes
wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.
Und die Idee, die hier in diesem 
Paper auf einer
Informatik Konferenz postuliert 
wird, ist.
Naja, lassen Sie uns mal 
annehmen,
das System könnte die 
Wahrscheinlichkeit einschätzen,
dass Sie ein bestimmtes Goal 
haben.
Die Maschine hat irgendeine 
Möglichkeit vorherzusagen,
was die Wahrscheinlichkeit,
dass Sie gerne Ihren Sport 
aufgezeichnet hätten.
Und jetzt haben wir hier noch 
gegebene Evidence.
N Lassen wir mal weg.
Intuition dahinter ist wenn die 
uhr Ihren Puls misst,
kann die uhr eine Vorhersage 
darüber machen.
Was ist denn eigentlich egal?
Möchten Sie es aufgezeichnet 
haben?
Gegeben Ihren Puls 
beispielsweise
so und das haben die in ein 
Modell gepackt,
was eigentlich sehr simpel ist 
dieses Modell,
wo gesagt wird Naja,
wir haben hier unten unsere 
Wahrscheinlichkeit,
dass ein Nutzer ein bestimmtes 
Ziel verfolgt.
Und entweder haben wir die 
Wahrscheinlichkeit,
dass dieses eine Goal eben nicht 
verfolgt wird.
Was heißt die Wahrscheinlichkeit 
ist
null Wahrscheinlichkeit Null oder die uhr macht einfach 
nichts in Aktion.
In diesen Extremfällen Jetzt 
gehen wir gleich
wieder in die Mitte davon.
Aber Sie laufen und Sie möchten,
dass Laufen aufgezeichnet haben 
Sie laufen, möchten,
dass nicht aufgezeichnet haben 
und
die Maschine kann selber die 
Aktion machen,
das aufzuzeichnen oder die uhr 
macht das eben nicht.
Und dann haben wir vier 
verschiedene Fälle und
das versuchen wir wieder über 
Nutzen darzustellen.
Ganz, ganz allgemein.
Der Nutzen, den ich davon habe,
wenn die uhr die Aktion 
durchführt
und die Messung beginnt, 
gegeben.
Ich möchte auch, dass diese 
Messung gemacht wird.
Wenn ich Sport mache versus die
uhr macht das einfach
und eigentlich will ich das gar 
nicht.
Umgekehrt hier und das ist wer 
kennt dieses
Symbol? Was bedeutet das?
Genau. Das ist aus der 
Mathematik der Logik.
Wenn Sie dieses seitlich 
gedrehte Liter sehen,
heißt das immer Nicht immer.
Nicht a, nicht Action.
Sie können eigentlich hier,
um das Beispiel zu 
vervollständigen.
Sie möchten gerne, dass 
aufgezeichnet wird,
aber die uhr macht es nicht 
automatisch.
Not Action.
Und ein anderer Fall ist,
Sie möchten es nicht und die uhr 
macht es nicht.
Diese vier Extremfälle haben wir 
jetzt wieder.",9.087941408,1,The generated answer and the ground truth are semantically identical. Both state that the smartwatch can estimate the probability that the user wants to record their activity.,1,"The generation is faithful. The context explicitly states the system would 'die Wahrscheinlichkeit einschÃ¤tzen, dass Sie ein bestimmtes Goal haben' (estimate the probability that you have a certain goal) based on evidence like pulse, and that it would not automatically start recording ('Not Action').",1,"The context directly answers the question. It states that the smartwatch could 'die Wahrscheinlichkeit einschÃ¤tzen, dass Sie ein bestimmtes Goal haben' (estimate the probability that you have a certain goal) [cite: 953], based on evidence like the user's pulse[cite: 956]."
The lecture describes two extremes in human-machine collaboration. What is 'Algorithm Aversion'?,"'Algorithm Aversion' is the observation that people often consciously choose not to use a helpful algorithm, even when they know it is objectively more accurate than their own opinion or that of other humans.","The provided text does not contain information about “Algorithm Aversion.” It focuses on topics like mitigating bias in AI, fair representations, and measuring discrimination in algorithmic decision-making.","Mitigating unwanted biases with adversarial 
learning. Proceedings of the 2018 AAAI/ACM 
Conference on AI, Ethics, and Society (pp. 335-
340).  
Zhao, H., & Gordon, G. (2019). Inherent tradeoffs in 
learning fair representations. Proceedings of the 
33rd Conference on Neural Information 
Processing Systems.  
Žliobaitė, I. (2017). Measuring discrimination in 
algorithmic decision making. Data Mining and 
Knowledge Discovery, 31(4), 1060-1089. acteristics, such as race and gender (Jackson, 2021). When
assessments consistently overestimate or underestimate a parti-
cular group’s scores, they produce “predictive bias” (Raghavan
et al.,2020
). Unfortunately, these discriminatory results are often
overlooked or disregarded due to the misconception that AI
processes are inherently“objective” and “neutral.”
REVIEW ARTICLE HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | https://doi.org/10.1057/s41599-023-02079-x
2 HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x Policy, 8(1), 78-115.  
Wong, K. F. E., & Kwong, J. Y. (2018). Resolving the 
judgment and decision -making paradox 
between adaptive learning and escalation of 
commitment. Management Science , 64(4), 
1911-1925.  
Wong, P. H. (2020). Democratizing algorithmic 
fairness. Philosophy & Technology, 33(2), 225-
244.  
Woodworth, B., Gunasekar, S., Ohannessian, M. I., & 
Srebro, N. (2017). Learning non-discriminatory 
predictors. Proceedings of the Conference on 
Learning Theory (pp. 1920-1953).  
Yao, S., & Huang, B. (2017). Beyond parity: Fairness 
objectives for collaborative filtering. 
Proceedings of the 31st Conference on Neural 
Information Processing Systems.  
Zafar, M. B., Valera, I., Rogriguez, M. G., & 
Gummadi, K. P. (2017). Fairness constraints: 
Mechanisms for fair classification. Proceedings 
of the  20th International Conference on 
Artificial Intelligence and Statistics  (pp. 962-
970).  
Zhang, B. H., Lemoine, B., & Mitchell, M. (2018). 
Mitigating unwanted biases with adversarial 
learning. Proceedings of the 2018 AAAI/ACM 
Conference on AI, Ethics, and Society (pp. 335-
340).  
Zhao, H., & Gordon, G. (2019). Inherent tradeoffs in 
learning fair representations. Proceedings of the",6.107293844,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct definition of Algorithm Aversion.",1,"The generation is faithful. It correctly states that the provided text does not contain information about 'Algorithm Aversion' and accurately summarizes the actual content of the text (mitigating bias, fair representations, etc.).",0,"The context is completely irrelevant. It contains a list of references and brief mentions of topics like 'learning fair representations' [cite: 986] and measuring discrimination[cite: 973], but it makes no mention of 'Algorithm Aversion'."
What are the two extremes of observation regarding human-machine collaboration?,Algorithm aversion and Algorithm appreciation.,"According to the text, the two extremes of observation regarding human-machine collaboration are:

*   **Algorithm aversion:** Preference for human help/decisions over AI ones.
*   **Algorithm appreciation:** Preference for AI help/decisions over human ones.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT Die System eins Muster.
Eine sehr schöne Interpretation
dieser grundlegenden Idee,
dass wir unterschiedliche
Lösungsstrategien gesucht und 
gefunden haben.
Durch unser Lernen, durch unsere 
Erfahrungen,
die wir im Leben individuell 
gemacht haben.
Beispiele von Mensch Maschine 
Interaktion im Alltag.
Das hier ist insbesondere
für Softwareentwicklung 
natürlich hochgradig 
interessant.
Ich fange an, Code zu schreiben.
Wenn Sie mit Charlotte,
die in der Übung die
Notebook Files schreiben und das 
mit
einem Pro Account auf Google 
Collab machen.
Dann fangen sie an, was zu 
schreiben und plötzlich
stehen die nächsten 20 Zeilen 
Code schon da,
weil die Maschine antizipiert,
was sie eigentlich schreiben 
wollen und
Sie klicken nur noch Enter und
der ganze Code ist plötzlich 
schon da.
Diese Suggestions Johns 
eigentlich.
Wenn wir uns hier wieder
auf diese zwei verschiedenen 
sehr
dichotomen Möglichkeiten der
Zusammenarbeit konzentrieren,
haben wir über diese
Code Recommendations
eigentlich 
Entscheidungsunterstützung,
wo das System uns sagt Nein,
das ist eine Möglichkeit,
wie man diesen Code weiterführt.
Auf der anderen Seite haben
wir unsere ganzen großen 
Sprachmodelle,
mit denen wir tagtäglich 
zusammenarbeiten,",7.044769049,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly defines 'Algorithm aversion: Preference for human help/decisions over AI ones' and 'Algorithm appreciation: Preference for AI help/decisions over human ones'.,1,The context directly answers the question. A slide explicitly defines 'Algorithm aversion' as the 'Preference for human help/decisions over AI ones' and 'Algorithm appreciation' as the 'Preference for AI help/decisions over human ones'[cite: 991].
What were the developers struggling with regarding Windows eight?,The developers of Windows 8 were struggling with the bad user interface.,"According to the text, the developers were struggling with the user interface of Windows 8. Users found it ""totale Katastrophe"" (a total disaster) and were ""total überfordert"" (completely overwhelmed).","5. Juni 2025
Acceptance and appropriate use of technology
2012, Microsoft released Windows 8 that significantly changed the user interface•Many of its features are standard nowadays•Yet, considered one of Microsoft’s worst OS•In 2021 its market share was about 1% while its predecessor Windows 7 was still at 15%
4
Remember Windows 8?
What does that imply for AI? the loop dahingehend Sie 
entscheiden,
Ich transferiere 
Entscheidungsgewalt in diesen
automatisierten Fällen oder Sie 
behalten immer
die Entscheidungsgewalt,
müssen sich aber entscheiden Wie
viel Gewicht geben Sie 
eigentlich?
Der Information von der Maschine 
macht es Sinn.
So fangen wir aber
mal einen Schritt weiter vorne 
an Erinnern Sie sich noch?
Ich weiß nicht, ob Sie dafür 
noch zu jung sind, Sogar.
Aber erinnern Sie sich noch
an diese Version von Microsoft?
Microsoft acht kam raus
2012 und es war ein vollkommenes 
Desaster für Microsoft.
Warum? Naja, viele der User 
waren total überfordert.
Das war eigentlich grundsätzlich 
ein
tolles System mit ganz vielen 
Neuerungen,
mit ganz vielen Funktionen,
wo sich die Entwickler gedacht 
haben Naja,
das sollte meinen Nutzern 
gefallen.
Das macht alles viel einfacher.
Da soll der Entscheidungsfindung 
Prozessen
arbeiten helfen.
War alles natürlich well 
intended.
Problem war, wir alle fanden das 
nicht so cool.
Das war bei mir auch der 
Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen, Zeitpunkt,
wo ich dann komplett auf Mac 
übergesprungen bin.
Und selbst neun Jahre später war 
es immer noch so,
dass der Vorgänger Windows 
sieben
2021 hatte Windows sieben
immer noch 15 % Marktanteil von 
Windows Systemen,
wohingegen Windows acht, was
viele neue gute Funktionen 
hatte,
immer noch nur um die 1 % war.
Und es ist wieder considered 
eines der
schlechtesten Operating Systeme, 
die es gibt.
weil es einfach niemand benutzt 
hat.
Jetzt natürlich die Frage Wenn 
ich Ihnen hier
so eine anekdotische Geschichte 
erzähle,
um Sie so ein bisschen emotional 
abzuholen,
was glauben Sie, hat das mit KI 
zu tun?
Wie könnte ich aus dem 
vorherigen Beispiel,
dass die KI mir ein Signal gibt?
Ich sehe das und es
ist eigentlich ein hilfreiches 
Signal.
Wie kann ich diese Idee hier auf 
diesen
anekdotischen Fall aus der 
Geschichte
mappen? Wo ist die Verbindung?
Genau hier.
Wir haben ein tolles System. Es 
kann uns helfen.
Viele der Features im
Übrigen sind heute in einer 
anderen Darstellung,
in einer anderen Art und Weise 
in
den Microsoftsystemen immer noch 
da.
Die wurden dort entwickelt für 
Windows acht,
aber die Oberfläche war totale 
Katastrophe.
User Interface Wir hatten 
Entwickler,
die sich ausschließlich damit",6.613144636,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly states that the user interface for Windows 8 was a 'totale Katastrophe' (total disaster) and that users were 'total Ã¼berfordert' (completely overwhelmed).,1,"The context directly answers the question. It states that for Windows 8, 'die OberflÃ¤che war totale Katastrophe' (the user interface was a total disaster) [cite: 1015] and that 'viele der User waren total Ã¼berfordert' (many of the users were completely overwhelmed)[cite: 1003]."
"In a case study about the role of the human, what was the investment amount for the P2P vehicle in the experiment?",100,"The text doesn't specify the exact investment amount for the P2P vehicle in the experiment. It only mentions that investors were considering whether to invest with a particular borrower with a ""festgeschriebenen Zins"" (fixed interest rate).","effect corresponds to annual savings in processing costs 
of approximately $340,000 and translates into an 8.7% 
increase in profits. From an industry-wide perspective, 
based on a projected $25.1 billion in online returns pro -
cessing expenses in the United States for 2023 (Zheng 
2023 ), the decrease would translate to annual cost sav -
ings of approximately $650 million. From a societal per -
spective, based on U.S. figures from 2022, the 2.6% 
return decrease would result in a reduction of 624,000 
metric tons of CO
2 
emissions (Calma 2023 ). This reduc -
tion is roughly equivalent to the annual emissions from 
the electricity consumption of 121,000 U.S. homes.
1
Second, further analyses using the CML model 
reveal that the green nudge substantially reduced 
returns for approximately 60% of customers but led to 
an increase for the remaining 40%, suggesting that 
green nudging can occasionally backfire. This backfir -
ing effect, which has also been observed in prior studies 
(e.g., Hagmann et al. 2019 , Osman 2020 ), diminishes 
the intervention’s effectiveness from both business and 
societal perspectives. An off-policy evaluation provides 
evidence that “smartly” administering a green nudge informiert.
Was wir gemacht haben, ist Wir 
haben
eine Feldstudie gemacht, zwei 
Stufen.
Wir haben für die erste Stufe 
ganz viele Daten gesammelt,
haben KI System gebaut,
was bewertet hat, ob ein 
bestimmtes Investitionsobjekt,
hier eben P2P loans,
also Privatkredite, finanziert 
werden sollten,
also ob ein Kunde 100 € 
beispielsweise in die in
jemand anderen als Privatkredit
investieren möchte mit einem 
festgeschriebenen Zins.
Und dieses KI System war darauf 
ausgelegt vorherzusagen,
ob dieser Privatkredit dann
tatsächlich zurückgezahlt wird 
oder nicht,
ob ein Ausfall dieses Kredits 
auftreten wird oder nicht.
Das System war eigentlich auch 
ziemlich gut.
Es hat irgendwie so 74 % 
Accuracy gehabt.
Was für so soziales
Verhalten oder menschliches 
Verhalten vorherzusagen,
schon immer ganz gut ist.
Dann sind wir in die Sparkasse 
rein und haben mit
27 Bankern haben
wir hingesetzt und gesagt Hier 
sind
verschiedene von diesen 
Privatkrediten.
Gebt doch mal bitte deine eigene 
unabhängige Meinung an,
ob dieser Kredit und ich gebe 
dir ein
paar Informationen zu dem 
Privatkredit,
ob der zurückgezahlt wird oder 
nicht.
Würdest du das deinem Kunden?
Du als Mensch ohne KI Hilfe?
Würdest du dem Kunden empfehlen investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records 
from all previous iterations and newly collected 
instances of borrowers who received an investment and 
made a repayment decision. 8  The next iteration then 
started, where we again randomly sample d—with 
replacement—and paired 50 borrowers and investors. 
Investors and their decisions : At the outset of each 
simulation prior to the first iteration, we randomly split 
Study B data into two equally sized shares. We used the 
first share (comprising 304 unique individuals with 20 
decisions each, yielding 6,080 discrete observations) to 
train an ML model that simulates whether an investor 
would choose to invest with a particular borrower, using 
31 features (20 personal characteristics of the investor, 
10 characteristics of the borrower, and a repayment 
forecast for the borrower). The employed model is a 
gradient boosted forest, executed using the XGBoost 
library in Python (Chen and Guestrin, 2016). We 
employed an automated process to optimize seven",7.658112526,0,"The generated answer is a refusal, stating the information is not in the text. The ground truth provides a direct, factual answer to the question. The two are semantically opposite.",1,"The generation is faithful. It correctly states that the text does not specify the exact investment amount, only mentioning that investors were considering an investment with a fixed interest rate.",0.9,"The context is highly relevant and almost provides the direct answer. It states that the experiment involved customers investing in P2P loans and mentions an example amount: 'ob ein Kunde 100 â‚¬ beispielsweise... investieren mÃ¶chte' (whether a customer would like to invest 100 â‚¬, for example...)[cite: 1025]. While phrased as an example, it is the only monetary figure mentioned for the investment, strongly suggesting it was the amount used in the experiment."
"According to the document, what did Eric Horvitz advocate in his 1999 paper?",Elegant coupling of automation with direct manipulation by user.,"The document doesn't mention a paper by Eric Horvitz, nor does it detail what he advocated in a 1999 paper. It does, however, describe a conversation about how to approach systems of automation and delegation, suggesting a framework where humans retain control over decisions rather than simply automating everything.","findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. 
Our results have important practical implications, especially considering the impending regulations 
mandating human involvement in ML-supported decision-making processes. 
Keywords: Continuous Updating, Algorithmic Discrimination, Human Discrimination, Feedback 
Loops, Investment Game 
John Qi Dong  was the accepting senior editor. This research article was submitted on  June 22, 2022 and underwent 
three revisions.  
1 Introduction 
With the aim of bolstering economic efficiency and 
societal welfare (Benbya et al., 2020; Teodurescu et al., 
2021), machine learning (ML) systems are augmenting 
human decision -making across diverse domains, 
including bail decisions (Kleinberg et al., 2018), hiring 
processes and student admissions (Horton, 2017; Zhao 
et al., 2020), banking (Fu et al., 2021), and medicine 
(Jussupow et al., 2021). In Europe, Big Data analytics 
software revenue reached US$14.6 billion in 2018 
alone. Wells and Spinoni’s projections (2019) indicate 
an expected annual growth rate of about 8% for this wissenschaftliches Papier 
zurück,
nämlich Anfang der 
Zweitausenderjahre.
Weiß jemand, wer das ist? Hier. 
Mal von ihm gehört?
Es ist der Chief Scientific 
Officer von Microsoft.
Der,
der für sehr viele
wissenschaftliche 
Strategieentscheidungen
letztlich verantwortlich ist.
Da. Der hat nämlich damals genau 
das gesagt,
diese ganze Diskussion zwischen 
entweder
automatisieren oder wir bleiben
im Decision Support.
Das sollten wir gar nicht so
als entweder oder betrachten,
sondern wir sollten es aus einer 
Perspektive betrachten,
dass wir vielleicht Systeme 
haben,
die uns Automatisiert von selbst 
bestimmte
Dinge abnehmen und dann autonom 
agieren.
Und für andere Dinge sollten Sie 
uns vielleicht fragen.
Das heißt, wir geben eigentlich 
die Entscheidung,
ob wir im Automation oder 
Delegation,
im Automation oder Augmentation 
Setting sind,
auch an die Maschine ab.
Und er hat dann sehr intuitives,
sehr intuitiven, konzeptionellen 
Vorschlag gemacht,
wie wir darüber nachdenken 
können.
Das ist jetzt auch wieder das 
Modelldenken,
was ich glaube sehr wichtig ist 
an Universitäten.
Gelernt werden sollte,
dass sie ein grundlegendes 
Schubladendenken haben,
wie sie über solche Probleme 
nachdenken können,
ohne dass wir jetzt keine They were therefore assured of their comprehension and consent.
Additional information
Supplementary informations The online version contains supplementary material
available at https://doi.org/10.1057/s41599-023-02079-x.
Correspondence and requests for materials should be addressed to Zhisheng Chen.
Reprints and permission informationis available athttp://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory",8.499409914,0,"The generated answer is a refusal, which is incorrect as the information is available. The ground truth provides the correct answer.",1,The generation is faithful. It correctly states that the document does not mention Eric Horvitz or his 1999 paper.,0,The context is irrelevant. It does not mention Eric Horvitz or any paper from 1999. The text discusses human involvement in ML-supported decision-making in the context of impending regulations [cite: 1040] but does not attribute any specific ideas to Horvitz.
What is augmentation in the context of human-AI collaboration?,AI serves as a decision support tool; human always makes final decision.,"According to the text, augmentation is where “AI serves as a decision support tool; human always makes final decision.” It’s a mode of collaboration where the AI provides advice, but the human retains control and makes the ultimate decision.","5. Juni 2025
Two modes of human-AI collaboration
Human-AI collaboration: the cooperative and coordinated interaction between humans (mostly non-AI experts) and AI to solve complex problems or achieve certain goalsTwo primary collaboration modes•Augmentation: AI serves as a decision support tool; human always makes final decision•Automation: AI serves as a tool to delegate tasksto; human or AI makes final decision8
AI
Human
Decision
Makes final decision
Gives advice
AugmentationAutomation
Task
“Observes“ task
Observes task
Task
HumanObserves task
AI
Decision
Human makes final decision(no delegation)
AI makes final decision(delegation) 5. Juni 2025
Goal: Human-AI complementarities
9
The objective of human-AI collaboration is achieving complementary performance•Humans plus AI outperform either party acting alone for a set of tasks•Complementary strengths can originate from•Access to different data (e.g., soft signals)•Learning of distinct problem-solving strategies (imperfectly correlated errors) 5. Juni 2025
Many factors shape human-AI collaboration
7
Mahmud et al. 2022
In the context of human-AI collaboration, we often refer to•Algorithm aversion: Preference for human help/decisions over AI ones•Algorithm appreciation: Preference for AI help/decisions over human onesThe occurrence of either depends on many factors that can be sorted under UTAUT",5.742554903,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly defines augmentation as a collaboration mode where 'AI serves as a decision support tool; human always makes final decision'.,1,"The context directly answers the question. A slide presents two primary collaboration modes, one of which is 'Augmentation: AI serves as a decision support tool; human always makes final decision'[cite: 1061]."
What is the mean of the prediction change when considering coalitions of features?,Δ=−0.1,The mean change in prediction (Δ) across all coalitions is −0.1.,"22. Mai 2025
Marginal contributions across coalitionsIdea:•Remove feature across all possible coalitions and compute changes in predictions•Compute a feature’s importance by marginalizing over all its contributionsSimplified example for “savings”:
21
Coalition of 3 features
Δ=−0.07
Coalitions of 2 features
Δ=−0.3
Δ=−0.02
Coalition of 1 feature
Δ=−0.01
Mean: Δ=−0.1
What is the prediction of no feature is present? with modern machine learning methods. This integra -
tion enables the estimation of individual-level differ -
ences between actual outcomes and hypothetical 
outcomes that would have occurred without the inter -
vention (Athey and Imbens 2015 ).
Contrasting with traditional machine learning’s 
focus on predicting outcomes from correlational data 
relationships, CML aims to forecast cause-and-effect 
relationships. This approach enhances the understand -
ing of how interventions affect outcomes of interest 
(Wager and Athey 2018 ). For example, although tradi -
tional machine learning may predict a customer’s likeli -
hood to purchase when exposed to a marketing 
intervention, CML methods determine how the inter -
vention itself alters the customer’s purchase likelihood 
compared with no exposure (i.e., they predict the 
causal effect attributable to the intervention).
Research on the application of CML in practice is still 
in its infancy. To date, only a few studies have applied 
these methods to analyze heterogeneous treatment 
effects. Nevertheless, initial applications are promising. 
For instance, Zhang and Luo ( 2023 ) employ causal for -
ests to explore the varied impact of social media posts availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating.",4.407527685,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context shows a diagram with different delta values for coalitions of features, culminating in a calculation that reads 'Mean: Î”=âˆ’0.1'.",1,The context directly provides the answer. A diagram illustrating the marginal contributions of a feature across different coalitions shows a final calculation: 'Mean: Î”=âˆ’0.1'[cite: 1064].
"According to the document, what is interpretability?",The ability to explain or provide the meaning in understandable terms to humans.,"According to the document, interpretability is “the ability to explain or provide the meaning in understandable terms to humans.”","22. Mai 2025
Some definitionsConfusing nomenclature: explainable / interpretable / transparent models•Interpretability:the ability to explain or provide the meaning in understandable terms to humans•Explainability:explanation as an interface between humans and a decision maker that is both an accurate proxy of the decision maker and comprehensible to humans•Transparency: a model is transparent if by itself it is understandable.Explaining different things:•Predictionsand model behavior (overarching logic the model learned)•Decisions(predictions plus rule that translates into decision)Explanation modes:•Analytic statement:natural language descriptions of elements and context that support decision•Visualizations:highlight parts of data that support the decisions•Cases:give typical/illustrative examples that support the decision•Rejections or alternative choice:counterfactuals that argue against the alternative output
15 des Effekts.
Sehr gut. Wie Ihre Kommilitonin 
hier gesagt hat,
haben wir nicht nur eine 
Mediation.
Wenn wir sagen, der Effekt von 
Performance expectancy
auf actual use wird
mediiert durch behavior 
intention to use.
Also wenn das hier hoch geht,
geht das hier hoch.
Dadurch, dass dieser Faktor
diesen Faktor positiv 
beeinflusst.
Wir haben aber auch noch 
Moderatoren.
Moderatoren sind bestimmte 
Variablen,
bestimmte Informationen,
welche den eigentlichen 
Zusammenhang,
an dem wir interessiert sind, 
beeinflussen.
Das kann zum Beispiel sein,
es wurde hier rausgefunden von
den Forschern oder 
zusammengefasst,
dass wir bestimmte Eigenschaften 
von Personen haben,
wie beispielsweise das Alter, 
das Geschlecht,
die Erfahrung mit Technologie 
und weitere Faktoren.
Und die beeinflussen nicht
direkt den eigentlichen Nutzung
und die eigentliche Akzeptanz,
aber sie beeinflussen die
Beziehung zwischen diesen 
grundlegenden Faktoren.
Beispielsweise umso älter sie 
sind,
umso schwächer ist der
Zusammenhang zwischen der 
Performance Expectancy
und ihrer Intention,
diese Technologie zu nutzen.
Wir haben hier also einen 
negativen Moderator.
Wenn Sie älter sind,
ist es weniger wichtig,
was Ihre Erwartungen an die
mögliche Performance für dieses betriebswirtschaftlich
ökonomische Entscheidungen zu 
treffen.
Wer sieht das nicht?
Noch mal, was ich letztes Mal 
gesagt habe Wenn Sie
was nicht verstehen, ist es 
nicht Ihre Schuld.
Dann habe ich es einfach 
schlecht erklärt.
Deswegen immer raus damit.
Dann gehe ich davon aus, dass 
jeder verstanden hat.
Aber eine andere Frage haben
wir ja theoretisch auch
noch andere 
Entscheidungsmöglichkeiten,
Parametern, die dieselbe 
Qualität liefern.
Vollkommen richtig. Da wäre 
jetzt die Frage,
ob es eine 
Entscheidungsmöglichkeit,
die zwei Parameter verwendet,
besser als eine auf einem 
Papier.
Das ist sehr, sehr gute Frage.
Da gibt es verschiedene Theorien 
zu.
Grundsätzlich, wie wir sehen 
werden,
für den technischen Teil der 
Vorlesung.
Wir können für das gleiche
Problem verschiedene Regeln 
identifizieren.
Verschiedene Machine Learning 
Modelle trainieren.
Diese Machine Learning Modelle 
können bessere oder
schlechtere Performance 
aufweisen.
Wenn sie aber genau die gleiche 
Performance aufweisen,
dann haben wir natürlich in 
einer gewissen Art und
Weise die Wahl, immer darauf 
achtend,
dass die einzige Performance, 
die wir sehen,
aktuell immer Performance auf 
historischen Daten ist.
Das heißt, wir müssen auf der",4.895293236,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly defines interpretability as 'the ability to explain or provide the meaning in understandable terms to humans'.,1,The context directly answers the question. It defines interpretability as 'the ability to explain or provide the meaning in understandable terms to humans'[cite: 1077].
What does the EU AI Act Art 86 demand?,Right to Explanation of Individual Decision-Making.,The provided text doesn't detail the specific demands of EU AI Act Article 86. It only mentions that the European Commission has drafted proposals regarding the influence of algorithmic systems on human rights and established a high-level expert group on AI.,"ethics, and responsibility in AI (Zuiderveen Borgesius,2020). The
Organization for Economic Cooperation and Development
(OECD) has provided recommendations on AI, while the
European Commission has drafted proposals regarding
the inﬂuence of algorithmic systems on human rights. In 2019,
the European Commission established a high-level expert group
on AI, which proposed ethical guidelines and self-regulatory
measures regarding AI and ethics.
Public organizations have played a role in establishing
mechanisms to safeguard algorithmic fairness. The Algorithm
Justice League (AJL) has outlined vital behaviors companies
should follow in a signable agreement. Holding accountable those
who design and deploy algorithms improves existing algorithms
in practice (36KE,2020). After evaluating IBM’s algorithm, AJL
provided feedback, and IBM responded promptly, stating that
they would address the identi ﬁed issue. As a result, IBM
signiﬁcantly improved the accuracy of its algorithm in minority
facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human 22. Mai 2025
EU AI act
10 Retailing 97(4):676–696.
European Union (2016) Regulation EU 2016/679 of the European 
Parliament and of the council of 27 April 2016, article 22. Offi -
cial J. Eur. Union 119:59.
European Union (2021) Proposal for a regulation EU of the Euro -
pean Parliament and of the council of April 21, 2021, laying 
down harmonised rules on artificial intelligence (Artificial Intel -
ligence Act) and amending certain union legislative acts. Official 
J. Eur. Union 119.
Frisch R, Waugh FV (1933) Partial time regressions as compared 
with individual trends. Econometrica 1(4):387–401.
Ghose A, Lee HA, Nam K, Oh W (2023) The effects of pressure and 
self-assurance nudges on product purchases and returns in 
online retailing: Evidence from a randomized field experiment. 
J. Marketing Res. 61(3):517–535.
Goldstein NJ, Cialdini RB, Griskevicius V (2008) A room with a 
viewpoint: Using social norms to motivate environmental con -
servation in hotels. J. Consumer Res. 35(3):472–482.
Goldstein NJ, Griskevicius V, Cialdini RB (2007) Invoking social 
norms: A social psychology perspective on improving hotels’ 
linen-reuse programs. Cornell Hotel Restaurant Admin. Quart. 
48(2):145–150.
Grahl J, Hinz O, Rothlauf F, Abdel-Karim BM, Mihale-Wilson C",6.503876925,0,"The generated answer is a refusal, which is incorrect as the information is available. The ground truth provides the correct answer.",1,"The generation is faithful. It correctly reports that the context does not detail the specific demands of EU AI Act Article 86, only mentioning that the European Commission has drafted proposals and established an expert group.",0,"The context is irrelevant. It mentions the European Commission establishing a high-level expert group on AI [cite: 1096] and discusses the GDPR[cite: 1106], but it does not contain any information about Article 86 of the EU AI Act or a 'Right to Explanation'."
What is a 'selective labels problem'?,It occurs when a machine learning model's prediction influences decisions that prevent the collection of data needed to evaluate the model's accuracy.,"According to the text, the “selective labeling problem” (also referred to as the “endogenous composition of future training data”) can “maintain or even exacerbate inaccurate or discriminatory predictions.” It’s a situation where machine learning models, by making predictions, shape the data they’re then used to learn from, potentially reinforcing bias.","endogenous composition of future training data, a 
problem referred to as selective labeling (Lakkaraju et 
al., 2017; Little & Rubin, 2019), can maintain or even 
exacerbate inaccurate or discriminatory predictions. 
In this paper, we explore the dynamic interplay 
between the continuous updating of ML models and 
algorithmic discrimination when ML predictions 
endogenously shape the composition of additional data 
available for model updating. In particular, our work 
focuses on the role of ongoing discrimination by 
human decision  makers who have the final say in an 
ML-supported decision-making task. A steady stream 
of previous work has examined how algorithmic 
discrimination can reproduce and perpetuate 
discriminatory patterns ingrained in societal structures 
(Lambrecht & Tucker, 2019; Morse et al., 202 2; 
Teodurescu et al., 2021). Some studies have developed 
theoretical arguments concerning the emergence of 
feedback loops when ML models continually learn 
from the discriminatory outcomes they help ed to 
generate (see, for example, Cowgill & Tucker, 2019; 
Ensign et al., 2017; Silva & Kenney, 2019). However, 
as Kordzadeh and Ghasemaghaei (2022) observed in 
their literature review, systematic empirical Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider
social issues in context. It is crucial to make them aware of
attribute selection’s impact on the algorithm (Yarger et al.,2019).
The algorithm engineer plays a crucial role in the entire system,
from setting goals for machine learning to selecting the
appropriate model and determining data characteristics such as
labels. If inappropriate goals are set, bias may be introduced from
the outset (36KE,2020).
An engineer is responsible for developing the algorithmic model.
If they hold certain beliefs and preconceptions, those personal
biases can be transmitted to the machine (Njoto,2020). Although
the device is responsible for selecting employee resumes, it operates
based on underlying programming. The programmer guides the AI
in making decisions about the best candidate, which can still result
in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to case the borrower is male. A comparison of panels for Iterations 1 and 100 for the two measures of algorithmic discrimination provides insights 
into their conditional development across iterations. Abbreviations: Outcome under perf. inf. = outcome under perfect information; Init. lab. imb. 
= initial label imbalance; Human discr. = human discrimination. 
Figure A9. Development of the Mean Share of Game Outcomes by Initial Label Imbalance  
and Degree of Human Discrimination",7.524492264,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context defines the 'selective labeling' problem as something that 'can maintain or even exacerbate inaccurate or discriminatory predictions' when 'ML predictions endogenously shape the composition of additional data'.,1,"The context directly defines the problem. It describes the 'selective labeling' problem as one where 'ML predictions endogenously shape the composition of additional data available for model updating' [cite: 1112], which can 'maintain or even exacerbate inaccurate or discriminatory predictions'[cite: 1111]."
"According to the text, what is one way to control for gender when analyzing a dataset?","By running the dataset through without including gender, but then rerunning it while including gender to compare performance.","According to the text, one way to control for gender when analyzing a dataset is to exclude the gender column altogether and only use features for prediction. The example given is of Amazon, where they removed the gender column to assess performance differences based on gender.","data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately 
for women. Second, it can be created by unrepresentative, 
or imbalanced data (Barocas & Selbst, 2016; Mehrabi et 
al., 2021). When the dataset underrepresents certain 
groups, such as women, the ML model can make more 
errors when predicting for this group. Imbalances occur 
when labels skew significantly within specific subgroups. 
Third, technical constraints or malpractices during model 
development and training may yield discriminatory 
outputs (Friedman & Nissenbaum, 1996; Williams et al., 
2018). Biases may emerge during data integration due to 
inconsistent data formats or when the integration of 
personal information like ethnicity is mishandled (Schelter 
& Stoyanovich, 2020; Williams et al., 2018). It is critical 
to appreciate the sociotechnological nature of algorithmic 
discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and Datensatz,
in dem man alles angibt,
außer das Geschlecht und lässt 
das durchlaufen.
Aber hierbei würden sie Gender 
mit rein nehmen.
Also in einem ersten Schritt,
um das zu kontrollieren.
Genau in diesem Beispiel von dem 
Amazon,
das haben Sie schon richtig 
erfasst,
war es so die haben Gender 
rausgelassen,
hatten aber eine Spalte im 
Datensatz,
wo Gender noch drin war,
haben die Maschine dann
basierend auf allen Features 
außer Gender die
Vorhersage machen lassen und 
konnten dann natürlich
innerhalb ihrer Daten trotzdem 
vergleichen.
Wie ist die Performance dort in 
den Einträgen,
wo wir Männer,
Frauen nonbinär hatten und haben 
das dann verglichen?
Praktisch ja.
Aber es gibt viele Argumente 
dafür,
dass wir diese Features, die 
protected sind,
dass wir eigentlich größere 
Probleme erzeugen,
wenn wir sie den Maschinen nicht 
zur Verfügung stellen,
weil diese Maschinen so mächtig 
sind,
Korrelationen herauszufinden,
die letztlich ablesen lassen,
ob es welches Geschlecht es ist 
und das implizit lernen.
Und das ist sehr, sehr, sehr 
versteckt ist,
wo eigentlich jetzt eine 
Diskriminierung herkommt.
Wenn wir es aber drin lassen,
gibt es bestimmte Methoden, das 
direkt abzulesen.
Aber die Frage was will man denn 
da, Statistics (pp. 166-175).  
Lambrecht, A., & Tucker, C. (2019). Algorithmic 
bias? an empirical study of apparent gender -
based discrimination in the display of stem 
career ads. Management Science, 65(7), 2966-
2981.  
Langer, N., Gopal, R. D., & Bapna, R. (2020). Onward 
and upward? An empirical investigation of 
gender and promotions in Information 
Technology Services. Information Systems 
Research, 31(2), 383-398.  
Leavy, S. (2018). Gender bias in artificial intelligence: 
The need for diversity and gender theory in 
machine learning. Proceedings of the 1st 
International Workshop on Gender Equality in 
Software Engineering (pp. 14-16).  
Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., 
Schafheitle, S., Wildhaber, I., & Kasper, G. 
(2019). The challenges of algorithm -based HR 
decision-making for personal integrity. Journal 
of Business Ethics, 160(2), 377-392.  
Lin, J., & Zhou, D. X. (2017). Online learning 
algorithms can converge comparably fast as 
batch learning. IEEE Transactions on Neural 
Networks and Learning Systems , 29(6), 2367 -
2378.  
Little, R. J., & Rubin, D. B. (2019). Statistical analysis 
with missing data. Wiley.  
Lohia, P. K., Ramamurthy, K. N., Bhide, M., Saha, D.,",7.796616316,0.8,The generated answer describes one valid method for controlling for a variable (running the analysis with and without the gender feature). The ground truth describes a slightly different but also valid method. They are semantically similar approaches to the same problem.,1,"The generation is faithful. The context describes the Amazon example where gender was excluded as a feature for prediction, but the gender column was kept in the dataset to compare performance, which matches the generated answer.",1,"The context directly answers the question. It describes a situation where they 'hatten aber eine Spalte im Datensatz, wo Gender noch drin war, haben die Maschine dann basierend auf allen Features auÃŸer Gender die Vorhersage machen lassen und konnten dann natÃ¼rlich... vergleichen' (but had a column in the dataset where gender was still present, then let the machine make the prediction based on all features except gender and could then of course... compare)[cite: 1138]."
"According to the text, what is one way large language models can exhibit biased tendencies?",They have been optimized to be overly agreeable and to 'say yes to everything.'.,"According to the text, large language models can exhibit biased tendencies by replicating existing patterns of bias and discrimination because they are typically trained to replicate the outcomes achieved by human decision-makers. Additionally, continuous model updating can be impeded by human bias.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” 
initial biases over time given sufficiently diverse and 
unbiased data, human bias es can impede this process. 
This insight is especially relevant in the current era of 
growing political and social divisions in human 
societies (e.g., Allcott et al., 2020). When people 
increasingly adopt biased attitudes and behaviors, 
biased, continuously updated ML models are less inherent to these models, adding a new dimension to 
the algorithmic fairness literature by shifting from 
static, one-time biases to a more temporal perspective: 
a comprehensive understanding of algorithmic 
discrimination and fairness must incorporate not just 
the initial conditions under which a model is trained, 
but also the dynamic environments where these models 
operate and evolve. Our results suggest that continuous 
model updating could serve as a mechanism for 
mitigating bias in ML systems and might act as a force 
to supplement traditional data cleaning and debiasing 
efforts (Shrestha et al., 2019) to reduce discrimination 
risks. 
On the other hand, our simulations reveal that the 
positive impact of continuous updating on algorithmic 
discrimination (and economic efficiency) largely 
hinges on the sociotechnological environment, which 
includes the degree of the system ’s initial bias and, 
perhaps more importantly, the persistence of biased 
decision-making by humans in the loop. We found that 
the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that",6.659817696,0.3,"The generated answer (models replicate biased human decisions) and the ground truth (models are optimized to be 'overly agreeable') describe different, though not mutually exclusive, mechanisms for how bias can manifest in LLMs. They are not semantically equivalent.",1,The generation is faithful. The claims that models replicate patterns of bias by being trained to 'replicate the outcomes achieved by human decision-makers' and that 'human bias es can impede' the process of continuous updating are both explicitly stated in the provided text.,0.8,"The context is highly relevant and provides a strong basis for the answer, but doesn't use the exact phrasing 'overly agreeable'. It states that AI systems are 'typically trained to replicate the outcomes achieved by human decision-makers'[cite: 1158], which often includes societal biases. It also notes that 'human bias es can impede' the model's ability to unlearn initial biases during continuous updating[cite: 1163]. This supports the idea that models replicate and are influenced by biased human patterns, which is a form of biased tendency."
What did Google do to address concerns about the representation of nurses in image search results?,Google filtered the results so that both men and women are shown when 'nurse' is entered.,"According to the text, Google implemented filters to ensure that both men and women were shown when searching for ""nurse."" This was done to address the initial issue where image search results overwhelmingly depicted nurses as female.","in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not reﬂect the opinions of those who create them. The persistent
underrepresentation of women, African-Americans, and Latino
professionals in the IT workforce leads to biased algorithms. For
instance, a study in 2019 found that only 2.5% of Google ’s
employees were black, while Microsoft and Facebook had only 4%
representation. Another study revealed that 80% of AI professors
in 2018 were male. Involving diverse individuals in data collection
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | https://doi.org/10.1057/s41599-023-02079-x REVIEW ARTICLE
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x 7 Das hat Google jetzt anscheinend 
durch Filter so gemacht,
dass wenn man Krankenpfleger 
eingibt,
dass sowohl Männer und Frauen 
gezeigt werden.
Aber bis vor zwei Jahren war
es immer noch ein sehr gutes 
Beispiel.
Wenn ich Sie frage, googeln Sie 
mal bitte nach Images.
Dann war Krankenpfleger zu 99 %
weiblich und Arzt zu 99 % 
männlich,
obwohl diese Bilder natürlich 
von beiden
eigentlich schon vorhanden sein 
sollen.
Das heißt, wir haben hier
irgendwelche gewachsenen 
technischen Grundlagen,
Datenbanken, die das einfach 
noch nicht abbilden können.
Having said that, die 
Unternehmen sind
sehr sensibilisiert durch diese 
ganzen PR Probleme,
die sie damit erlitten haben und 
haben angefangen,
diese Daten zu bereinigen.
Aber das erfordert eben massiv 
viel Investitionen
in Datenbereinigung,
wo wir wieder in dem 
Kriegsmodell,
bei dem Data Preparation sind.
Sie müssen sich klar sein Wie 
sind
Ihre Strukturen der Daten 
eigentlich aufgebaut?
Soweit ich weiß es auch manchmal
das Labeling von Menschen 
gemacht.
Richtig? Genau. Es könnte so 
sein,
dass wir haben eine Gruppe von 
Menschen,
die nicht politisch korrekt 
denken.
Und dann würden sie es bewusst 
falsch.
Richtig. Auch ein massives 
Problem.
Wir haben auf der einen Seite",6.518428564,1,The generated answer and the ground truth are semantically identical. Both state that Google applied filters to its image search to ensure both men and women are shown for the query 'nurse'.,1,"The generation is faithful. The context explicitly states that 'Das hat Google jetzt anscheinend durch Filter so gemacht, dass wenn man Krankenpfleger eingibt, dass sowohl MÃ¤nner und Frauen gezeigt werden' (Google has now apparently used filters so that when you enter 'nurse', both men and women are shown).",1,"The context directly answers the question. It states that Google 'hat Google jetzt anscheinend durch Filter so gemacht, dass wenn man Krankenpfleger eingibt, dass sowohl MÃ¤nner und Frauen gezeigt werden' (Google has now apparently done it with filters so that when you enter nurse, both men and women are shown)[cite: 1178]."
"According to the document, what was the reason Amazon's AI recruitment system discriminated against women?","It recognized correlations between hobbies, universities attended, and gender due to historical data showing fewer women attending certain universities and pursuing technical fields.","According to the document, Amazon’s AI recruitment system discriminated against women because it was trained on a dataset of CVs that were predominantly male. This led the algorithm to perceive this biased model as indicative of success, resulting in discrimination against female applicants.","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T",7.142222166,0.2,"The generated answer (trained on predominantly male CVs) and the ground truth (recognized correlations between hobbies/universities and gender) point to different sources of bias. While both relate to historical data, they describe different mechanisms, making them semantically inconsistent.",1,"The generation is faithful. The context explicitly states that Amazon's system discriminated against women because it 'recognized correlations between hobbies, universities attended, and gender due to historical data showing fewer women attending certain universities and pursuing technical fields'.",1,"The context directly answers the question. It states, 'The bias stemmed from training the AI system on predominantly male employees â€™ CVs' [cite: 1193], which led the algorithm to perceive this as a model for success and thus discriminate against female applicants[cite: 1194]."
"According to the lecture, what is the CRISP-DM model and what is its first and most important step?","The CRISP-DM (Cross-Industry Standard Process for Data Mining) is a blueprint for developing machine learning applications. Its first and most important step is Business Understanding, which involves clearly defining the business problem you are trying to solve.","According to the lecture, the CRISP-DM model is a standard process for structuring AI projects. Its first and most important step is **Business Understanding**.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 15. Mai 2025
Overview CRISP-DM
9",5.306087494,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context introduces the CRISP-DM model as a standard for structuring AI projects and a diagram clearly shows that the first step is 'Business Understanding'.,1,"The context directly answers the question. It provides an image of the CRISP-DM process model, which is described as a way of 'Structuring an AI project'[cite: 1213]. The first step in the diagram is clearly labeled 'Business Understanding'[cite: 1213]."
The lecture describes three levels of integrating machine learning solutions into an organization. What is a 'System Solution' (the third level)?,"A 'System Solution' is the deepest level of integration, where machine learning fundamentally changes the entire organizational structure and business model, requiring a complete rethinking of processes. The example given is Amazon's 'Anticipatory Shipping' patent.","According to the lecture, a “System Solution” is a “Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems.” It’s enterprise-level and highly integrated.","System solutions•Definition:Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems.•Scope:Broad; enterprise-level•Integration:Highly integrated with multiple components and data flows.•Use case:Innovation must transform or optimize an entire organization•Example:Amazon’s “Ship then Shop” (Prediction + Inventory + Logistics + …) Und diese Systeme haben ein
Zusammenspiel dann was bestimmt,
ob ich eigentlich das System 
nutze.
Das war eines der fundamentalen 
Ergebnisse,
die Sie hier gefunden haben in 
der Studie.
Und dann Sie haben gesagt okay, 
wenn ich weiß,
dass wir die Menschen in
ihrer Kognition überzeugen 
müssen,
dass sie das System nutzen 
sollten und akzeptieren
sollten entweder über System
eins oder über System zwei 
Aktivierungen,
dann lass uns doch mal drüber 
nachdenken,
welche Faktoren denn hier eine 
besondere Rolle spielen.
Und da haben Sie insgesamt in
diesem Modell vier verschiedene 
Faktoren identifiziert.
Einer der Faktoren nennen Sie 
die Performance Expectancy.
Das können Sie sich vorstellen.
Sie haben ein System vor sich 
und ob
Sie eine Bereitschaft 
grundsätzlich Formen
in ihrer Kognition des System zu 
benutzen,
hängt damit zusammen, ob sie 
glauben,
dass dieses System Ihnen hilft,
bestimmte Ziele zu erreichen.
Wenn Sie sich die verschiedenen 
GPTsysteme,
die aktuell verfügbar sind,
auch in den öffentlichen 
Versionen,
dann sind da ganz viele 
verschiedene.
Dann gibt es aktuell vier 
Punkte, wo ich glaube,
das ist generell ZugReifbar 
mittlerweile.
Dann haben wir drei,
wir haben 4.1 4:05 usw.
Wenn ich die Entscheidung 
treffe, hinzubekommen.
Und auf der einen Seite immer
diese technische Perspektive auf 
die Systeme,
die müssen ausreichend 
performant und gut sein.
Aber wir brauchen eben auch
diese Einbettung der Systeme der
technischen Systeme in das Sozio 
system.
Deswegen sprechen wir 
grundsätzlich dann
von diesen soziotechnischen 
Systemen,
wo diese Systeme wirklich 
genutzt
werden und letztlich 
Veränderungen hervorrufen.
Und viel davon passiert eben 
hier
Information Systems auf Deutsch 
aus
der Perspektive der 
Wirtschaftsinformatik.
Was immer so eine 
Schnittstellendisziplin ist,
wo wir aus ganz verschiedenen 
Blickwinkeln Psychologie,
Management, Ökonomie,
Verhaltensökonomie, Neurologie
auf diese Phänomene draufgucken,
wie tatsächlich Menschen, 
Organisationen,
soziale Systeme mit
diesen technischen Systemen 
interagieren.
Und in diesem Sinne, ich denke, 
diejenigen,
die Wirtschaftsinformatik ich 
weiß gar nicht,
wer es gerade gibt,
Peter oder Olli, einer meiner 
Kollegen der Abteilung.
Wenn Sie Wirtschaftsinformatik 
eins
oder zwei besucht haben,
werden Sie bestimmt schon mal 
über das Unified,
also Unified Theory of 
Acceptance
and Use of Technology gehört 
haben.
Wer hier hat das in der 
Vorlesung schon mal gesehen?",6.494735956,1,"The generated answer and the ground truth provide definitions that are semantically consistent, both describing a highly integrated, organization-wide solution.",1,"The generation is faithful. The context defines a 'System solution' as a 'Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems' at an 'enterprise-level', which aligns with the generated answer.",0.9,"The context is highly relevant and provides a strong foundation for the answer. It defines 'System solutions' as 'Comprehensive, integrated set of applications and technologies that work together to address complex, organization-wide problems' [cite: 1222] with an enterprise-level scope[cite: 1223]. It gives the example of Amazon's 'Ship then Shop' which involves '(Prediction + Inventory + Logistics + â€¦)'[cite: 1223], clearly indicating a fundamental change to the business model."
"According to the text, what is one thing machine learning models currently cannot do?",Machine learning models currently cannot provide a valuation of the consequences of a particular state of the world.,"According to the text, one thing machine learning models currently cannot do is “with unstructured data like words simply deal.” They require a significant amount of “data preprocessing.”","Sie müssen das sauber machen.
Maschinen, wie wir sehen werden,
können nicht mit 
unstrukturierten
Daten wie Wörtern einfach so 
umgehen.
Wir haben ganz viel 
Datenvorarbeit zu betreiben
und es gibt bestimmte Machine 
Learning Modelle,
die sind sehr schön,
die viel dieser Arbeit durch
bestimmte algorithmische 
Lernverfahren abfangen.
Zum Beispiel der 
Entscheidungsbaum
oder auch der Random Forest.
Aber es gibt andere Methoden und 
insbesondere die
most sophisticated Methoden wie 
neuronale Netze,
die auf jede Kleinigkeit in
den Daten massiv sensitiv 
reagieren,
müssen Daten standardisieren.
Wir müssen auf eine bestimmte 
Art und
Weise sampeln müssen
bestimmte Observationen 
rausschmeißen.
All das, wie Sie hier sehen,
braucht ungefähr 60 bis
80 % der Zeit eines solchen Data 
Science Projekts.
Es ist teilweise über Wochen, 
Monate.
Wenn wir Daten von Unternehmen 
bekommen,
mit denen wir zusammenarbeiten,
um Forschungsprojekte zu machen,
dass wir erst mal eine Idee 
bekommen,
was diese Daten alles bedeuten.
Sie haben bei mir in der im 
Seminar
gesessen letztes Semester diese 
Daten,
die Sie da erhalten haben, von 
der Firma.
Totales Chaos.
Muss man sich erstmal reindenken 
und
das braucht massiv viel Zeit. sind dann in eine Case Study 
gemacht,
wo ich ihnen gezeigt habe,
wie man Machine Learning 
anwenden kann.
Nicht immer nur zu negativen 
Aspekten,
aber diese Predictions,
die Machine Learning uns liefern 
kann,
wie man die auch dazu nutzen 
kann,
zum Beispiel zur Vorhersage,
welcher Kunde wie auf welches 
Messaging reagiert,
um letztlich die
Rücksenderaten von Produkten zu 
verringern.
Letzte Woche sind wir dann mehr 
ins
Detail schon reingesprungen, 
Eingesprungen,
haben uns überlegt, aus einer 
ökonomischen Perspektive,
was Prediction bedeuten könnte,
wenn wir bei General Purpose 
Technologien gesprochen.
Wir haben darüber gesprochen,
dass wir uns, wie
es bei so vielen General Purpose 
Technologien ist,
aktuell auf einen bestimmten 
Teil der Kurve,
die noch befinden,
die Kurve als die Darstellung, 
die fundamentale Idee,
dass wir nicht in große 
Technologie Investitionen habe,
dass sich die nicht sofort 
auszahlt.
Das ist immer so ein bisschen 
Verzögerung gibt,
am Anfang sogar die marginale
Produktivitätsveränderungen 
negativ sein kann.
Das ist so dieser untere 
Halbkreis von dem J.
Und das kann verschiedene Gründe 
haben.
Unter anderem eben, dass diese
Technologien noch nicht 
ausreichend in den
Organisationen mit Stelle sein.
Sie müssen also identifizieren 
welche
Daten welche Variablen sind 
eigentlich da?
Müssen Sie vielleicht neue 
Informationen erheben?
Sind bestimmte Informationen gar 
nicht zugänglich?
Und das Letzte kann tatsächlich 
und findet häufig statt,
dass man sagt Ich habe eine 
Riesenidee,
wie ich eigentlich Machine 
Learning nutzen kann.
Dazu brauche ich aber in einer 
First World ABCDE.
Und dann kommt der Betriebsrat 
und sagt ihnen Nee,
das können wir nicht machen, das 
funktioniert so nicht.
Und dann stehen sie dort und 
sagen Ja gut,
dann macht meine Modellidee,
um dieses Businessproblem zu 
lösen.
So wie ich es mir vorgestellt 
habe,
vielleicht gar keinen Sinn.
Deswegen befindet sich dann 
dieser Pfeil hier zurück.
Und Sie müssen natürlich sagen,
in der First Best World würde 
ich es gerne so haben.
Das sollte immer Ihr 
Ausgangspunkt sein.
Und dann werden Sie mit
der Realität konfrontiert und 
merken,
dass einige Sachen technisch 
einfach nicht da sind,
vielleicht noch nicht da sind.
Vielleicht bedarf es eben 
Änderungen
in der Art und Weise,
welche Informationen erheben 
werden müssen.
Es kann sein, dass Sie zu der 
Entscheidung kommen,
hier in diesem Data 
Understanding, dass Sie sagen 
Hey,",6.187930346,0,The generated answer (models cannot handle unstructured data) and the ground truth (models cannot provide a valuation of consequences) describe two completely different limitations of machine learning. They are semantically inconsistent.,1,The generation is faithful. The context explicitly states that 'Maschinen... kÃ¶nnen nicht mit unstrukturierten Daten wie WÃ¶rtern einfach so umgehen' (Machines... cannot simply deal with unstructured data like words) and require 'Datenvorarbeit' (data preprocessing).,0,The context is irrelevant. It states that machines 'kÃ¶nnen nicht mit unstrukturierten Daten wie WÃ¶rtern einfach so umgehen' (cannot simply deal with unstructured data like words)[cite: 1240]. It does not mention anything about the valuation of consequences.
What are two types of bias metrics?,Individual-level metrics and group-level metrics.,"According to the text, two types of bias metrics are:

*   **Individual-level metrics:** These ensure that people with similar qualifications receive similar outcomes.
*   **Group-level metrics:** These ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups.","15. Mai 2025
Measurement of algorithmic fairness
•There is no single agreed-upon measure for discrimination/fairness•There are different types of bias metrics that we can distinguish•Individual-level metrics:ensure that people who have similar qualifications with respect to a task receive similar outcomes•Group-level metrics:ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups
15
Verma & Rubin, 2018 data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",6.333279371,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context clearly distinguishes between 'Individual-level metrics' (ensuring similar outcomes for similar qualifications) and 'Group-level metrics' (ensuring outcomes do not disproportionately affect groups).,1,"The context directly answers the question. It introduces two types of metrics: 'Individual-level metrics' which 'ensure that people who have similar qualifications... receive similar outcomes' [cite: 1265], and 'Group-level metrics' which 'ensure that algorithmic outcomes do not disproportionately and negatively affect particular groups'[cite: 1265]."
"According to the CRISP-DM framework, what is the approximate percentage of time spent on data preparation within the CRISP-DM process?",Approximately 60-80% of time.,"The provided text doesn't give an exact percentage of time spent on data preparation. It simply states that the ""Data Preparation phase covers all activities to construct the final dataset"" and that tasks are ""likely to be performed multiple times, and not in any prescribed order.""","form ulation  of the data m ining problem  and the project plan require at least som e
understanding of the available data.
• D ata Preparation
The data preparation phase covers all activities to construct the final dataset (data that w ill be
fed into the m odeling tool(s)) from  the initial raw  data. D ata preparation tasks are likely to be
perform ed m ultiple tim es, and not in any prescribed order. Tasks include table, record, and 15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining 15. Mai 2025
Overview CRISP-DM
9",6.870602846,0,"The generated answer is a refusal, stating the information is not in the text. The ground truth provides a specific, factual answer to the question. The two are semantically opposite.",1,"The generation is faithful. It correctly states that the text does not provide a percentage for time spent on data preparation, and accurately quotes the information that is available.",0,"The context is irrelevant. It describes the phases of the CRISP-DM data mining process model, such as 'Data Preparation'[cite: 1281]. It makes no mention of the time spent on any specific phase."
What does the document state about the impact of Generative AI on employee happiness?,"The document states that employees became happier, with less escalation and longer tenure.","You are absolutely correct. The document focuses on algorithmic discrimination in recruitment and selection, not the impact of AI, including Generative AI, on employee happiness.","employment opportunities and the potential for discrimination
without accountability. To harness the beneﬁts of AI in recruiting,
organizations should exercise careful selection of their programs,
promote the adoption of accountable algorithms, and advocate
for improvements in racial and gender diversity within high-tech
companies.
The general construct of this study is,ﬁrst, an extension of
statistical discrimination theory in the context of the algorithmic
economy; second, a synthesis of the current literature on the
beneﬁts of algorithmic hiring, the roots and classi ﬁcation of
algorithmic discrimination; and third, initiatives to eliminate the
existence of algorithmic hiring discrimination; fourth, based on
the Grounded Theory, we conduct surveys with respondents and
analyze primary data to support the study.
The contributions of this study are as follows:
First, discuss job market discrimination theories in the digital
age context. When considering statistical discrimination theories,
we should consider the current circumstances. It is necessary to
apply these discrimination theories to evaluate the issues that
arise from the use of technology in the digital age, particularly technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender a study on the impact of artiﬁcial intelligence on eliminating hiring bias from
recruitment and selection process. J Comput Theor Nanosci 17(9):4404–4407
Ruwanpura KN (2008) Multiple identities, multiple-discrimination: a critical
review. Fem Econ 14(3):77–105
Samuelson PA (1952) Spatial price equilibrium and linear programming. Am Econ
Rev 42(3):283–303
Shaw J (2019) Artiﬁcial intelligence and ethics. Perspect: Policy Pract High Educ
30, 1–11
Shin D, Park YJJCIHB (2019) Role of fairness, accountability, and transparency in
algorithmic affordance. Perspect: Policy Pract High Educ 98, 277–284
Smith B, Shum H (2018). The future computed. Microsoft
Tilcsik A (2021) Statistical discrimination and the rationalization of stereotypes.
Am Sociol Rev 86(1):93–122
Timmermans S, Tavory I (2012) Theory construction in qualitative research: from
grounded theory to abductive analysis. Sociol Theory 30(3):167–186. https://
doi.org/10.1177/0735275112457914
Upadhyay AK, Khandelwal K (2018) Applying artiﬁcial intelligence: implications
for recruitment. Strateg HR Rev 17(5):255–258
van Esch P, Black JS, Ferolie J (2019) Marketing AI recruitment: the next phase in
job application and selection. Comput Hum Behav 90:215–222. https://doi.",5.816755056,0,"The generated answer is a refusal, which is incorrect as the information is available. The ground truth provides the correct answer.",1,The generation is faithful. It correctly states that the document does not contain information about the impact of Generative AI on employee happiness and accurately describes the actual focus of the text on algorithmic discrimination.,0,"The context is entirely irrelevant. It discusses discrimination in algorithmic hiring and provides references to various studies [cite: 1287-1291], but it contains no information about the impact of Generative AI on employee happiness."
What did Amazon patent in 2014?,A 'Chip Shop' (patent).,You are absolutely correct. The document does not contain information about what Amazon patented in 2014.,"profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for 
components of this work owned by others than the Association for Information Systems must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior 
specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, 
GA, 30301-2712 Attn: Reprints, or via email from publications@aisnet.org. indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly from
the copyright holder. To view a copy of this license, visithttp://creativecommons.org/
licenses/by/4.0/.
© The Author(s) 2023
REVIEW ARTICLE HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | https://doi.org/10.1057/s41599-023-02079-x
12 HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS|          (2023) 10:567 | https://doi.org/10.1057/s41599-023-02079-x License. You are free to download this work and share with others, but cannot change in any way or use
commercially without permission, and you must attribute this work as “Marketing Science. Copyright © 2024 The
Author(s). https://doi.org/10.1287/mksc.2022.0393, used under a Creative Commons Attribution License: https://
creativecommons.org/licenses/by-nc-nd/4.0/.”
Copyright © 2024 The Author(s)
Please scroll down for article—it is on subsequent pages
With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations
research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning
opportunities for individual professionals, and organizations of all types and sizes, to better understand and use
O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",4.552864075,0,"The generated answer is a refusal, which is incorrect as the information is available. The ground truth provides the correct answer.",1,The generation is faithful. It correctly states that the provided document contains no information about an Amazon patent from 2014.,0,The context is irrelevant. It contains licensing and publisher information for various articles but includes no mention of Amazon or any patents from 2014.
The lecture identifies two fundamental aspects that make AI different from previous General Purpose Technologies like electricity. What are they?,"First, it is a technology that can improve itself through its own use (self-learning). Second, this self-learning creates massive cross-industry spillover effects, where use in one industry can improve the system for another.","According to the text, the two fundamental aspects that make AI different from previous General Purpose Technologies like electricity are:

1.  **The technology itself can continuously learn and improve** through use – it’s the first time a technology has this capability with cross-industry spillovers.
2.  **The creation of new data** through use that is then fed back into the technology to further improve it.","8. Mai 2025
AI as General-Purpose technologyWhat is a General-Purpose Technology (GePuT)?•Pervasive: can be applied across sectorsof the economy. •Rapidly improving & cheaper over time: learning-by-doing, scale economies, Moore-like curves•Complements: sparks waves of follow-on innovations, new business models, organizational redesign.•Previous examples: Steam engine, Electricity, Digital ComputingAI be seen as a GePuTas well –why?•Core function gets dramatically cheaper: prediction making•Ubiquitous use-cases across every industry.•For the first time: continuous learning of the technology itself with cross industry spillovers•Catalyst for complements:•New inputs: sensors, connected devices, synthetic data•New “orgware”: data pipelines, ML-ops, AI-first processes•…
7
Managerial perspective: invest in complementary technologies, andadjust organization to unlock its full value. some plants. After the mid-1990s, artiﬁcial intelligence software
saw signiﬁcant advances. Until today’s digital economy, AI has
been commonly used in various industries (Hmoud and Laszlo,
2019).
Artiﬁcial intelligence is deﬁned as the ability of something like
a machine to understand, learn, and interpret on its own in a
human-like manner (Johansson and Herranen,2019). Artiﬁcial
intelligence aims “to understand and simulate human thought
processes and to design machines that mimic this behavior.”It is
designed to be a thinking machine with a level of human
Table 1 Research theme, papers, and main content.
Theme Research papers Main content
1. AI-enabled recruitment
application and its beneﬁts.
Bogen and Rieke (2018), Ahmed (2018), Hmoud and Laszlo
(2019), Albert (2019), van Esch et al. (2019), Köchling et al.
(2022), and Chen (2023)
Summarize the recruitment process into four
main steps: sourcing, screening, interviewing, and
selection
Beattie et al. (2012), Newell (2015), Raub (2018), Miasato and
Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal- drauf gucken,
ob es das Richtige ist,
die Kohlekraftwerke wieder 
hochzufahren.
Grundsätzlich kann das aber als
Kerninnovation KI KI braucht.
Das wollen wir alle machen,
braucht aber weitere 
Innovationen,
dass wir es wirklich gut nutzen 
können.
Was glauben Sie denn,
wenn Sie KI mit den vorherigen 
Innovationen vergleichen?
Was macht KI besonders?
Was ist ganz fundamental
unterschiedlich
zu allen vorherigen General 
Purpose Technologies.
Und ich habe es gerade schon 
gezeigt Ja,
man braucht keine neue Hardware 
als Nutzer.
Am Ende doch die.
Also man braucht immer wieder 
andere Hardware.
Irgendwann reiben sich auch 
GPU's zum Beispiel auf,
geht aber in die richtige 
Richtung,
in das Technische zu denken.
Yes, das ist ein Teil davon.
Wir haben zum Ersten Mal Systeme 
oder eine Technologie,
die sich selbst verbessern kann.
Wenn Sie sich überlegen Sie 
haben den Hammer,
was auch eine General Purpose 
Technologie ist,
nur weil Sie dreimal den Hammer 
benutzen,
wird der Hammer per se nicht 
besser.
If anything wird der Hammer.
Irgendwie über die Zeit 
schlechter.
Wir haben aber jetzt eine 
Technologie.
Dadurch, dass wir sie nutzen,
wir neue Daten kreieren und
diese neuen Daten wieder in
die Maschine reingefüttert 
werden können,",9.625683308,1,The generated answer and the ground truth are semantically identical. Both state that AI can learn and improve itself through use and that this creates cross-industry spillover effects.,1,The generation is faithful. The claims that AI is a technology that can improve itself through use ('continuous learning of the technology itself') and that this learning creates 'cross industry spillovers' are both stated explicitly in the context.,1,"The context directly answers the question. It states that unlike previous technologies, with AI 'For the first time: continuous learning of the technology itself with cross industry spillovers' occurs[cite: 1323]. It also notes that the use of the technology creates new data that can be fed back into the system to improve it[cite: 1343]."
"From a purely economic perspective, what is the single most important function that machine learning technology provides?","It is a technology that massively lowers the cost of making high-quality, accurate predictions.","From a purely economic perspective, machine learning technology provides the single most important function of **improved prediction and decision-making based on data.**

The text highlights this by stating that ML allows for “the prediction of Markttrends” and “the optimization of Marketingkampagnen.” This core ability – to analyze vast datasets and forecast outcomes – directly translates to increased efficiency, reduced risk, and ultimately, better economic outcomes for businesses.","Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. Studierende werden mit den wichtigsten Fragestellungen rund um den Einsatz von Machine Learning (ML) in Managemententscheidungen vertraut gemacht. Sie erwerben grundlegende theoretische Kompetenzen im Bereich ML und verstehen, wie diese Technologien betriebliche Prozesse und strategische Entscheidungen beeinflussen können.
Studierende erlernen den Umgang mit grundlegenden Methoden und Werkzeugen des maschinellen Lernens, einschließlich Datenaufbereitung, Modelltraining und -bewertung. Sie entwickeln die Fähigkeit, wirtschaftliche Fragestellungen in ML-Probleme zu übersetzen und datenbasierte Lösungen analytisch sowie strukturiert zu erarbeiten.
Im Rahmen der Veranstaltung werden zahlreiche Praxisbeispiele und Fallstudien aus verschiedenen Industrien behandelt, die die Anwendung von ML und generativer KI im geschäftlichen Kontext beleuchten. Insbesondere in den angebotenen Übungen und praktischen Programmieraufgaben lernen Studierende, das erlangte Wissen anzuwenden, auch ohne vorherige Programmiererfahrung. 24. April 2025
However, beyond the hype what does Machine Learning actually deliver?",8.757335424,1,"The generated answer is semantically consistent with the ground truth. The ground truth is more concise ('lowers the cost of... predictions'), while the generation is more descriptive ('improved prediction and decision-making'), but they capture the same core economic function.",1,"The generation is faithful. The context states that ML offers various approaches to make management decisions 'fundierter und effizienter' (more well-founded and efficient) through tasks like 'Vorhersage von Markttrends' (prediction of market trends), directly supporting the claim that its primary function is improved prediction and decision-making.",0.9,"The context is highly relevant. It states that Machine Learning (ML) is an indispensable tool for making data-based decisions, increasing efficiency, and driving innovation[cite: 1345]. It provides examples like 'Optimierung von Marketingkampagnen' (optimization of marketing campaigns) and 'Vorhersage von Markttrends' (prediction of market trends)[cite: 1346]. This strongly implies that its key economic function is to lower the cost of prediction, even if it doesn't use that exact phrasing."
"According to the text, what does the 'J-curve' represent?","The 'J-curve' represents a phenomenon where, with many technologies, productivity initially decreases before eventually increasing.","The provided text does not contain any information about a “J-curve”. It discusses data mining, machine learning models, and data representation, but it doesn't mention a J-curve.","The sequence of the phases is not strict. The arrow s indicate only the m ost im portant and frequent
dependencies betw een phases, but in a particular project, it depends on the outcom e of each phase
w hich phase, or w hich particular task of a phase, has to be perform ed next.
The outer circle in Figure 2 sym bolizes the cyclic nature of data m ining itself. D ata m ining is not
finished once a solution is deployed. The lessons learned during the process and from  the
deployed solution can trigger new , often m ore focused business questions. Subsequent data
m ining processes w ill benefit from  the experiences of previous ones (cf. the virtuous cycle of
(B erry and Linoff, 1997)). dieser Gleichung hier 
ausdrücken.
Wir haben drei verschiedene 
Komponenten.
Wir haben hier dieses Hütchen,
wir haben das fix und wir haben 
hier dieses Y Hütchen.
Meine erste Frage an Sie Was war 
das? F Hütchen.
Yes, genau richtig.
Wir haben hier das erlernte 
Machine Learning Modell,
was die Muster, die wir in 
historischen
Daten durch Algorithmen erkannt 
haben, abspeichert.
Hier in diesem F. Steckt unser 
gesamtes Wissen drin,
was wir irgendwie extrahieren
und wie genau wir das 
extrahieren.
Das wird der zweite Teil unserer 
Vorlesung sein.
Wichtig hierbei Hütchen ist 
immer dafür ein Indikator,
dass wir von einer Approximation 
sprechen.
Das ist ein Modell.
Das ist nicht der wahre 
unterliegende Prozess,
der die Daten generiert.
Das ist nicht die Landschaft von 
Frankfurt sozusagen,
sondern es ist die Landkarte.
Es ist ein explizit abstrakteres 
Modell,
eine Darstellung der Realität.
Und da können eben Fehler drin 
sein.
Es ist ganz wichtig, sich
immer wieder vor Augen zu 
halten.
Dann haben wir die zweite 
Komponente.
Das war das XML. Was ist das?.X. 
Jemand eine Idee?
Jetzt dahinten? Genau das sind 
die Daten,
die wir in diese Maschine 
reintun.
Wir haben zum Beispiel, wenn Sie 
überlegen,
Sie interagieren mit Chat GPT 10. Juli 2025
Gradient descent: the intuition
18
Imagine a data set with three observations:
Can you spot the relationship between 𝑥and 𝑦?We train a perceptron with one weight 𝑤and no bias
Assume 𝑓𝑤⋅𝑥=𝑤⋅𝑥(linear activation function). What is the optimal 𝑤?
𝑥 𝑦3 -0.21-5 0.359 -0.63
𝑓(𝑤⋅𝑥)𝑥 A𝑦",5.965408325,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct definition of the J-curve.",1,The generation is faithful. It correctly reports that the provided context does not contain any information about a 'J-curve'.,0,The context is irrelevant. It discusses the components of a machine learning model using the analogy of a map versus the landscape [cite: 1368] and the cyclical nature of the CRISP-DM data mining process[cite: 1358]. It does not contain any information about a 'J-curve'.
What is a potential new attack vector related to large language models?,"Systems can be hacked, leading to false or harmful code being written into company codebases.","The provided text doesn't offer information about potential attack vectors related to large language models. It discusses:

*   The interplay between continuous model updating and algorithmic discrimination.
*   The origins and impacts of algorithmic feedback loops.
*   Details about the Word2Vec model and transformer architecture.

It does not contain any details about attack vectors.","affecting ML systems’ functionality by explaining the 
interplay b etween continuous model updating and 
algorithmic discrimination. Despite its critical importance 
in developing countermeasures and refining policy 
recommendations, the complex interplay between 
algorithmic discrimination and continuous ML model 
updating remains an underexplored area in the literature.  
Second, our paper complements the nascent body of 
studies exploring the origins and impacts of 
algorithmic feedback loops (Burghardt & Lerman, 
2022; Cowgill, 2018). For instance, Lum and Isaac 
(2016) revealed that the iterative updates of a 
predictive poli cing system led to increased 
discrimination against minority groups. Similarly, 
Ensign et al. (2017) showed how feedback loops could 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18 17. Juli 2025
A game changer
27
The transformer architecture1.Transform words into numbers –Word embeddings2.Control word positioning of words –Positional encoding3.Represent words in the context of other words –Self-attention4.Predict the next word using a feed-forward neural net",7.346422672,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct answer.",1,The generation is faithful. It correctly states that the text does not provide information on this topic and accurately lists the topics that are covered.,0,"The context is irrelevant. It discusses algorithmic feedback loops [cite: 1379, 1381] and the transformer architecture [cite: 1382] but contains no information about security vulnerabilities or attack vectors related to large language models."
"According to the text, what is a problem with relying on a perfect machine learning model to predict future outcomes?","It would shift us into a situation where there appears to be no risk, but this is conceptually problematic.","According to the text, a problem with relying on a machine learning model is that “it does not necessarily have to be right.” The model’s output – a prediction – “is not necessarily right.” It’s aiming for accuracy, but it’s not guaranteed to be correct.","1933 ), uses machine learning functions to flexibly esti -
mate residuals in treatment and outcome prediction 
equations, enhancing the robustness of our treatment 
effect estimates and aligning with accepted practices 
(detailed further in Online Appendix A.6). We further 
conform to data science standards by dividing our data 
into training (80%) and testing (20%) sets (Hastie et al. 
2009 ). We fine-tune the causal forest’s hyperparameters 
through a grid search with fivefold crossvalidation on 
the training data, focusing on out-of-sample R-score per -
formance (Schuler et al. 2018 ). We provide hyperpara -
meter tuning details in Online Appendix A.7.
4.1.3. Evaluation of the CML Model. Our evaluation of 
the CML model is based solely on test data and follows 
two steps. First, we assess the CML model’s predictions 
through group average treatment effects following 
Chernozhukov et al. ( 2018 ), where we separately esti -
mate the treatment effects for individuals who the 
CML model would treat and those who it would not 
treat. Second, we evaluate the CML-based treatment 
policy through off-policy evaluation following Hitsch 
et al. ( 2024 ). Off-policy evaluation estimates the effect 12. Juni 2025
Remember the following case study from lecture 1.(a) Explain in your own words what the central machine learning prediction helps to do. (b)Provide a brief discussion about the potential dangers associated using ML models for such purposes in the context of misinformation.
5 Das ist unsere Prädiktion,
auf Englisch auch wieder 
indiziert durch das Hütchen.
Das Hütchen zeigt uns an dieser 
Stelle,
dass wir nicht ganz explizit 
wissen,
ob das eigentlich das reale 
Outcome ist.
Wenn Sie eine Machine Learning 
System haben,
was Ihnen basierend auf den
Soziodemografischen Faktoren 
einer Person
eine Vorhersage gibt darüber,
ob diese Person tatsächlich 
kreditwürdig ist oder nicht.
Ist dieser Output der
Maschine das Y immer eine 
Prediction,
eine Vorhersage, die nicht
notwendigerweise richtig sein 
muss.
Wir hoffen, dass sie so akkurat 
wie möglich ist.
Das ist aber nicht zwangsläufig 
gegeben.
Und wer erinnert sich an das 
Ziel von
Machine Learning aus einer 
konzeptionellen Perspektive?
Wenn wir über die overarching 
Objectives reden,
was wollen wir damit erreichen?
Wofür ist dieses ganze Modell 
gebastelt etc.
da? Die bestmöglichen 
Vorhersagen?
Genau, Und für welche Daten?
Ja, und eine Sache will ich noch 
rauskitzeln.
Was sind die Inputdaten am 
besten?
Daten, die das Modell noch nicht 
kennt? Genau das.
Es geht uns nicht darum,
dass wir ein Modell haben,
was uns optimal erklärt,
was in der Vergangenheit alles 
passiert ist.
Aber wir wollen keinen 
Vorhersagemodell
über den Stockmarket Price",7.835913658,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct answer.",1,The generation is faithful. The context explicitly states that a machine learning prediction 'ist nicht notwendigerweise richtig' (is not necessarily right).,1,"The context directly answers the question. It states that a problem with relying on a perfect model is that its output 'ist dieser Output der Maschine das Y immer eine Prediction, eine Vorhersage, die nicht notwendigerweise richtig sein muss' (this output of the machine, the Y, is always a prediction, a forecast that does not necessarily have to be right)[cite: 1397]."
How much have the costs for inference (using GPT or other Large Language Models) decreased in the last two years?,The costs have decreased by a factor of 34.,"According to the text, the costs for inference (using GPT or other Large Language Models) have decreased by a factor of 34 in the last two years.","Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen.
Wenn wir über GPT Systeme 
sprechen,
weil die da sehen wir natürlich 
merklich unterschiedlich,
ob ich 60 Gigabyte Text
reinknalle oder das halbe 
Internet.
Dann habe ich natürlich schon 
unterschiedliche
Kommunikationskosten.
Wobei jetzt in dem neuen Report
war zum Beispiel angezeigt,
dass die Kosten zumindest für 
die Inferenz,
also wenn Sie GPT benutzen oder
andere Large Language Models,
die Kosten sind seit zwei zwei 
Jahren Haben
die sich bei einem Faktor 34 
verringert?
Ja, das wird massiv effizienter.
Wir sehen jetzt so ein bisschen 
die
Laws of Scaling einsetzen.
Ganz kurz noch hat das Ihre 
Frage.
Das ist die beantwortet mit 
Ockhams Razor.
Wenn wir jetzt nur nach 
Temperatur entscheiden,
wäre nach diesem Gesetz kein 
Gesetz,
das vorsichtig sein. Das ist so 
eine.
Ich würde es als Daumenregel 
bezeichnen.
Deswegen hat man auch davon 
abgesehen,
dass irgendwie Rule zu nennen.
Aber grundsätzlich so ein 
bisschen aus der
Physik motiviert zu sagen naja,
ein einfacheres Modell, was mir 22. Mai 2025
A frequent trade-off
Model complexity, prediction accuracy, and model interpretability•Linear models simple to interpret as input-output relationships can be quantified in magnitude and direction, see, e.g., OLS coefficients•More complex models typically more accurate in predictions•Often a tradeoff; but not always (Sometimes “Occam’s Razor” seems to apply)
12
Model complexity / ability to capture pattern
Interpretability
BayesLinear regressionDecision tree
Deep NNEnsemble methods (e.g., RF)SVM
NOTE: Graph just an illustration without precise scales; exact relationship highly dependent on actual problem 17. Juli 2025
Word2VecIn practice:•Word2Vec has much larger embeddings layer (>100 dimensions)•Trained on large data set, e.g.entire, Wikipedia•Alternative prediction goal is skipgram•Predict surrounding words from word itself (the other way round for CBOW)Training embeddings requires high computational power•Typically, more than 100 dimensionalembedding•Size of dictionary is in the million•w2v speeds up training by negative sampling
18",6.084137678,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly states that inference costs 'haben die sich bei einem Faktor 34 verringert' (have been reduced by a factor of 34).,1,"The context directly answers the question. It states that 'die Kosten zumindest fÃ¼r die Inferenz... haben die sich bei einem Faktor 34 verringert' (the costs, at least for inference... have been reduced by a factor of 34)[cite: 1407]."
"What is the idea behind using simpler models, according to Ockhams Razor?",To prefer a model that needs less information to make a good prediction.,"According to Ockham’s Razor, the idea behind using simpler models is to favor models that require less information to make a good prediction. Essentially, it’s about minimizing complexity and assuming the simplest explanation is usually the best. The text states that “dass man versuchen sollte, ein Modell so einfach wie möglich zu belassen.”","Ockhams Razor aber dafür,
dass man versuchen sollte,
ein Modell so einfach wie 
möglich zu belassen.
Bedeutet in unserem Beispiel ich 
sollte ein Modell
laut diesem Ockhams Razor 
bevorzugen,
was weniger Informationen 
benötigt,
um eine gute Vorhersage zu 
machen.
Wäre das nicht auch teilweise 
ökonomischer?
In dem Modell, eine Menge 
Energie zu verbrauchen,
würden vielleicht weniger Daten
das gleiche Ergebnis bringen.
Das ist auch ein wichtiger Punkt 
grundsätzlich,
den wir aber in den Datenmengen,
über die wir alle sprechen,
wenn wir über tabulare Daten
insbesondere sprechen, 
ignorieren können,
weil die Kosten dafür so gering 
sind,
dass das keine Rolle spielt,
ob ich einen oder zwei Features 
mehr nehme.
Es gibt Unternehmen, die viel 
mehr Daten.
Das stimmt. Das ist 
grundsätzlich richtig.
Es kommt auf den 
Trainingsprozess an,
es kommt auf die Art des Modells
an für unsere Purpose is.
Aber das meine ich damit Für 
unsere Datensätze,
die wir nutzen, ist das eine 
Sache,
die wir erstmal ausblenden.
Wir gehen davon aus,
dass die marginalen Kosten der
Produktion und die marginalen 
Kosten,
eine weitere Einheit von 
Komplexität dazuzunehmen,
de facto null sind.
Wir sprechen da dann drüber, so 
ein bisschen. die das beeinflussen und
was hier Venkatesh und Kollegen 
gemacht haben,
die haben acht verschiedene 
Modelle
aus ganz unterschiedlichen
Disziplinen zusammengenommen und
haben jahrelang rumgewerkelt, um 
zu schauen,
was sind eigentlich die latent 
ähnlichen Faktoren,
was können wir irgendwie 
zusammenfassen und
was sind Faktoren,
die möglichst orthogonal einzeln 
gesehen
unterschiedlich sind und
viel Erklärungsgehalt dafür 
haben
in der Vorhersage,
ob ein Mensch die Technologie 
nutzen wird oder nicht.
Und das haben Sie das Modell, 
was entwickelt haben
letztlich auf ganz viele 
empirische Daten
gelegt und gezeigt,
dass dieses Modell um die 70 % 
der Variation darin,
ob Leute mit Technologie 
interagieren,
diese wirklich nutzen und 
akzeptieren, erklären kann.
Als Spoiler schon mal 70 % der
Variation zu erklären ist 
massiv,
insbesondere in solchen 
soziotechnischen Systemen,
wo wie immer der Mensch die 
Schwachstelle ist.
Womit haben Sie angefangen? Sie 
haben gesagt.
Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell Und dieses Modell ist
vor allen Dingen aus der 
Perspektive
geschaffen worden damals.
Wie kriegen wir eigentlich Leute 
in Unternehmen dazu,
die bereitgestellte Technologie 
zu nutzen?
Es gibt mittlerweile auch zwei
Erweiterungen von diesem Modell
oder auch ein paar mehr kleinere 
Erweiterungen noch,
die es dann mehr auf
die Konsumentenperspektive 
erweitert haben.
Aber das Grundmodell und das 
ist,
was ich Ihnen beibringen möchte.
Und ich dachte eigentlich,
ich könnte es relativ schnell 
machen,
aber weil es so wichtig ist,
sprechen wir doch ein paar 
Minuten drüber.
Dieses Grundmodell, das ist 
wichtig zu verstehen.
Auf der einen Seite wurde 
argumentiert,
ob meine Mitarbeiter in
meinem Unternehmen eine 
Technologie,
die ich bereitstelle,
benutzen und das gilt auch 
grundsätzlich für Kunden
mit ein paar Abwandlungen ist,
ob es sogenannte Facilitating 
conditions gibt.
Habe ich einen Hilfscenter,
habe ich in meiner Organisation 
ein einen Lehrgang,
nenne ich es jetzt einfach mal
for lack of a better world,
wo mir beigebracht wird,
wie ich mit dem System 
interagiere.
Habe ich einen IQ, habe ich 
Gruppen Arbeitsgruppen,
die sich mit dem System 
beschäftigen,
habe ich die Möglichkeit,
bestimmte Fehler von diesem 
System",9.270684242,1,"The generated answer and the ground truth are semantically identical. Both identify pervasiveness, improvement over time, and spawning complementary innovations as the three defining characteristics.",1,"The generation is faithful. The context states that according to Ockham's Razor, one should prefer a model 'was weniger Informationen benÃ¶tigt, um eine gute Vorhersage zu machen' (that needs less information to make a good prediction).",1,"The context directly answers the question by explaining Ockham's Razor. It states one 'sollte ein Modell laut diesem Ockhams Razor bevorzugen, was weniger Informationen benÃ¶tigt, um eine gute Vorhersage zu machen' (should prefer a model according to Ockham's Razor that requires less information to make a good prediction)[cite: 1417]."
"In the paper 'The Productivity J-Curve' by Brynjolfsson et al., what does the Fortune 500 enterprise software company specialize in?",The company specializes in providing business process software for small and medium-sized U.S. businesses.,The provided text doesn't contain information about the Fortune 500 enterprise software company or what it specializes in. It focuses on a lecture series about Machine Learning for Management Decisions.,"Die Digitalisierung und datengetriebene Transformation verändern die Unternehmenswelt grundlegend. Machine Learning (ML) ist dabei zu einem unverzichtbaren Werkzeug geworden, um datenbasierte Entscheidungen zu treffen, Effizienz zu steigern und Innovationen voranzutreiben. Von der Optimierung von Marketingkampagnen über die Analyse von Kundendaten bis hin zur Vorhersage von Markttrends – ML bietet vielfältige Ansätze, um Managemententscheidungen fundierter und effizienter zu gestalten.
Dieser Kurs richtet sich an Studierende der Wirtschaftswissenschaften (WiWi), die die Potenziale von ML verstehen und praktisch anwenden möchten, ohne dass tiefgehende Informatik- oder Programmierkenntnisse vorausgesetzt werden. Die Studierenden lernen nicht nur die grundlegenden Funktionsweisen von ML-Methoden kennen, sondern auch, wie diese in der Praxis zur Lösung unternehmerischer Herausforderungen eingesetzt werden können. 8. Mai 2025
MachineLearning für Management Entscheidungen (MLME)Lecture2Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz 23. Juli 2025
MachineLearning für Management Entscheidungen (MLME)Lecture11Prof. Dr. Kevin BauerLehrstuhl für Spieltheoretische und Kausale Künstliche Intelligenz",6.324248791,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. It correctly reports that the context does not contain the requested information and accurately describes the actual content of the provided text.,0,"The context is irrelevant. It provides general course information for 'MachineLearning fÃ¼r Management Entscheidungen (MLME)' [cite: 1441, 1442] but does not mention the 'Productivity J-Curve' paper or the company it discusses."
"According to the document, what is the economic decision rule for an investor?","'Invest'  if  p ⋅U (15,15)+ (1−p) ⋅ U(0,20) ≥U (10,0). 'Not invest' otherwise.","According to the document, the economic decision rule for an investor is:

“Invest” iff “𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)” “Not invest” otherwise.

Where:

*   𝑝 is the probability that the borrower would make a re-transfer.
*   𝑈15,15+1 is the utility of investing 15 million € and the startup working hard and paying back.
*   𝑈0,20 is the utility of investing 0 € and the startup working a little and running away with money.
*   𝑈(10,0) is the utility of investing 10 mil € and the startup working a little and running away with money.","8. Mai 2025
Data driven decision-makingWhat you implicitly did (probably) are two parts:•Compared how they feel about certain outcomes•Assigned a probability 𝑝 that the borrower would make a re-transfer Choose the strategy that maximizes your „utility“ (whatever this is)Economic decision rule for investor: „Invest“  iff  𝑝⋅𝑈15,15+1−𝑝⋅𝑈0,20≥𝑈(10,0)  „Not invest“ otherwise
12
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup : 20 mil €
Investor: 10 mil €Startup : 0 mil € human decisions from a nother prior empirical study 
(Study A, see below for more details) where we used 
the strategy method to measure participants ’ choices 
assuming the investor initially made the investment. 
That is, we kn ew whether each individual borrower 
would or would not make a repayment if an investor 
were to initially invest 10 MU with them. Importantly, 
due to the strategy method , we kn ew the borrower ’s 
conditional repayment decision regardless of the actual 
investor decision, i.e., we kn ew counterfactual 
outcomes. Game outcomes were determined by pairing 
a simulated investor decision with the borrower ’s 
actual conditional decision. 
Where do the ML system, selective labels problem, 
and continuous updating come into play? There are 
two pertinent aspects of the investment game. First, 
investors make initial decisions under uncertainty and 
form beliefs about the likelihood of borrowers making 
a repayment. Here, an ML  model prediction can 
decrease information asymmetries (Agrawal et al., 
2019) by providing individual-level predictions on the 
borrower’s repayment likelihood, increasing the 
potential for economically efficient outcomes. Second, 
investors only observe borrower behavior if they 8. Mai 2025
Data driven decision-making: an exampleConsider the following economic transaction
11
Investor: 15 mil €Startup : 15 mil €Invest 10 mil €You(the investor)
Not invest 0 €
StartupWork hard and pay back
Work a little and run away with moneyInvestor: 0 €Startup: 20 mil €
Investor: 10 mil €Startup: 0 €
What would you do, and why?",13.74808908,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. All components of the economic decision rule presented in the generated answer are explicitly present in the formula provided in the context.,1,"The context directly provides the economic decision rule in a clear formula: 'â€žInvestâ€œ iff ð‘â‹…ð‘ˆ(15,15)+(1âˆ’ð‘)â‹…ð‘ˆ(0,20)â‰¥ð‘ˆ(10,0) â€žNot investâ€œ otherwise'[cite: 1443]."
"According to the document, what are two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses?",A short-run dip and a long-run rebound.,"According to the document, the two phases of a productivity pattern that emerge when a General-Purpose Technology diffuses are:

1.  **Initial Dip:** A short-run dip in productivity is measured, often due to heavy up-front investment.
2.  **Long-Run Rebound:** Productivity accelerates once complementary investments are in place, driven by learning effects, scale economies, and network externalities.","8. Mai 2025
Expect a J-Curve
J-Curve: two-phase productivity pattern that emerges when a GePuTdiffuses•Short-run dipas measured productivity falls or stalls•Heavy up-front investmentin data, skills, process redesign, and complementary tech•Intangible assets (software, know-how) rise, but are poorly captured in standard measures•Resources diverted from current production to capability-building (output forgone)•Long-run rebound as productivity accelerates once complements are in place•Learning effects, scale economies, and network externalities kick in•Complementary investments start paying off measurably8 der Produktivität, dann sehen 
wir,
dass am Anfang bei vielen 
Technologien eigentlich
und das ist ein Average
erstmal die Produktivität runter 
geht.
Wir sind sozusagen an der 
unteren dieser Kurve,
diesem unteren Halbkreis von dem 
J.
Und erst danach geht es 
eigentlich nach oben.
Wir haben eine massiven 
Produktivität.
Das ist wie gesagt der die
Veränderung wir sehen hier ist 
erstmal runtergeht,
dann geht steil nach oben und 
das ist ein Phänomen,
was wir diese Kurve nennen.
Und das sehen wir immer wieder 
mit
verschiedenen General purpose 
technology.
Und das hat ganz einfache 
Gründe, nämlich,
dass am Anfang,
wenn wir in diese Technologien 
investieren,
wie in GPT investiert wird,
haben wir erstmal riesige 
anfängliche Investitionen,
die sich nicht sofort auszahlen.
Wir haben massive Investitionen,
beispielsweise darin,
dass Leute die Technologie 
richtig nutzen können.
Ich muss Schulungen machen,
ich muss die Infrastruktur 
schaffen,
ich muss die Anbindung an meine 
Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, Datenbank schaffen.
Ich muss ganz viele intangible 
Assets Know how kreieren,
um dann letztlich auch wieder 
weitere Innovationen.
Wir reden über General Purpose 
Technologien,
komplementäre Investitionen 
anzuregen.
Und das führt natürlich dazu, 
dass viele Ressourcen,
die eigentlich vielleicht auf 
der Produktivität
per se gelegen haben,
abgezogen werden und in
diese Produktivitätssteigerung 
investiert werden.
Und damit haben wir immer so ein 
bisschen.
Wir brauchen erstmal viel Zeit
und sollten nicht zu ungeduldig 
sein,
bis wir die Effekte dieser
neuen Technologie wirklich 
sehen.
Ich habe ihn letztes
Mal schon gesagt der große 
Missglauben
bei vielen Unternehmen ist 
weiterhin Ich nehme ei,
schmeiß es ins Unternehmen und 
es bringt sofort was.
Ne KI muss integriert sein.
Wir müssen die Prozesse da herum 
bauen,
Genauso wie wir die Fabriken
mit der Einführung der 
Elektrizität
im Vergleich zur Steam Engine 
umbauen mussten,
müssen wir unsere Organisationen 
adaptieren.
Und das führt dazu, dass wir 
eben am
Anfang diese Zeit brauchen,
um aus dieser Krise 
rauszukommen.
Und wir sehen die ersten 
Anzeichen,
dass wir jetzt möglicherweise
über KI Agenten nachdenken, da 
rauskommen.
Bei Software Development sind",9.842615604,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly describes the J-Curve as a 'two-phase productivity pattern' that includes a 'Short-run dip' and a 'Long-run rebound'.,1,The context directly answers the question. A slide titled 'Expect a J-Curve' describes the phenomenon as a 'two-phase productivity pattern' consisting of a 'Short-run dip' and a 'Long-run rebound'[cite: 1459].
What is the date of the first lecture?,45771,"The date of the first lecture is April 24, 2025.","24. April 2025
General informationLecture and tutorialLecture to introduce topics and cover theoryTutorial to work on applications (case studies & coding)Six tutorials by Charlotte Knickrehm How can you ask questions?Always ask questions on the runSend an e-mailOffice hours by appointmentWhat is expected of you?Prepare readingsIn class participation (lecture & tutorial)
20
This course is discussion-driven and relies a lot on your participation. 24. April 2025
Course material
The course•Is organized on OLAT (MLME); you need to register.•Material is in English because the majority ofthe central terminology is in English.
19Titel der Veranstaltung 24. April 2025
Any questions on course structure?",3.593627691,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context contains a slide dated '24. April 2025' which outlines the course structure, clearly indicating this is the first lecture.",1,"The context directly answers the question. A slide with the date '24. April 2025' [cite: 1476] has a header 'General information' and describes the structure of the lecture and tutorial, clearly indicating it is the first session."
What is the purpose of the tutorial sessions?,To work on applications (case studies & coding).,"The purpose of the tutorial sessions is to help participants develop a solid understanding of ML tools and to recognize their usefulness for management decisions. They also focus on practical exercises using the Python programming language to implement ML projects, even for those without prior knowledge.","23. Juli 2025
Clear & Specific
10
Before prompting•understand the context and the task you use the LLM for•think about what you want to achieveEffective prompts•clearly define objectives•feature direct and specific instructions•Give context•separate inputs and instructions consistently•Break up complex sentences Trainieren
selber gar nicht lange und
wir können sehr viele Kniffe 
benutzen,
wie wir dieses diesen
Trainingsprozess in einer 
gewissen Art
und Weise automatisieren können,
indem wir zum Beispiel
die Modellstruktur, die 
Modellauswahl,
sogar die Auswahl der relevanten
Inputinformationen durch 
bestimmte sogenannte
Cross Validation Verfahren durch
die Daten selber identifizieren 
lassen.
Ich hasse diesen Satz,
aber hier passiert dann sehr 
häufig dieses Let
the data speak so wenig anhaben 
wie möglich machen.
Wir versuchen einfach, das beste 
Modell zu finden.
Und auch das sei wieder betont, 
ist fundamental.
Uns geht es darum, ein Modell zu 
finden,
was nicht unsere historischen
Daten so gut es geht erklärt,
sondern sobald das Modell da 
ist,
möchten wir ein Modell haben,
was es uns ermöglicht,
Vorhersagen für die Zukunft zu 
machen,
also Vorhersagen für Beispiele,
Observationen, die das Modell 
noch nicht gesehen hat.
Denken Sie an das 
EiscremeBeispiel zurück.
Es ging nicht darum, perfekt zu 
erklären,
wie sich in der
Vergangenheit unsere 
Eisverkäufer abhängig
von bestimmten 
Wetterinformationen dargestellt 
haben,
sondern es ging darum Ich habe 
einen bestimmte
Logik identifiziert aus den 
historischen Daten. Ein besonderer Fokus des Kurses liegt auf praxisnahen Übungen: Mithilfe der Programmiersprache Python werden die Teilnehmer Schritt für Schritt an die Umsetzung von ML-Projekten herangeführt. Alle Programmierübungen sind so gestaltet, dass sie auch ohne Vorkenntnisse lösbar sind, und die Studierenden erhalten die notwendige Unterstützung, um die Inhalte erfolgreich umzusetzen. Ziel ist es, ein solides Verständnis für ML-Tools zu entwickeln und deren Nutzen für Managemententscheidungen zu erkennen.
Darüber hinaus wird der Kurs die aufkommende Bedeutung von generativen KI-Modellen wie GPT beleuchten. Studierende lernen, wie diese Modelle durch gezieltes Prompting eingesetzt werden können, um spezifische Managementfragen zu adressieren und Entscheidungsprozesse zu unterstützen.",6.24635148,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context states that a goal of the practical exercises is to 'entwickeln und deren Nutzen fÃ¼r Managemententscheidungen zu erkennen' (develop and recognize their usefulness for management decisions).,1,"The context directly answers the question. It states that a key focus is on practical exercises and that the 'Ziel ist es, ein solides VerstÃ¤ndnis fÃ¼r ML-Tools zu entwickeln und deren Nutzen fÃ¼r Managemententscheidungen zu erkennen' (The goal is to develop a solid understanding of ML tools and to recognize their usefulness for management decisions)[cite: 1487]."
"According to the document, what is a Class 2 customer?",A Profitable customer.,"The document does not define or mention ""Class 2 customer."" It describes data used to characterize customers, but doesn't categorize them into classes.","integration. The experience highlighted the crucial role 
of early, clear communication on data needs and the 
value of combining academic and business approaches 
to enrich data analysis and consumer insights. For further 
details on the implementation, see Online Appendix A.3.
3.2. Results
In our field experiment, we analyze results in two parts: 
first, the impact of green nudging on key business 
metrics and second, the mechanism underlying the 
effect on customer product returns. Importantly, our 
analyses comprise all customers who viewed their 
shopping cart at least once, even if they did not make a 
Figure 1. (Color online) Elements of Green Nudging 
Notes. The prompt appears when consumers check their carts, whereas the reminder is shown after payment. The dual nudge comprises both 
elements, and the single nudge consists of only the reminder after payment.
Table 1. Outcomes Captured in the Field Experiment for Individual Customers
Outcome Type Description
Returns (shipment) Binary 1 if the customer has returned at least one product, 0 otherwise
Returns (value) Numerical Value of products that the customer has returned
Cart conversion Binary 1 if the customer has made a purchase of the viewed cart, 0 otherwise ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - 24. April 2025
Data to characterize customers
30
Initial cartBrowsing""Digital footprint"" (Berg et al. 2020)•Cart value•# of products•# of eco-friendly products•# of duplicate products
•Visited category ""Women""•Visited category ""Kids""•Visited category ""Sale""•Visited category ""Sport""•Visit during working hours
•Operating system•Internet Browser•Internet Provider•State (IP geolocation)",5.308335543,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct list of contexts.",1,The generation is faithful. It correctly reports that the document does not define or mention a 'Class 2 customer'.,0,"The context is irrelevant. It describes data captured in a field experiment to characterize customers, such as cart value and browsing history[cite: 1506], but it does not categorize customers into classes like 'Class 2'."
What companies were mentioned as having faced discrimination and harassment allegations in 2020?,"Amazon, McDonald’s, and Pinterest.","According to the Business Insider article provided, Amazon, McDonald’s, and Pinterest were some of the companies that faced discrimination and harassment lawsuits in 2020.","against major companies like Amazon , 
McDonald’s, and Pinterest: These are some of 
the year ’s high -profile legal battles.  Business 
Insider. https://www.businessinsider.com/
every-company-that-was-sued-discrimination-
and-harassment-lawsuits-2020-2021-1  
Srivastava, M., Heidari, H., & Krause, A. (2019). 
Mathematical notions vs. human perception of 
fairness: A descriptive approach to fairness for 
machine learning. Proceedings of the 25th 
ACM SIGKDD International Conference on that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",5.619580507,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context contains a citation to a Business Insider article that lists 'Amazon, McDonaldâ€™s, and Pinterest' as companies facing legal battles.",1,"The context directly answers the question. It cites a Business Insider article with the title: '...against major companies like Amazon , McDonaldâ€™s, and Pinterest: These are some of the year â€™s high -profile legal battles'[cite: 1504]."
What is the URL for the document by Cowgill and Tucker (2019)?,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3361280,The URL for the document by Cowgill and Tucker (2019) is: https://doi.org/10.17705/1jais.00853,"DOI: 10.17705/1jais.00853 
Available at: https://aisel.aisnet.org/jais/vol25/iss4/9 
This material is brought to you by the AIS Journals at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in Journal of the Association for Information Systems by an authorized administrator of AIS Electronic 
Library (AISeL). For more information, please contact elibrary@aisnet.org. ness partner gained valuable insights into academic 
research, appreciating the depth and rigor that this 
approach adds to refining marketing strategies. For fur -
ther details on the design process and the considera -
tions made at this point, see Online Appendix A.1.
3.1.2. Treatments. Figure 1 illustrates the timing and 
appearance of our two green nudge elements. In our 
main treatment specification (dual nudge), customers 
observe both design elements throughout their shop -
ping journey. Specifically, when they first visit their 
cart, the cart prompt appears, and after they finish their 
payment, the reminder prompt appears.
To better understand the idiosyncratic impact of the 
two design elements, we further implemented a single- 
nudge treatment condition where customers only 
observe the reminder prompt. By doing so, we can 
explore whether the mere postpurchase intervention, 
which by definition, cannot affect the purchase behav -
ior as such, would suffice to curtail product returns.
Notably, the academic team members initially aimed 
to isolate the effects of the cart prompt and the 
reminder prompt, seeking to pinpoint their causal 
effects. However, budget and time limitations con - work as “Marketing Science. Copyright © 2024 The Author(s). https:/ /doi.org/10.1287/mksc.2022. 
0393, used under a Creative Commons Attribution License: https:/ /creativecommons.org/licenses/ 
by-nc-nd/4.0/.” 
Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE 
and the Deutsche Forschungsgemeinschaft [Grant 449023539]. 
Supplemental Material: The online appendix and data files are available at https:/ /doi.org/10.1287/mksc. 
2022.0393. 
Keywords: electronic commerce • nudging • causal forest • digital footprint • consumer returns • artificial intelligence
1. Introduction
In recent years, global e-commerce has experienced sig -
nificant and sustained growth. E-commerce presents 
numerous advantages for both customers and retailers. 
However, the lenient, and frequently free, return poli -
cies introduce drawbacks that complicate this expan -
sion. In the fashion industry, the share of purchases 
returned can reach a staggering 50% (Dzyabura et al. 
2023 ).
Product returns pose a substantial financial burden 
on retailers (Nageswaran et al. 2020 ). The costs of estab -
lishing return infrastructures (Ofek et al. 2011 ), incur -
ring transportation expenses (Zhou and Hinz 2016 ),",6.351892948,0,The generated answer provides the wrong URL. The ground truth provides the correct URL. They are factually different.,1,The generation is faithful. It correctly identifies that the URL provided in the context (aisel.aisnet.org) is different from the one in the query (ssrn.com).,0,"The context is irrelevant. It provides a DOI and a URL to an article on aisel.aisnet.org[cite: 1520], not to the ssrn.com URL specified in the query."
"In the 'Feedback Loops in Machine Learning' study, what did the authors note about the batch learning methodology?",This technique retrains the learning algorithm on all the currently available training data.,"The provided text doesn’t detail any specific observations about “batch learning methodology” within the “Feedback Loops in Machine Learning” study. It primarily focuses on the simulations built using data from Study A and Study B, which involved incentivized field studies and online experiments respectively.","15. Mai 2025
(Unwanted) feedback loops
A core benefit of machine learning: continued learning•Tackles data drift issues •Update model using newly collected data regularly•Retraining v. incremental learning proceduresHowever, ML predictions influence decisions•Selective labels problems(Lakkarajuet al. 2017)•Can endogenize dataavailable for updates19
DataML ModelPredictionDecision
Feedback Feedback Loops in Machine Learning 
 
817 
Despite this result, the left panel in Figure 5 indicates 
that continuous updating could not entirely restore 
economic efficiency to the level it would have attained 
in the absence of initial label imbalances in the training 
data. Nevertheless, our findings underscore significant 
and noteworthy improvements in economic efficiency 
concerning female borrowers. For male borrowers, the 
development of both economic efficiency measures was 
nearly identical, regardless of the presence or absence of 
initial label imbalances for women in the training data. 
In presenting results for the most extreme label 
imbalance scenario —where no positive (repaying) 
examples exist for women —a question naturally 
arises concerning the role of the initial degree of label 
imbalance. Does the initial degree of label imbalance 
influence the ability of continuous updating to 
mitigate algorithmic discrimination and enhance 
economic efficiency?  
We address this question using regression analyses 
reported in Table 1 (See Figures A1-A6 in the Appendix 
for a graphical illustration). Columns (1) and (2) use the 
gender gap in the ML model ’s share of positive Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were",7.114974976,0,"The generated answer is a refusal, stating the information is not in the provided text. The ground truth provides a specific factual answer. The two are semantically opposite.",1,The generation is faithful. It correctly states that the provided text does not contain specific observations about 'batch learning methodology' and accurately describes the content that is present.,0,"The context is irrelevant. It describes simulations built on data from two empirical studies [cite: 1545, 1547] but provides no details about 'batch learning methodology'."
"According to the document, what is one practical implication for organizations regarding continuously updated ML models?",Organizations should invest in comprehensive anti-discrimination and unconscious bias training for their decision makers.,"According to the document, one practical implication for organizations is that they should “invest in comprehensive anti -discrimination and unconscious bias training for their decision makers.”","ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that 
allows algorithmic discrimination to persist. This 
observation provides a new perspective on the 
dynamic between concept and data drift challenges, 
enriching prior research on drift origins and its 
mitigation (see, e.g., Lu et al. , 2018; Jameel et al., 
2020; Sahiner et al., 2023).  
5.3 Practical Implications 
From a practical standpoint, our results underscore the 
necessity for organizations and policymakers to 
consider both the dynamic and the sociotechnological 
nature of algorithmic discrimination. Our findings, 
highlighting the influence of human behavior on the 
discriminatory outcomes of continuously updated ML 
models, suggest that organizations should not only 
focus on reducing bias within ML models but also 
diligently address potential bias within the human 
decision-making processes that these models suppo rt. 
The implication for organizations is that they need to 
invest in comprehensive anti -discrimination and 
unconscious bias training for their decision  makers. availability of this data point for future model updates and potentially affecting the model ’s 
performance. This paper delves into the relationship between the continuous updating of ML models 
and algorithmic discrimination in environments where predictions endogenously influence the 
creation of new training data. Using comprehensive simulations based on secondary empirical data, 
we examine the dynamic evolution of an ML model ’s fairness and economic consequences in a 
setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate 
that continuous updating can help mitigat e algorithmic discrimination and enhance economic 
efficiency over time. Importantly, we provide evidence that human decision makers in the loop who 
possess the authority to override ML predictions may impede the self -correction of discriminatory 
models and even induce initially unbiased models to become discriminatory with time. These 
findings underscore the complex sociotechnological nature of algori thmic discrimination and 
highlight the role that humans play in addressing it when ML models undergo continuous updating. long term. Organizations adopting a dynamic 
sociotechnological perspective might succeed in 
creating a more equitable, effective, and efficient use 
of ML models. This approach aligns with societal 
expectations and regulatory requirements. 
From a policymaker ’s perspective, our findings 
regarding human involvement become particularly 
significant in light of impending regulations that 
mandate human participation in ML decision -making 
processes. These regulations pertain to final decisions in 
high-risk scenarios such as loan approval and hiring 
processes, as outlined in Article 22 of the General Data 
Protection Regulation and the European Commission’s 
Artificial Intelligence Act. However, our findings 
indicate a potential risk when humans have the final say 
in ML -supported decision -making processes. If 
individuals engage in discriminatory practices and the 
models update continuously, there is a risk of 
unintentionally perpetuating or amplifying algorithmic 
discrimination, as human discrimination hinders the self-
correcting capacities of ML models. Therefore, in 
conjunction with these regulations, we advocate 
measures aimed at identifying and mitigating 
(subconscious) discriminatory practices by",5.531574249,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly states that 'The implication for organizations is that they need to invest in comprehensive anti-discrimination and unconscious bias training for their decision makers'.,1,"The context directly answers the question. It states, 'The implication for organizations is that they need to invest in comprehensive anti-discrimination and unconscious bias training for their decision makers'[cite: 1556]."
"According to the document, what is one impact of human discrimination on the self-correction capabilities of continuously updated ML models?",It impedes the self-correction abilities.,"According to the document, the greater the extent of human discrimination in a dynamic system where ML models are continuously updated, the less likely it is that self-correcting mechanisms will occur. Furthermore, humans in the loop can undermine the powerful learning capabilities of ML systems, inadvertently teaching the machines to maintain or even adopt discriminatory behaviors over time.","imbalance combined with high or very high levels of 
human discrimination (see Columns 4 and 5).  
In sum, these results emphasize that the capacity of 
continuous ML model updating to mitigate algorithmic 
discrimination critically relies on the absence of 
discriminatory behavior by human decision makers. If 
such behaviors persist —potentially giving rise to the 
initial label imbalance in the training data—continuous 
updating has a minimal effect on improving the ML 
model’s behavior toward the discriminated group. Our 
findings even suggest that human discrimination can 
cause ML models, originally trained on balanced data and 
displaying no algorithmic discrimination, to adopt 
increasingly discriminatory behaviors, as measured by 
statistical parity. In the context of our loan approval 
example, the results reveal that the efficacy of 
continuously updated ML models notably declines if a 
loan officer persistently discriminates against female 
applicants, regardless of the creditworthiness assessment 
generated by the ML model. Consequently, the ML 
model’s prediction may persistently demonstrate gender 
disparity across two key dimensions: the ML model ’s 
propensity to deem female applicants as creditworthy and the greater the extent of human discrimination in a 
dynamic system where ML models are continuously 
updated, the less likely it is that self-correcting 
mechanisms will occur . We even observe d that 
unbiased models learn biased behaviors from humans 
over time. This happens because humans interfere with 
the endogenous creation of new observations, further 
reducing the chance that the existing training data can 
be supplemented with observations enabling the model 
to make better predictions for the initially 
disadvantaged group. This observation supports the 
arguments of IS researchers to consider (and examine) 
algorithmic discrimination as a sociotechnolo gical 
phenomenon (e.g., Favaretto et al., 2019). According 
to our findings, in environments where ML models are 
continuously updated, the behavior of human decision  
makers who observe ML predictions but have the final 
say may be a crucial factor in understanding and 
potentially even anticipating persistent algorithmic 
discrimination. Biases in human decision-making may 
directly influence the behavior of continuously 
updated ML models. While modern machine learning 
models inherently possess the capability to  “unlearn” discrimination. We highlight that continuous updating, 
absent enduring human discrimination, may stimulate  
self-correcting mechanisms. Theoretically, our findings 
underscore the sociotechnological nature of algorithmic 
discrimination. We extend the existing literature by 
revealing how algorithmic discrimination can diminish 
or emerge due to the interaction bet ween ML systems 
and human users in environments where continuous ly 
updating ML models is essential. We f ound evidence 
that humans in the loop can undermine the powerful 
learning capabilities of ML systems, inadvertently 
teaching the machines to maintain or even adopt 
discriminatory behaviors over time. Our insights 
provide a novel, dynamic perspective on algorithmic 
discrimination, enhancing the comprehension of its 
origins and moderating factors and highlighting the 
potency of continuous model updating to mitigate it. 
Our findings also speak to broader discussions on ways 
to address model drift challenges (see, e.g., Mallick et 
al., 2022). In our context, unjustified algorithmic 
discrimination can be thought of as an outdated input -
output relationship that the model learned from training 
data, i.e., as resulting from a concept drift. On the other",8.483189106,1,The generated answer and the ground truth are semantically identical. Both state that human discrimination impedes the self-correction ability of the models.,1,The generation is faithful. Both claims are explicitly supported by the text. The context states that 'the greater the extent of human discrimination... the less likely it is that self-correcting mechanisms will occur' and that humans can 'inadvertently teaching the machines to maintain or even adopt discriminatory behaviors'.,1,"The context directly answers the question. It states, 'We found that the greater the extent of human discrimination in a dynamic system where ML models are continuously updated, the less likely it is that self-correcting mechanisms will occur'[cite: 1569]. It also mentions that humans can teach machines to 'maintain or even adopt discriminatory behaviors over time'[cite: 1584]."
"According to the document, what is the approximate percentage point change in the share of positive predictions for female borrowers when there is no human discrimination and the maximum label imbalance is present?",0.22,"According to the document, the share of positive predictions for female borrowers increases from 0% in Iteration 1 to 22% in Iteration 100 when there is no human discrimination and the maximum label imbalance is present.","investors’ probability to invest with female borrowers. 
Figure 6 shows that the share of positive predictions  
(the two panels at the top) and false-negative rates (two 
panels at the bottom) for female borrowers depends on 
the level of human discrimination and the degree of the 
initial label imbalance . To illustrate the change over 
time in a parsimonious way, we pre sent the 
visualization for the first and the last iteration only.  
Figure 6 reveals that the effectiveness of continuously 
updating an ML model in mitigating algorithmic 
discrimination diminishes in the presence of 
discrimination by human decision makers. Specifically, 
the extent of human discrimination negatively impacts the 
growth of positive predictions for female borrowers 
across iterations, regardless of the initial label imbalance. 
For instance, a comparison of the upper two panels shows 
that at the maximum label imbalance (x-axis), the share 
of positive predictions (y-axis) for women increases from 
0% in Iterations 1 (left panel) to 22% in Iteration 100 
(right panel) if there is no human discrimination (z-axis). 
However, this growth is significantly curtailed, reaching 
merely 1.2%, 6.9%, 14.3% under very high, high, and perfect information benchmark (for female borrowers) is 
consistent across conditions (approx. +0.2 percentage 
points per iteration). However, the initial degree of 
economic efficiency diminishes with increasing label 
imbalance. Regarding the optimal investment with 
repaying female borrowers (Column 3), the positive time 
trend is higher in medium, high, and maximum label 
imbalance conditions (respectively +0.09, +0.13, +0.15 
percentage points) than in situations with low or no initial 
label imbalance (reference category). 
In summary, our findings highlight that the observed self-
correction properties of continuous updating are 
moderated by the degree of initial label imbalance in the 
training data. This holds, in particular, for algorithmic 
discrimination as measured by the gender gap in false-
negative error rates. These results, when interpreted in the 
context of our loan approval example, suggest that the 
extent of historical discrimination against female 
applicants, as reflected in the training data, influences the 
effectiveness of continuous model updating in mitigating 
the gender gap in the ML prediction model. This result 
holds for both the gap in the likelihood that the model will investments with repaying borrowers in the lower two 
panels) for female borrowers from Iteration 1 to 100 , 
conditional on combinations of initial label imbalances 
(x-axis) and the extent of persistent human 
discrimination ( z-axis).15 Echoing our results on the 
role of human discrimination in mitigating algorithmic 
discrimination over time, we f ound that the ability of 
continuous updating to enhance economic efficiency 
decreases as human discrimination intensifies. For 
instance, a comparison of the upper two panels shows 
that when the initial label imbalance is at its maximum 
(x-axis), the share of game outcomes with female 
borrowers that match the perfect information 
benchmark (y-axis), grows from 42.3% In Iteration 1 
(left panel)  to 62.5% in Iteration 100 (right panel)  if 
there is no human discrimination ( z-axis). A similar 
comparison of the two panels at the bottom reveals that 
the share of investments with repaying borrowers also 
grows from 5.7% in Iteration 1 (left panel) to 34.9% in 
Iteration 100 (right panel). 
 
15 We present corresponding figures for male borrowers in 
Figures A9 and A10 in the Appendix. 
Conversely, under conditions of medium, high, and",8.413595438,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly states that 'the share of positive predictions... for women increases from 0% in Iterations 1... to 22% in Iteration 100... if there is no human discrimination'.,1,"The context directly answers the question. It states that 'at the maximum label imbalance (x-axis), the share of positive predictions (y-axis) for women increases from 0% in Iterations 1 (left panel) to 22% in Iteration 100 (right panel) if there is no human discrimination'[cite: 1593]."
"In the 'Feedback Loops in Machine Learning' study, what was the share of game outcomes matching the perfect information benchmark for female borrowers in the first iteration?",42.3%.,"The text states that with a high initial label imbalance, the false-negative error rate for female borrowers decreased from 55.4% in Iteration 1 to 35.8% in Iteration 100 if there was no human discrimination. However, it doesn’t explicitly state the share of game outcomes matching the perfect information benchmark in the first iteration.","perfect information benchmark (for female borrowers) is 
consistent across conditions (approx. +0.2 percentage 
points per iteration). However, the initial degree of 
economic efficiency diminishes with increasing label 
imbalance. Regarding the optimal investment with 
repaying female borrowers (Column 3), the positive time 
trend is higher in medium, high, and maximum label 
imbalance conditions (respectively +0.09, +0.13, +0.15 
percentage points) than in situations with low or no initial 
label imbalance (reference category). 
In summary, our findings highlight that the observed self-
correction properties of continuous updating are 
moderated by the degree of initial label imbalance in the 
training data. This holds, in particular, for algorithmic 
discrimination as measured by the gender gap in false-
negative error rates. These results, when interpreted in the 
context of our loan approval example, suggest that the 
extent of historical discrimination against female 
applicants, as reflected in the training data, influences the 
effectiveness of continuous model updating in mitigating 
the gender gap in the ML prediction model. This result 
holds for both the gap in the likelihood that the model will Feedback Loops in Machine Learning 
 
819 
 
 
Note: We illustrate how the mean share of positive predictions and mean false-negative error rates for female borrowers in Iterations 1 and 100 
depend on the initial label imbalance in the training data and the degree of human discrimination by the investor. A comparis on of panels for 
Iterations 1 and 100 for the two measures of algorithmic discrimination provides insights into their conditional development across iter ations. 
Abbreviations: False-neg. rate = false-negative rate; Init. lab. imb. = initial label imbalance; Human discr. = human discrimination. 
Figure 6. Development of the Mean Share of Positive Predictions and False-Negative Error Rates 
Figure 6 also shows that an increase in human 
discrimination undermines the reduction in false -
negative error rates for female borrowers over time. 
For instance, the lower two panels show that with a 
high initial label imbalance (x-axis), the false-negative 
error rate (y-axis) for female borrowers decreases from 
55.4% in Iteration 1 (left panel)  to 35.8% in Iteration 
100 (right panel)  if there is no human discrimination  
(z-axis). However, under very high human 
discrimination, this rate only drops to 52.2%  in fold to 34.8%, signifying an impressive gain in 
efficiency (see dashed line with marker in Panel i). In 
our loan approval example, continuous updating of the 
ML model increase d the number of repaying female 
applicants who could secure credit. 
 
Note: We depict the development of the mean share of game outcomes that are equal to outcomes under perfect information and mean shares of 
games where investors invest in repaying borrowers. We show results separately for female and male borrowers. The left Panel (i) depicts results 
for the condition where the initial label imbalance for the minority class was at the maximum, whereas the right Panel (ii) depicts results for the 
condition where there was no initial label imbalance. 
Figure 5. Development of the Mean Share of Game Outcomes",9.758308172,0,"The generated answer is a refusal, stating the information is not in the text. The ground truth provides a direct, factual answer to the question. The two are semantically opposite.",1,The generation is faithful. It correctly reports that the context does not contain the specific information requested.,0,"The context is irrelevant. It provides information about the false-negative error rate for female borrowers under certain conditions[cite: 1615], but it does not state the share of game outcomes that matched the perfect information benchmark in the first iteration."
"In the ""Feedback Loops in Machine Learning"" study, what type of model was utilized to predict whether an investment would occur in the investment game simulation?",A gradient boosted forest.,The study utilized a trained machine learning model to predict whether an investment would occur. The model leveraged 31 features to predict whether a given investor would invest with a given borrower.,"Feedback Loops in Machine Learning 
 
813 
Armed with this information, the investor decided 
whether to invest. Notably, in our simulation , we 
mimicked investor decisions using a trained model (see 
below for more information). If an investment was 
made, the borrower decided on repayment, and a new 
observation encapsulating a borrower’s specific 
personal traits (features) and actual repayment decision 
(label) was appended to the dataset used to train the ML 
model predicting repayment. 
Importantly, to avoid inappropriate overfitting and 
spillover effects, this observation was only included if 
the dataset didn’t already contain a record of this exact 
borrower. Conversely, if an investor declined to invest, 
the individual game conclude d without eliciting a 
repayment decision from the borrower or appending a 
new observation to the training data. In other words, the 
growth of the training data was endogenous and 
selectively include d borrowers who received an 
investment (Lakkaraju et al., 2017). After all decisions 
were made, the iteration concluded with an update of the 
ML model predicting repayment by training it from 
scratch on the expanded dataset incorporating records Feedback Loops in Machine Learning 
 
811 
3.2 Previous Empirical Studies 
We buil t our simulations on two secondary datasets, 
collected by several authors of this paper as part of 
different empirical studies. Study A was an incentivized 
field study conducted over three years from 2016 to 
2019. Study A measured participant s’ behavior in the 
investment game  defined above  in their  role as 
borrowers. The data from Study A allow ed us to 
develop an ML model predicting borrowers’ repayment 
decision and serves as the foundation for simulating 
borrower behavior. Study B, conducted in December 
2020, was an incentivized online experiment where 
participants engaged in the outlined investment game in 
the role of investors. In this experiment, investors could 
access an ML model predicting borrowers ’ repayment 
likelihood. Notably, this model, along with the 
borrowers with whom investors from Study B 
interacted, originated from Study A data. The data from 
Study B provide d the basis for simulating investor 
decisions. We present the exact instructions given in 
these studies in Appendix B.1. 
Study A: At the onset of each semester, first -semester 
economics students from a large German university were The second random part of our Study B data populated 
our pool of investors for our simulation (303 unique 
participants from Study B), whose observations we 
excluded from the model’s training. Each investor was 
characterized by their 20 personal attributes, and their 
investment decisions for a specific borrower were 
simulated. When a certain investor from the pool was 
randomly selected in an iteration and paired with a 
borrower, the trained model predict ed whether an 
investment would occur. The prediction of the 
likelihood of an investment occurring was based on the 
investor’s 20 personal characteristics, the borrower ’s 
10 characteristics, and a repayment forecast for the 
borrower, i.e., the model leverage d 31 features to 
predict whether a given investor w ould invest with a 
given borrower. An investment took place only if the 
predicted probability surpasse d the threshold of 50 % 
(in the condition where there was no human 
discrimination). When using the borrowers the 
individuals from our investor pool actually i nteracted 
with in Study B as a true out -of-sample test set and a 
50% threshold, we found that, on average, the trained 
model correctly replicated investor decisions in",7.156384468,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context states that a 'trained model' was used to predict investments, and another part of the text specifies that 'The employed model is a gradient boosted forest'.",1,"The context directly answers the question. While it also mentions simulating investor decisions with a trained model [cite: 1621], it specifies that the underlying model type used for prediction was a 'gradient boosted forest'[cite: 1037]."
What are some contexts in which algorithmic discrimination has been observed?,"Algorithmic discrimination has been observed in recidivism risk assessments, education, recruitment, predictive policing, health risk evaluations, targeted advertising, crowdlending scenarios, and facial recognition tasks.","Based on the provided text, here are some contexts in which algorithmic discrimination has been observed:

*   **Algorithmic Recruitment:** Bias can arise in the development of rules for algorithmic recruitment.
*   **Forecasting:** Historical inequalities are projected into the future by algorithms.
*   **Development of Algorithmic Rules:** Bias can be introduced during the development of rules by engineers and annotators when handling unstructured data.

The text suggests that this bias can be found in systems trained on data lacking diverse representation.","data already reﬂects social prejudices, and the algorithm also
incorporates biased relationships, leading to the“bias in and bias
out” phenomenon (36KE, 2020). This phenomenon means that
discrimination and disparities exist, just like in forecasting, where
historical inequalities are projected into the future and may even
be ampliﬁed (Mayson, 2018).
A research team at Princeton University discovered that
algorithms lack access to the absolute truth. The machine corpus
contains biases that closely resemble the implicit biases observed
in the human brain. Artiﬁcial intelligence has the potential to
perpetuate existing patterns of bias and discrimination because
these systems are typically trained to replicate the outcomes
achieved by human decision-makers (Raso et al.2018). What is
worse, the perception of objectivity surrounding high-tech
systems obscures this fact.
In summary, if an algorithmic system is trained on biased and
unrepresentative data, it runs the risk of replicating that bias.
Data feature selection: designer bias. The introduction of bias is
sometimes not immediately apparent in model construction
because computer scientists are often not trained to consider discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible of inadvertently perpetuating or igniting algorithmic 
discrimination if these individuals engage in 
discriminatory practices. In this context, efforts aimed at 
identifying and mitigating (subconscious) discriminatory 
practices of organizational decision  makers, such as 
awareness training, become an indispensable 
complementary element to ensure that continuously 
updated ML models exhibit desirable behaviors. 
2 Theoretical and Conceptual 
Background 
In this section, we first provide the conceptual 
underpinnings of algorithmic discrimination. 
Subsequently, we discuss the literature gaps our work 
aims to address.",10.0212965,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. It correctly reports that the context mentions algorithmic bias in recruitment and forecasting but does not include the other specific examples from the ground truth.,0,"The context is irrelevant. It discusses algorithmic bias in the context of recruitment and forecasting [cite: 1640, 1641] but does not mention the specific examples listed in the ground truth answer (recidivism, predictive policing, etc.)."
What is algorithmic discrimination?,Algorithmic discrimination refers to discriminatory hiring practices that arise from algorithmic bias.,"According to the text, algorithmic discrimination is:

“Discrimination involves statements or actions that unjustifiably disadvantage members of particular social groups. These actions typically stem from conscious or subconscious attitudes, prejudices, or emotional associations tied to characteristics such as gender, age, disability, race, language, religion, or sexual identity.”

More simply, it’s when algorithmic systems (based on ML models) produce outputs that disproportionately benefit or disadvantage specific groups, even when there’s no intended justification for that disparity. It can arise from biases within the training data itself.","Feedback Loops in Machine Learning 
 
807 
2.1 Algorithmic Discrimination 
Algorithmic discrimination: Discrimination involves 
statements or actions that unjustifiably disadvantage 
members of particular social groups. These actions 
typically stem from conscious or subconscious attitudes, 
prejudices, or emotional associations tied to 
characteristics such as  gender, age, disability, race, 
language, religion, or sexual identity (Scherr, 2008). In 
today’s era, where algorithmic systems based on ML 
models frequently aid human decisions, researchers and 
practitioners often employ the te rm “algorithmic 
discrimination” to refer to system outputs that 
disproportionately benefit or disadvantage specific 
groups, even when no justification for the disparate 
treatment exists from a ground -truth perspective 
(Kordzadeh & Ghasemaghaei, 2022).1  
Algorithmic discrimination can originate from various 
sources. First, it may stem from intentional or 
unintentional social biases encoded within the training 
data (Friedman & Nissenbaum, 1996; Romei & Ruggieri, 
2013; Kleinberg et al., 2018). For example, if a training 
dataset holds more positively labeled examples for men, 
the ML model may predict positive labels less accurately Despite algorithms aiming for objectivity and clarity in their
procedures, they can become biased when they receive partial
input data from humans. Modern algorithms may appear neutral
but can disproportionately harm protected class members, posing
the risk of“agentic discrimination”(Prince and Schwarcz,2019).
If mishandled, algorithms can exacerbate inequalities and
perpetuate discrimination against minority groups (Lloyd,2018).
Within the recruitment process, algorithmic bias can manifest
concerning gender, race, color, and personality.
Research methodology
The primary research strategy was a literature review approach.
This review aimed to assess current research on recruitment
supported by artiﬁcial intelligence algorithms. The systematic
review process included gathering and evaluating the selected
studies’ literature and topics. Driven by the direction of the
research, studies focusing on algorithmic discrimination in
recruitment over the past 10 years were included unless past
literature was worth reviewing. This is because this is a relatively
new phenomenon that has become prominent over the past 10
years. In deﬁning the “algorithmic and hiring discrimination”
literature, a fairly broad approach was taken based on article discrimination
Algorithms are not inherently discriminatory, and engineers
rarely intentionally introduce bias into algorithms. However, bias
can still arise in algorithmic recruitment. This issue is closely
linked to the fundamental technology behind AI and ML. The ML
process can be simpliﬁed into several stages, each involving three
key components contributing to algorithmic bias: dataset con-
struction, the engineer’s target formulation, and feature selection
(36KE, 2020). When the dataset lacks diverse representation from
different companies, bias may be introduced during the devel-
opment of algorithmic rules by engineers and when annotators
handle unstructured data (Zixun,2020).
Datasets: bias soil. Datasets serve as the foundation of machine
learning (ML). If an algorithm’s data collection lacks quantity and
quality, it will fail to represent reality objectively, leading to
inevitable bias in algorithmic decisions. Researchers commonly
use a 95% conﬁdence level, which provides 95% certainty but still
leaves a one in twenty chance of bias (Raub,2018). Nearly every
ML algorithm relies on biased databases.
One issue arises when datasets are skewed towards accessible",10.54373622,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The quoted definition and the subsequent summary are both directly supported by the explicit definition of 'Algorithmic discrimination' provided in the context.,1,"The context directly answers the question. It defines 'Algorithmic discrimination' as system outputs that 'unjustifiably disadvantage members of particular social groups' [cite: 1662] and 'disproportionately benefit or disadvantage specific groups, even when no justification for the disparate treatment exists'[cite: 1663]."
"According to the document, what software was used as an auxiliary tool to clarify ideas and improve work efficiency?",Nvivo 12.0 Plus qualitative analysis software.,"The document mentions a ""Scrips Modell"" as a tool to decompose workflows and organize projects. However, it doesn't elaborate on its specific functionalities or how it aids in clarifying ideas or improving work efficiency.","5. Juni 2025
Idea of Mixed-Initiative (Horvitz, 1999)
Mixed-initiative: let system decide when to automate, provide advice, or let human take over•Classic paper by Eric Horvitz from 1999 (Chief Scientific Officer, Microsoft)•Advocates elegant coupling of automation with direct manipulation by user•Autonomous actions should be taken only when a system infers that they will have greater expected valuethan inaction for the user.Assume a system can infer p(G|E): likelihood of user goal being G given evidence E•Decide whether to take actionA, based on the utility that action would have in achieving goal G •rather than other possible outcomes (not-G) 12
𝑈𝐴,𝐺 𝑈𝐴,¬𝐺𝑈¬𝐴,𝐺𝑈¬𝐴,¬𝐺No actionActionDesiredgoal Undesiredgoal
Consequences of automated system actions nachzudenken,
ist diese Dekomposition von 
Workflows und die
Organisation von solchen 
Projekten
kann stattfinden über diese 
Scrips Modell.
Und letztlich haben wir 
unterschiedliche Level der
Integration von diesen Machine 
Learning Innovationen
innerhalb der Organisation.
Hört sich alles super an!
Problem ist da kann massiv viel 
schiefgehen.
Selbst wenn ich diesen ganzen 
Prozess durchgedacht habe,
sind es an vielen Ecken immer 
noch Probleme,
die sich beispielsweise hier bei 
Amazon vor einigen
Jahren trotz hochintelligenter 
Leute,
trotz super Strukturen,
trotz tollen technologischen 
Möglichkeiten
ist es da zu einem großen 
Skandal gekommen,
die man von diesem Skandal 
damals gehört.
Müsste ungefähr in die Zeit, wo 
sie alle
Abitur gemacht haben, gewesen 
sein.
Ja, das war weil die.
Also ich bin mir nicht mehr 
sicher.
Ich glaube, da wurden die 
früheren
Recruitingdaten von Amazon 
genommen,
in eine AI eingespeist.
Und da die früheren Recruiter
biased gegenüber Frauen waren,
hat die AI das Muster erkannt 
und ebenso übernommen.
Genau.
Wir haben ein System
gehabt und ich würde gar nicht 
mal sagen,
dass die Recruiter per se 
gebeizt waren.
Ich wahrscheinlich auch ein 
Teil.
Ich will den Leuten da nichts 
unterstellen. den besten Bewerber.
Finde aus einer größeren Menge 
von möglichen Bewerbern.
Ich denke also wieder mit Was 
sollte sich innerhalb
meines Prozesses
innerhalb dieses Workflows auch 
verändern?
Integriert mehr Integration?
Und ein Beispiel hierfür ist 
Ada. Kennt das jemand?
Hat jemand schon mal gehört,
es war ein Startup,
was in
der Pre Curve Zeit ein Customer 
Service Angebot hatte?
Die haben die Automatisierung
damals auch nicht mitgenerativen 
KI Systemen.
Aber ein sehr
clever zusammengestelltes 
Applikationssystem,
wo Customer Compliance gehandelt 
worden sind.
Das wurde aufgekauft innerhalb 
der
kurze Zeit von einem uns
allen bekannten großen Provider 
von digitalen Calls,
nämlich Zoom Zoom hat 
festgestellt,
als alle in die digitalen
Räume zu kurze Zeit gegangen 
sind,
dass es sehr viele Beschwerden
insbesondere von bestimmten 
Altersgruppen gab,
die mit dieser Technologie nicht 
klargekommen sind,
was dazu geführt hat,
dass dieser Ansturm, dieser 
Komplements,
die gesamte Service und
die gesamte Marketing Abteilung 
von Zoom
eigentlich den ganzen Tag auf 
Trab gehalten hatten.
Sie konnte trotzdem nicht alles
abarbeiten und dieses System,
was sie installiert haben,
war jetzt nicht einfach nur Ich 
habe ein System,",7.108155251,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct name of the software.",1,"The generation is faithful. It accurately reports that the document mentions a 'Scrips Modell' but does not provide specific details about its functionality, and it does not mention 'Nvivo 12.0 Plus'.",0,"The context is irrelevant. It mentions a 'Scrips Modell' [cite: 1684] for organizing projects, but does not name or describe the 'Nvivo 12.0 Plus' software."
What was the result of Microsoft revising their dataset for training the Face API?,A 20-fold reduction in the recognition error ratio between men and women with darker skin tones.,"According to the text, Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones and a 9-fold reduction for women.","uncover hidden biases and remove them automatically. For
example, Microsoft revised their dataset for training the Face API,
resulting in a 20-fold reduction in the recognition error ratio
between men and women with darker skin tones and a 9-fold
reduction for women by balancing factors such as skin color, age,
and gender (Grabovskyi and Martynovych,2019).
Integrating “small data” and “big data” can enhance accuracy
(36KE, 2020). Data should not solely rely on extensive collections
but also focus on precision. While big data analysis tends to
emphasize correlations, which can lead to errors when inferring
causation, small data, which is more user-speciﬁc, offers detailed
information and helps avoid such mistakes. Combining the
vastness of big data with the precision of small data can help
somewhat mitigate hiring errors (Kitchin and Lauriault,2015).
Biases in datasets can be identi ﬁed through autonomous
testing. The inaccuracies stemming from incomplete past data
can be addressed through “oversampling” (Bornstein, 2018).
Researchers from MIT demonstrated how an AI system called
DB-VEA (unsupervised learning) can automatically reduce bias
by re-sampling data. This approach allows the model to learn one model predicts it differently (Gulzar et al.,2019).
Bias detection tool. In September 2018, Google introduced the
innovative What-If tool for detecting bias (Mishra, 2022). It
assists designers in identifying the causes of misclassiﬁcation,
determining decision boundaries, and detecting algorithmic
fairness through interactive visual interfaces. Additionally, Face-
book has developed Fairness Flow, an emerging tool for
correcting algorithmic bias. Fairness Flow automatically notiﬁes
developers if an algorithm makes unfair judgments based on race,
gender, or age (Kessing,2021).
Improving the algorithm ’s ethics from a management
perspective
Internal ethics governance. Several major technology companies
have published AI principles addressing bias governance, sig-
naling the start of self-regulation (36KE, 2020). Microsoft has
formed an AI and ethical standards committee to enforce these
principles, subjecting all future AI products to ethics scrutiny
(Smith and Shum,2018). Google has responded by introducing a
Model Card function, similar to an algorithm manual, that
explains the employed algorithm, highlights strengths and
weaknesses, and even shares operational results from various
datasets (Mitchell et al.,2019). a data -generating process distinct from subsequent 
simulation iterations, then the ML model’s bias against 
women essentially emerges from a concept dri ft 
(Widmer & Kubat, 1996). This means the initial ML 
model approximates a once-accurate but now outdated 
data-generating process. The observed feedback loop, 
where the ML model progressively becomes less 
biased, then demonstrates its ability to overcome 
concept drifts when updated with new training data it 
influences. Conversely, persistent human 
discrimination shifts the training data ’s gender 
composition, increasingly diverging from the borrower 
pool over iterations. However, the core relationship 
between borrower attributes and their repayment 
propensity remains unchanged. Thus, ongoing human 
discrimination aligns with a n ongoing  data drift 
(Mallick et al. , 2022), as the incoming data ’s 
composition on which the model predicts, deviates 
from its training data. With biased decision-making by 
humans in the loop  undermining the discriminating 
ML model ’s capacity to self -correct, i.e., overcome 
concept drift through continuous updating, one may 
interpret our finding s as evidence for an adverse 
interaction between a data and a concept drift that",6.980874538,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that 'Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones'.",1,"The context provides a direct answer. It states, 'Microsoft revised their dataset for training the Face API, resulting in a 20-fold reduction in the recognition error ratio between men and women with darker skin tones'[cite: 1698]."
What did Amazon do with their ML-based hiring tool?,Amazon withdrew the tool and developed a new unbiased algorithm.,"According to the text, Amazon withdrew their ML-based hiring tool after it exhibited gender bias. They then developed a new, unbiased algorithm.","The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not
classify candidates neutrally for gender (Miasato and Silva,2019).
The bias stemmed from training the AI system on predominantly
male employees ’ CVs (Beneduce, 2020). Accordingly, the
recruitment algorithm perceived this biased model as indicative
of success, resulting in discrimination against female applicants
(Langenkamp et al. 2019). The algorithm even downgraded
applicants with keywords such as “female” (Faragher, 2019).
These ﬁndings compelled Amazon to withdraw the tool and
develop a new unbiased algorithm. However, this discrimination
was inadvertent, revealing theﬂaws inherent in algorithmic bias
that perpetuates existing gender inequalities and social biases
(O’neil, 2016).
Race. Microsoft’s chatbot Tay learned to produce sexist and racist
remarks on Twitter. By interacting with users on the platform,
Tay absorbed the natural form of human language, using human
tweets as its training data. Unfortunately, the innocent chatbot
quickly adopted hate speech targeting women and black indivi- in discrimination (Miasato and Silva,2019).
Furthermore, personal biases can manifest in the selection of
data characteristics. For example, engineers may prioritize speciﬁc
features or variables based on how they want the machine to
behave (Miasato and Silva, 2019)). The Amazon hiring case
illustrates this, where engineers considered education, occupation,
and gender when assigning labels to the algorithm. When gender
is considered the crucial criterion, it inﬂ
uences how the algorithm
responds to the data.
Theme III. Which algorithmic recruitment discrimination
exists
In the recruitment process, algorithmic bias can be manifested in
terms of gender, race, color, and personality.
Gender. Gender stereotypes have inﬁltrated the “lexical embed-
ding framework” utilized in natural language processing (NLP)
techniques and machine learning (ML). Munson ’s research
indicates that “occupational picture search outcomes slightly
exaggerate gender stereotypes, portraying minority-gender occu-
pations as less professional”((Avery et al.,2023; Kay et al.,2015).
The impact of gender stereotypes on AI hiring poses genuine
risks (Beneduce,2020). In 2014, Amazon developed an ML-based
hiring tool, but it exhibited gender bias. The system did not (Bogen and Rieke,2018). The resumes are screened to match the
job description better. The system can rank candidates according
to the relevance of the qualiﬁcation metrics.
The following phase is the interview. It is probably the most
individual stage of the selection process and, thus, unlikely to be
fully automated by artiﬁcial intelligence. However, some AI tools
enable recruiters to conduct video interviews and research
candidates’reactions, voice tones, and facial expressions (Ahmed,
2018).
The ﬁnal stage is the selection stage, which is the stage where
the employer makes theﬁnal employment decision. In this stage,
AI systems can calculate remuneration and bene ﬁts for
companies and anticipate the risk that candidates would violate
workplace rules. (Bogen and Rieke,2018).
AI-based recruitment beneﬁts
Recruitment quality. Beattie et al. (2012) found that some large
companies believe unconscious bias affects recruitment quality.
Organizations may need to hire more qualiﬁed people to avoid
ﬁnancial losses (Newell,2015). Artiﬁcial intelligence has become a
part of the recruitment industry to automate the recruiting and
selecting process, which can remove unconscious human bias that",5.461895943,1,The generated answer and the ground truth are semantically identical.,1,The generation is faithful. The context explicitly states that Amazon was 'compelled... to withdraw the tool and develop a new unbiased algorithm'.,1,"The context directly answers the question. It says that after exhibiting gender bias, Amazon was 'compelled... to withdraw the tool and develop a new unbiased algorithm'[cite: 1727]."
"According to the document, what is “agent discrimination”?","When partial human data is provided to a machine, it will eventually lead to the risk of “agent discrimination.”","The document does not define or discuss “agent discrimination.” It outlines four theoretical groups for understanding discrimination in algorithmic recruitment:

1.  Competitive market theory
2.  Monopoly model of discrimination
3.  Statistical theory of discrimination
4.  Antecedent market discrimination hypothesis

It focuses on algorithmic discrimination within recruitment, not a specific type called “agent discrimination.”","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. facial identiﬁcation.
Data protection and non-discrimination laws safeguard against
discriminatory practices in algorithmic decision-making. In the
EU region, Article 14 of the European Convention on Human
Rights (ECHR) guarantees the rights and freedoms outlined in
the Convention, prohibiting direct and indirect discrimination
(Zuiderveen Borgesius, 2020). Non-discrimination laws, particu-
larly those about indirect discrimination, serve as a means to
prevent various forms of algorithmic discrimination. The EU
General Data Protection Regulation (GDPR), implemented in
May 2018, addresses the impact of ML algorithms and offers a
“right to explanation” (e.g., Articles 13–15) (Peña et al., 2020),
enabling individuals to request explanations for algorithmic
decisions and demand measures to avoid discriminatory
inﬂuences when handling sensitive data. The GDPR mandates
organizations to conduct a Data Protection Impact Assessment
(DPIA), with each EU member state must maintain an
independent data protection authority vested with investigative
powers. Under the GDPR, a data protection authority can access
an organization’s premises and computers using personal data
(Zuiderveen Borgesius, 2020).
Investigation and analysis technical and managerial aspects to tackle discrimination in
algorithmic hiring. This study contends that resolving algorithmic
discrimination in recruitment requires technical solutions and the
implementation of internal ethical governance and external
regulations.
The subsequent study is structured intoﬁve parts. The ﬁrst
section provides the theoretical background for this research. The
following section outlines the research methodology employed in
the literature review and identiﬁes four key themes. The third
section delves into a detailed discussion of these four themes:
applications and beneﬁts of AI-based recruitment, factors con-
tributing to algorithmic recruitment discrimination, types of
discrimination in algorithmic recruitment, and measures to
mitigate algorithmic hiring discrimination. The fourth section
involves conducting a survey among respondents and analyzing
the primary data collected to support our study. Theﬁnal section
concludes by suggesting future directions for research.
Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender",8.235580206,0,"The generated answer is a refusal, which is incorrect. The ground truth provides the correct definition.",1,The generation is faithful. It correctly reports that the document does not define 'agent discrimination' and accurately lists the discrimination theories that are discussed.,0,"The context is irrelevant. It discusses various theories of discrimination in the labor market, such as competitive market theory and statistical discrimination theory[cite: 1749, 1751], but it does not define or mention 'agent discrimination'."
"According to the document, what are some characteristics used in discrimination theory?","Race, ethnicity, color, and gender.","According to the document, here are some characteristics used in discrimination theory:

*   **Race, ethnicity, color, and gender** (as defined by the ILO’s Convention 111)
*   **Ethnic origin, gender, skin color, and age**
*   **Inadequate information** (as a non-objective variable)

The document also discusses various theoretical models of discrimination, including:

*   Competitive market theory
*   Monopoly model of discrimination
*   Statistical theory of discrimination
*   Antecedent market discrimination hypothesis","Theory background
Discrimination theory. Discrimination in the labor market is
deﬁned by the ILO’s Convention 111, which encompasses any
unfavorable treatment based on race, ethnicity, color, and gender
that undermines employment equality (Ruwanpura, 2008).
Economist Samuelson (1952) offers a similar deﬁnition, indicat-
ing that discrimination involves differential treatment based on
personal characteristics, such as ethnic origin, gender, skin color,
and age.
Various perspectives on the causes and manifestations of
discrimination can be broadly categorized into four theoretical
groups. Theﬁrst is the competitive market theory, which explains
discriminatory practices within an equilibrium of perfect
competition (Lundberg and Startz, 1983). This view attributes
discrimination primarily to personal prejudice. The second is the
monopoly model of discrimination, which posits that mono-
polistic power leads to discriminatory behavior (Cain,1986). The
third is the statistical theory of discrimination, which suggests
that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category. that nonobjective variables, such as inadequate information,
contribute to biased outcomes (Dickinson and Oaxaca, 2009).
Lastly, we have the antecedent market discrimination hypothesis
as the fourth category.
Statistical discrimination theory. Statistical discrimination refers
to prejudice from assessment criteria that generalize group
characteristics to individuals (Tilcsik, 2021). It arises due to
limitations in employers’research techniques or the cost con-
straint of obtaining information in the asymmetry between
employers and job seekers. Even without monopolistic power,
statistical discrimination can occur in the labor market due to
information-gathering methods. Employers are primarily inter-
ested in assessing candidates ’ competitiveness when making
recruitment decisions. However, obtaining this information
directly is challenging, so employers rely on various indirect
techniques.
Discrimination carries both individual and societal economic
costs. The social cost arises from the decrease in overall economic
output caused by discrimination. However, this is still deemed
efﬁcient under imperfect information and aligns with the
employer’s proﬁt maximization goal. Therefore, it is likely that discrimination (Favaretto et al., 2019; Kordzadeh & 
Ghasemaghaei, 2022). The social component often 
originates from societal and economic structural biases, 
which can foster the differential treatment and 
marginalization of certain groups. On the other hand, the 
technological element emanates from the perpetuated 
discriminatory behaviors of algorithmic systems. 
Algorithmic feedback loops: Feedback loops play a 
significant role in algorithmic discrimination in the 
context of continuously updated ML models. These loops 
arise when the outcomes of algorithmic decisions provide 
new endogenous training data for subsequent model 
updates (Cowgill, 2018). Such loops iteratively weave 
together the social environment ’s outcomes and the 
technical performance of the algorithmic output. For 
instance, take the loan approval process depicted in 
 
1 Since the biased or disparate treatment of particular groups 
typically yields discrimination and unfairness, researchers 
often use the terms algorithmic bias, algorithmic 
Figure 1. If an ML model’s prediction of creditworthiness 
guides loan approval, this prediction will shape the data 
available for future updates. As the bank only observes",11.0202527,1,"The generated answer and the ground truth are semantically consistent. The generation provides a direct quote from the text which serves as a valid definition, capturing the same meaning as the ground truth.",1,The generation is faithful. All listed characteristics and theories are explicitly mentioned in the provided text.,1,"The context provides a direct answer. It defines discrimination in the labor market based on the ILOâ€™s Convention 111, which includes 'race, ethnicity, color, and gender' [cite: 1768], and also mentions a similar definition that includes 'ethnic origin, gender, skin color, and age'[cite: 1769]."
What is the generic C R ISP-D M process model useful for?,"Planning, documentation and communication.","According to the text, the generic CRISP-DM process model is useful for:

*   **Planning**
*   **Communication** within and outside the project team
*   **Documentation**","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans developed during these initial projects. These tem plates proved to be quite helpful especially for
people w ho are not proficient w ith our particular data m ining tool.
5 L essons learned
In this section, w e try to sum m arize som e of the experiences w e m ade in both applying the
generic C R ISP-D M  process m odel and in developing a specialized process m odel.
W e expected the generic process m odel to be useful for planning and docum entation, and this
turned out to be the case. H ow ever, the use of the m odel for com m unication both w ithin and
outside the project w as m uch m ore advantageous than w e originally anticipated. Presenting the
project plan and status reports in term s of the process m odel and, of course, the fact that w e
follow ed a process, inspired a lot of confidence in users and sponsors. It also facilitated status
m eetings because the process m odel provided a clear reference and a com m on term inology.
A lthough w e relied on the reference m odel, w e did not alw ays follow  the advice of the user
guide. Som etim es it w as faster to just go ahead (and this is ok w ith the C R ISP-D M
                                                          
2 C lem entine is a tradem ark of SPSS, Inc.",5.923378229,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that the 'generic C R ISP-D M process m odel is useful for planning, com m unication w ithin and outside the project team , and docum entation'.",1,"The context directly answers the question. It states that 'The generic C R ISP-D M process m odel is useful for planning, com m unication w ithin and outside the project team , and docum entation'[cite: 1796]."
"According to the document, what can AI-based recruitment reduce?",Costs.,"According to the document, AI-based recruitment can reduce:

*   **Costs**
*   **Time**
*   **Transactional workload**","recruitment based on artiﬁcial intelligence support and its
beneﬁts. Bogen and Rieke (2018), Ahmed (2018), Hmoud
and Laszlo (2019), Albert (2019), van Esch et al. (2019),
Köchling et al. ( 2022), and Chen ( 2023) consider the
recruitment process as a set of tasks that may be divided
into four steps sourcing, screening, interviewing, and
selection. Each step includes different activities, and AI
algorithms can change how each stage is executed. Some
studies point out that AI-supported recruitment has
beneﬁts. Beattie et al. (2012), Newell (2015), Raub (2018),
Miasato and Silva (2019), Beneduce (2020), and Johnson
et al. (2020) state that it can reduce costs; Hmoud and
Laszlo (2019), Johansson and Herranen (2019), Raveendra
et al. (2020), Black and van Esch (2020), and Allal-Chérif
et al. ( 2021) suggest it saves time; Upadhyay and
Khandelwal (2018) and Johansson and Herranen (2019)
present it reducing transactional workload.
2. The second theme is the causes of algorithmic discrimina-
tion. McFarland and McFarland (2015), Mayson (2018),
Raso et al. (2018), Raub (2018), Raghavan et al. (2020),
Njoto (2020), Zixun (2020), and Jackson (2021) suggest that
the reason for algorithmic discrimination is related to data REVIEW ARTICLE
Ethics and discrimination in artiﬁcial intelligence-
enabled recruitment practices
Zhisheng Chen 1✉
This study aims to address the research gap on algorithmic discrimination caused by AI-
enabled recruitment and explore technical and managerial solutions. The primary research
approach used is a literature review. Theﬁndings suggest that AI-enabled recruitment has
the potential to enhance recruitment quality, increase efﬁciency, and reduce transactional
work. However, algorithmic bias results in discriminatory hiring practices based on gender,
race, color, and personality traits. The study indicates that algorithmic bias stems from
limited raw data sets and biased algorithm designers. To mitigate this issue, it is recom-
mended to implement technical measures, such as unbiased dataset frameworks and
improved algorithmic transparency, as well as management measures like internal corporate
ethical governance and external oversight. Employing Grounded Theory, the study conducted
survey analysis to collectﬁrsthand data on respondents’ experiences and perceptions of AI-
driven recruitment applications and discrimination.
Introduction
T Silva (2019), and Beneduce (2020), Johnson et al. (2020).
Claim that AI-supported recruitment can reduce
costs
Hmoud and Laszlo (2019), Johansson and Herranen (2019),
Raveendra et al. (2020), Black and van Esch (2020), and Allal-
Chérif et al. (2021)
Suggests that AI support can save time
Upadhyay and Khandelwal (2018), Johansson and Herranen
(2019)
Propose that AI support can reduce transactional
workload
2. Causes of algorithmic
discrimination
McFarland and McFarland (2015), Mayson (2018), Raso et al.
(2018), Raub (2018), Raghavan et al. (2020), Njoto (2020),
Zixun (2020), and Jackson (2021)
Suggests that the reason for algorithmic
discrimination is related to data selection
Yarger et al. (2019), Miasato and Silva (2019), and Njoto
(2020)
Propose that discrimination is due to the
designer-induced selection of data features
3. Algorithmic recruitment
discrimination
Correll et al. (2007), Kay et al. (2015), O’neil (2016), Raso et al.
(2018), Miasato and Silva (2019), Langenkamp et al. (2019),
Faragher (2019), Ong (2019), Fernández and Fernández (2019),
Beneduce (2020), Jackson (2021), Yarger et al. (2023), and
Avery et al. (2023)
In recruitment, algorithmic bias can manifest in
gender, race, skin color, and personality",6.019911766,1,The generated answer and the ground truth are semantically identical.,1,"The generation is faithful. The context explicitly states that AI-based recruitment can 'reduce costs', 'save time', and reduce 'transactional workload'.",1,"The context provides a direct answer. It states that AI-supported recruitment 'can reduce costs' [cite: 1809], 'saves time' [cite: 1810], and can reduce 'transactional workload'[cite: 1811]."
"According to the document, what is the aim of the C R ISP-D M process model?","To make large data mining projects less costly, more reliable, more repeatable, more manageable, and faster.","According to the document, the aim of the CRISP-DM process model is to specify a process that can be reliably and efficiently repeated by different people and adapted to different situations. It’s designed to be used by both experienced data mining people and those with lower technical skills.","15. Mai 2025
Structuring an AI project: CRISP-DM 
6
1. Business Understanding2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation
6. Deployment
Data 
Cross Industry Standard Process for Data Mining project w as to specify a process w hich can be reliably and efficiently repeated by different people and adapted to
different situations. The initial projects w ere perform ed by experienced data m ining people; future projects are to be
perform ed by people w ith low er technical skills and w ith very little tim e to experim ent w ith different approaches. It
turned out, that the C R ISP-D M  m ethodology w ith its distinction of generic and specialized process m odels provides
both the structure and the flexibility necessary to suit the needs of both groups.
The generic C R ISP-D M  process m odel is useful for planning, com m unication w ithin and outside the project team , and
docum entation. The generic check-lists are helpful even for experienced people. The generic process m odel provides an
excellent foundation for developing a specialized process m odel w hich prescribes the steps to be taken in detail and
w hich gives practical advice for all these steps.
1 Introduction
D ata m ining is a creative process w hich requires a num ber of different skills and know ledge.
C urrently there is no standard fram ew ork in w hich to carry out data m ining projects. This m eans 15. Mai 2025
Overview CRISP-DM
9",6.721095562,0.9,"The generated answer ('can be reliably and efficiently repeated') captures a key aspect of the ground truth ('less costly, more reliable, more repeatable, more manageable, and faster'). While the ground truth is more comprehensive, the core meaning is consistent.",1,The generation is faithful. The context states the aim was 'to specify a process w hich can be reliably and efficiently repeated by different people and adapted to different situations'.,1,The context directly answers the question. It states the aim was 'to specify a process w hich can be reliably and efficiently repeated by different people and adapted to different situations'[cite: 1830].
